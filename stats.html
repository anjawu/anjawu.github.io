<!DOCTYPE html>
<!-- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->
<head>
<meta charset="utf-8"/>
<!-- Set the viewport width to device width for mobile -->
<meta name="viewport" content="width=device-width"/>
<title>Anja's Stats Information</title>
<!-- CSS Files-->
<link rel="stylesheet" href="stylesheets/style.css">

<link rel="stylesheet" href="stylesheets/skins/blue.css">
<!-- skin color -->
<link rel="stylesheet" href="stylesheets/responsive.css">
<!-- IE Fix for HTML5 Tags -->
<!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-216360630-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-216360630-1');
</script> -->


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VSCS5ZQX38"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VSCS5ZQX38');
</script>

<!-- creating buckets for time spent on page -->
<script type="text/javascript">
function timer11(){gtag('event','1', {'event_category':'stats_TimeOnPage','event_label':'11-30 seconds','non_interaction':true});}
function timer31(){gtag('event','2', {'event_category':'stats_TimeOnPage','event_label':'31-60 seconds','non_interaction':true});}
function timer61(){gtag('event','3', {'event_category':'stats_TimeOnPage','event_label':'61-180 seconds','non_interaction':true});}
function timer181(){gtag('event','4', {'event_category':'stats_TimeOnPage','event_label':'181-360 seconds','non_interaction':true});}
function timer361(){gtag('event','5', {'event_category':'stats_TimeOnPage','event_label':'361-600 seconds','non_interaction':true});}
function timer601(){gtag('event','6', {'event_category':'stats_TimeOnPage','event_label':'601-1800 seconds','non_interaction':true});}
function timer1801(){gtag('event','7', {'event_category':'stats_TimeOnPage','event_label':'1801+ seconds','non_interaction':true});}
gtag('event','0', {'event_category':'stats_TimeOnPage','event_label':'0-10 seconds','non_interaction':true});
setTimeout(timer11,11000);
setTimeout(timer31,31000);
setTimeout(timer61,61000);
setTimeout(timer181,181000);
setTimeout(timer361,361000);
setTimeout(timer601,601000);
setTimeout(timer1801,1801000);
</script>

</head>
<body>
<!-- HIDDEN PANEL 
================================================== -->
<div id="panel">
	<div class="row">
		<div class="twelve columns">
			<img src="http://www.wowthemes.net/demo/studiofrancesca/images/info.png" class="pics" alt="info">
			<div class="infotext">
				 Thank you for visiting my website!
			</div>
		</div>
	</div>
</div>
<p class="slide">
	<a href="#" class="btn-slide"></a>
</p>
<!-- HEADER
================================================== -->
<div class="row">
	<div class="headerlogo four columns">
		<div class="logo">
			<a href="index.html">
			<h4>Anja Wu</h4>
			</a>
		</div>
	</div>
	<div class="headermenu eight columns noleftmarg">
		<nav id="nav-wrap">
		<ul id="main-menu" class="nav-bar sf-menu">
			<li class="current">
			<a href="index.html">Home</a>
			</li>
			<li>
			Coding Info
			<ul>
				<li><a href="sql.html">SQL</a></li>
				<li><a href="ganalytics.html">Google Data</a></li>
				<li><a href="libraries.html">Python Libraries</a></li>
				<li><a href="stats.html">Statistics</a></li>
				<li><a href="ml.html">Machine Learning</a></li>
				<li><a href="github.html">GitHub</a></li>
				<li><a href="funlearn.html">Fun Things Learnt</a></li>
				<li><a href="versus.html">Versus</a></li>
				<li><a href="environmentsetup.html">Environment Setup</a></li>
			</ul>
			</li>
			<li>
			<a href="projects.html">Projects</a>
			<ul>
				<li><a href="collisions.html">Ottawa Collisions</a></li>				
				<li><a href="ottawaparks.html">Explore Ottawa Parks</a></li>
				<li><a href="redlight.html">Red Light</a></li>
				<li><a href="costco.html">Costco</a></li>
				<li><a href="schoolboard.html">School Boards</a></li>				
				<li><a href="website.html">Website</a></li>
				<li><a href="indeed.html">Indeed</a></li>
				<li><a href="imdb.html">IMDb</a></li>
				<li><a href="hangman.html">Hangman</a></li>
			</ul>
			</li>
<!-- 			<li>
			<a href="contact.html">Contact</a>
			</li> -->
		</ul>
		</nav>
	</div>
</div>
<div class="clear">
</div>
<!-- SUBHEADER
================================================== -->
<div id="subheader">
	<div class="row">
		<div class="twelve columns">
			<p class="left">
				Statistics and R
			</p>
		</div>
	</div>
</div>
<div class="hr">
</div>
<!-- CONTENT 
================================================== -->
<div class="row">
	<div class="twelve columns">
		<p>
			I took some free statistics and data science courses through Harvard University and have decided to write down my learnings.
		</p>

		<p>
			The courses I took are:
			<div class="four columns">
				<ul class="disc">
					<li><a href="#statsR" class="saymore">Statistics and R</a></li>
					<li><a href="#visual" class="saymore">Visualization</a></li>
				</ul>
			</div>
			<div class="four columns">
				<ul class="disc">
					<li><a href="#prob" class="saymore">Probability</a></li>
					<li><a href="#linreg" class="saymore">Linear Regression</a></li>
					<!-- <li><a href="#wrangle" class="saymore">Wrangling</a></li> -->
				</ul>
			</div>
			<div class="four columns">
				<ul class="disc">
					<li><a href="#ml" class="saymore">Machine Learning</a></li>
					<li><a href="#inference" class="saymore">Inference and Modeling</a></li>
				</ul>
			</div>
		</p>
	</div>

	
	<!-- Statistics and R-->
	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>R Stats
			</div>
		</div>
		<h4 id="statsR">Statistics and R</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/statistics-and-r" class="saymore">here</a>. Within this course we look at an introduction to R coding and surface level statistic concepts.
		</p>
		<p>

			
		</p>

		<!-- Accordion -->
		<ul class="accordion">

		<!-- Accordion: R Basics -->
			<li class="">
				<div class="title">
					<b>R Basics</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>

					<!-- Tabs -->
					<dl class="tabs">
						<dd class="active"><a href="#rinstall">R Install</a></dd>
						<dd><a href="#rstudio">R-Studio Install</a></dd>
						<dd><a href="#swirl">Swirl</a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="rinstallTab">
							<p>
								<a href="https://cran.r-project.org/index.html" class="saymore">Installing R file</a> 
							</p>
							<h6>Basic commands</h6>
							<p>
								In order to have a package to use in R, you must install (once) and import (each .r file you want to use it in):
								<div id="testimonials"><blockquote>
									install.packages("")<br>
									library()
								</blockquote></div>
							</p>

							<h6>Download CSV File</h6>
							<p>
								First, install downloader package:
								<div id="testimonials"><blockquote>
									install.packages("downloader")
								</blockquote></div>
								Then in the file we want to:
								<div id="testimonials"><blockquote>
									library(downloader)<br>
									url <- "https..."<br>
									filename <- "...csv"<br>
									download(url, destfile = filename)
								</blockquote></div>
							</p>
							<h6>dplyr</h6>
							<p>
								There is also a great package for data manipulation and importation: <i>dplyr</i>
								<div id="testimonials"><blockquote>
									install.packages("dplyr")<br>
									library(dplyr)
								</blockquote></div>
							</p>
							<p>
								You can use the following:
								<ul class="disc">
									<li><i>View(data)</i> which lets you open a new tab to see the data</li>
									<li><i>filter()</i> let's you look at just specific categories of data you want</li>
									<li><i>select()</i> allows you to select and work with specific subset of data you want</li>
									<li><i>unlist()</i> removes stuff from list format - becomes vector</li>
									<li><i>%>%</i> is the inflix operator which works like a pipe, i.e., it passes the left hand side of the operator to the first argument of the right hand side of the operator. The point of it is to save time typing out data and graph changes</li>
									<li><i>basename(url)</i> returns name.csv from a url</li>
									<li><i>summarize(aggregate_fn(column))</i></li>
								</ul>
							</p>
							<p>
								An example using a lot of these would be:
								<div id="testimonials"><blockquote>
									primate_sleep_list <- filter(m_sleep_df, order == "Primates") %>%
									<div class="tab">
										select(sleep_total) %>%<br>
										unlist
									</div>
								</blockquote></div>
							</p>
							<p>You can find my dplyr exercises <a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/1.1.3_dplyr.R" class="saymore">here</a>.
						</li>

						<li class="active" id="rstudioTab">
							<p>
								<a href="https://posit.co/download/rstudio-desktop/" class="saymore">Installing RStudio for Mac</a>
							</p>
							<p>
								There are four panels in RStudio:
								<ul class="disc">
									<li><b>top left</b> <i>source:</i> where you open files and work on them</li>
									<li><b>bottom left</b> <i>console:</i> where output is printed out and you can do operations </li>
									<li><b>top right</b> <i>environment/history:</i> you can see what variables you have created</li>
									<li><b>bottom right</b> <i>help/plots/etc.:</i> has all information and plots from the source/console</li>
								</ul>
							</p>

						</li>

						<li class="active" id="swirlTab">
							<p>
								This great open source library that walks you through the basics of R. <a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/r-stats/swirl_tutorial" class="saymore">Here</a> you can find a couple exercises I completed from the swirl tutorial. It goes through the following:
							</p>

							<ul class="disc">
								<div class="four columns">
									<li><a href="#basics" class="saymore">Basic building blocks</a></li>
									<li><a href="#files" class="saymore">Workspace & files</a></li>
									<li><a href="#seq" class="saymore">Sequence of numbers</a></li>
									<li><a href="#vec" class="saymore">Vectors</a></li>
									<li><a href="#missing" class="saymore">Missing values</a></li>
								</div>
								<div class="four columns">
									<li><a href="#subset" class="saymore">Subsetting vectors</a></li>
									<li><a href="#matrix" class="saymore">Matrix and dataframes</a></li>
									<li><a href="#logic" class="saymore">Logic</a></li>
									<li><a href="#fns" class="saymore">Functions</a></li>
									<li><a href="#apply1" class="saymore">Lapply and sapply</a></li>
								</div>
								<div class="four columns">
									<li><a href="#apply2" class="saymore">Vapply and tapply</a></li>
									<li><a href="#data" class="saymore">Looking at data</a></li>
									<li><a href="#sim" class="saymore">Simulation</a></li>
									<li><a href="#dates" class="saymore">Dates and time</a></li>
									<li><a href="#graphics" class="saymore">Base graphics</a></li>
								</div>
							</ul>

							<div class="twelve columns">
							<h5 id="basics">Basic building blocks</h5>
								<p>
									R has a lot of functions. As a coding language it is best used for mathematics and statistic calculations along with simulations.
								</p>
								<p>
									The math operations are:
									<ul class="disc">
										<li><- : assigns a value to a variable</li>
										<li>+ : adding</li>
										<li>- : subtracting </li>
										<li>/ : dividing </li>
										<li>* : multiplying </li>
										<li>^ : exponent </li>
										<li>%% : gives remainder</li>
										<li>remainder(num, divisor = #)</li>
										<li>sqrt() : square root</li>
										<li>abs() : absolute value</li>
										<li>mean() </li>
										<li>median()</li>
										<li>mode()</li>
										<li>c() : "concatenate"; creates a vector combining the objects within the brackets</li> 
									</ul>
								</p>
								<p>
									For vector operations between vectors are done unit-to-unit if vectors are the same size, however, if vectors are different size, then the shorter vector is recycled until it is the same length as the longer one.
								</p>

							<h5 id="files">Workspace & files</h5>
								<p>
									Some commands that are useful for working on your system:
									<ul class="disc">
										<li>getwd() : get working directory</li>
										<li>ls() : list objects in directory (variables created)</li>
										<li>list.files() : list files in directory</li>
										<li>args() : when used on a function name can let you see what arguments go into the function</li>
										<li>dir.create("name") : this let's you create a directory with a set "name"</li>
										<li>setwd("") : set which directory you want to work in</li>
										<li>file.create("name.R")</li>
										<li>file.rename("old.R", "new.R")</li>
										<li>file.remove("remove-name.R")</li>
										<li>file.copy("original.R", "copy.R")</li>
										<li>file.path("name.R")</li>
										<li>unlink("directory", recursive = TRUE) : deletes directory</li>
									</ul>
								</p>	
								
							<h5 id="seq">Sequence of numbers</h5>
								<p>
									There are several different ways to get a sequence of numbers:
									<ul class="disc">
										<li>: <i>e.g. 1:3 gives us 1 2 3</i></li>
										<li>seq(#, #,  by = increment, length = # of #s you want)</li>
										<li>seq(along.with = #) <i>OR</i> seq_along(#) : gives you a list of integers up until value inputted </li>
										<li>rep(#, times = how many times)</li>
									</ul>
								</p>
								
							<h5 id="vec">Vector</h5>
								<p>
									<u>Atomic vector</u> is a ve tor that contains exactly one data type (i.e. logical, character, integer, etc). <u>List</u> is a vector than cna contain multiple data types. <u>Logical vector</u> has TRUE or FALSE; it checks things using: NA!A, A|B, A&B, ==, !=, <=, >=, <, >. <u>Character vectors</u> double quotes are used to distinguish character objects.
								</p>
								<p>
									Two useful vector things to know:
									<ul class="disc">
										<li>LETTERS : is a predefined R variable for all the letters in the English alphabet.</li>
										<li>paste(vector, collapse = " ") : puts characters together from vector with a space in between; you can also specify something else to go inbetween each part of the vector using this function.
									</ul>
								</p>
								
							<h5 id="missing">Missing Values</h5>
								<p>
									Working with missing values is a regular occurence for data. It is important to understand how to find out if there is missing data and also if to fill it and with what. Here are a couple of good to know functions when working with missing data:
									<ul class="disc">
										<li>is.na(data_vector) : prints out TRUE or FALSE</li>
										<li>rnorm(#) : draws # of values from a standard normal distribution - which can be useful for inputting missing data 
										<li>sample(data, n) : lets you pull out random sample from your data - which can be useful for inputting missing data</li>
									</ul>
									Helpful to know:
									<ul class="disc">
										<li>NA : null</li>
										<li>NaN : not a number</li>
										<li>Inf : infinity</li>
									</ul>
								</p>
								
							<h5 id="subset">Subsetting vectors</h5>
								<p>
									You can create subsets of vectors using <i>vec[#:#]</i> which uses index to pull out values from vector. We can also exclude indices: <i>x[c(-2, -10)]</i> or <i>x[-c(2,10)]</i>.
								</p>
								<p>
									You can also check if the vector is empty or not: <i>is.na(vector)</i> or <i>!is.na(vector)</i>.
								</p>
								<p>
									We can get the titles of the rows of a matrix or dataframe using <i>names(matrix)</i>. And we can also check if two vectors are the same using <i>identical(vector1, vector2)</i> - which returns TRUE or FALSE. 
								</p>

								
							<h5 id="matrix">Matrix and dataframes</h5>
								<p>
									Matrices can only contain one class of data, whereas, dataframes can contain more than one class. Here are some useful functions:
									<ul class="disc">
										<li>length(df)</li>
										<li>attributes(df) : lists things like dimension, # of rows/columns, etc.</li>
										<li>matrix(data = , nrow = #, ncol = #) : creates a matrix with set row and columns</li>
										<li>cbind(vector, vector/matrix) : combines columns of two objects</li>
										<li>data.frame(vector/matrix, vector/matrix) : combines columns into dataframe</li>
										<li>colnames(df) <- list : let's you assign column names from a list </li>
										<li>rownames(df) <- list : let's you assign row names from a list </li>
									</ul>
								</p>
								<p>
									Neat fact is that you can access columns to perfomr aggregate functions by using <i>df$column_name</i>. For instance: sum(df$column_name).
								</p>
								
							<h5 id="logic">Logic</h5>
								<p>
									Above there were some logic functions that were spoken of already. Here are some more:
									<ul class="disc">
										<li>isTRUE(eq'n) : returns TRUE or FALSE</li>
										<li>xor(expression, expression) : only one expression needs to be true to return TRUE</li>
										<li>which(logical_vector) : returns true indices</li>
										<li>any(logical_vector) : returns TRUE if at least one component is true</li>
										<li>all(logical_vector) : returns TRUE is all of the components are true</li>
									</ul>
								</p>
								<p>
									Note that the number of typographical symbols is important:
									<ul class="disc">
										<li>& : evaluates across a vector</li>
										<li>&& : only evaluates the 1st member of a vector</li>
									</ul>
								</p>

								
							<h5 id="fns">Functions</h5>
								<p>
									Functions are created if you plan on reusing them. They are small pieces of reusable code. You can create a function in the same .R file or in a separate. After you have saved your function be sure to type <i>submit()</i> in your console to ensure you can use your function. Here is an example of one:
									<div id="testimonials"><blockquote>
										boring_function <- function(x) {
										<div class="tab">
										  x
										</div>
										}
									</blockquote></div>
								</p>
								<p>
									R has some very useful functions built in: lapply, sapply, vapply, and tapply - which I will go into more detail for. 
								</p>


							<h5 id="apply1">Lapply and sapply</h5>
								<p>
									These are loop functions. They allow us to perform a function through a list. 
								</p>
								<p>
									<u>lapply</u>: applies a function to a list and returns a list.
								</p>
								<p>
									<u>sapply</u>: applies a function to a list but returns values instead of list (i.e. a character list is returned). So for elements length one, it returns a vector. For elements that are the same sized vectors, it returns a matrix. If it cannot figure out what type it is, it returns a list. 
								</p>
								
							<h5 id="apply2">Vapply and tapply</h5>
								<p>
									<u>vapply</u>: does same thing as sapply but allows you to specify format of results - thus speeding up the process for larger datasets.
								</p>
								<p>
									<u>tapply</u>: allows the splitting of data by a specific value of some variable. An example:
									<div id="testimonials"><blockquote>
										tapply(flags$animals, flags$landmass, mean)
									</blockquote></div>
									this function takes the mean of animals separated by landmass.
								</p>
								
							<h5 id="data">Looking at data</h5>
								<p>
									Some useful dataframe functions are:
									<ul class="disc">
										<li>head(df, #)</li>
										<li>tail(df, #)</li>
										<li>summary(df) : gives various details based on datatype per column</li>
										<li>dim(df)</li>
										<li>nrow(df)</li>
										<li>ncol(df)</li>
										<li>names(df) : column names - character vector</li>
										<li>str() : returns structure of df/function/etc.</li>
										<li>class(object)</li>
										<li>read.csv() : default stores it into a dataframe</li>
										<li>read.table() : also store in dataframe</li>
										<li>object.size(df) : size occupying memory</li>
										<li>unique(vector) : returns vector with duplicates removed</li>
									</ul>
								</p>
								
							<h5 id="sim">Simulation</h5>
								<p>
									Simulations are super useful for running experiments that are randomized to collect data. Here are the common functions used for simulations:
									<ul class="disc">
										<li>sample(data, size, replace = FALSE, prob = NULL) : you can specify probability as a vector: <i>c(0.3, 0.7)</i> but it does not have to be known.</li>
										<li>replicate(number of times, distribution) : returns matrix of n trials of the specified distribution. You can also specify your own function instead of a distribution by using <i>{ }</i>.</li>
										<li>colMeans(matrix) : takes mean of each column</li>
										<li>
											We have a lot of different distributions/functions and variables that can be used for creating simulations. Here is a table that breaks down some of them:
											<table>
												<th>
													<td style="text-align:center;">r* <br> <i>random function</i></td>
													<td style="text-align:center;">d*<br> <i>density function</i></td>
													<td style="text-align:center;">p*<br> <i>probability function</i></td>
													<td style="text-align:center;">q*<br> <i>quantile function</i></td>
												</th>
												<tr>
													<td style="text-align:center;">*binom() <br> <i>binomial distribution (R.V.)</i></td>
													<td>rbinom()</td>
													<td>dbinom()</td>
													<td>pbinom()</td>
													<td>qbinom()</td>
												</tr>
												<tr>
													<td style="text-align:center;">*norm() <br> <i>normal distribution (R.V.)</i></td>
													<td></td>
													<td></td>
													<td></td>
													<td></td>
												</tr>
												<tr>
													<td style="text-align:center;">*pois() <br> <i>poisson distribution (R.V.)</i></td>
													<td></td>
													<td></td>
													<td></td>
													<td></td>
												</tr>
												<tr>
													<td style="text-align:center;">*exp() <br> <i>exponential distribution (R.V.)</i></td>
													<td></td>
													<td></td>
													<td></td>
													<td></td>
												</tr>
												<tr>
													<td style="text-align:center;">*chisq() <br> <i>chi-square distribution (R.V.)</i></td>
													<td></td>
													<td></td>
													<td></td>
													<td></td>
												</tr>
												<tr>
													<td style="text-align:center;">*gamma() <br> <i>gamma distribution (R.V.)</i></td>
													<td></td>
													<td></td>
													<td></td>
													<td></td>
												</tr>
											</table>
										</li>
										<li>hist(data) : creates histogram of data - more on visualization in a <a href="#visual" class="saymore">different course</a>.
									</ul>
								</p>
								
							<h5 id="dates">Dates and time</h5>
								<p>
									Dates and times are represented by POSIXct (the number of dats.# of seconds since 1970-01-01) and POSIXlt (list of seconds, minutes, hour, etc.). Some useful function to know:
									<ul class="disc">
										<li>Sys.Date() : days since 1970-01-01</li>
										<li>Sys.time() : time in POSIXlt formate</li>
										<li>weekdays()</li>
										<li>months()</li>
										<li>quarters()</li>
										<li>strtime() : ?strptime gives you a list of variablwe to specify which correspons to what (e.g. %B is the full month name)</li>
										<li>difftime(time1, time 2, units="days") : gives the number of days between the two times</li>
									</ul>
								</p>

								
							<h5 id="graphics">Base graphics</h5>
								<p>
									As mentioned before this we will go over in more detail in the <a href="#visual" class="saymore">visualization course section</a> later on. Here are some basic functions to get us started:
									<ul class="disc">
										<li>data(data) : loads dataframe with data given</li>
										<li>
											plot(data) : creates scatterplot; can access parameters of plot using <i>?par</i>. An example of a basic scatterplot:
											<div id="testimonials"><blockquote>
												plot(x=df$name, y=df$name, xlab = "xlabel", xlim = c(#beginx, #endx), ylab = "ylabel", main = "Main Title", sub = "subtitle", col = number representing color for plot)
											</blockquote></div>
										</li>
										<li>boxplot()</li>
									</ul>
								</p>
								
							</div>

						</li>

					</ul>

				</div>
			</li>


		<!-- Accordion: Exploratory Data Analysis -->
			<li class="">
				<div class="title">
					<b>Exploratory Data Analysis</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>

					<!-- Tabs -->
					<dl class="tabs">
						<dd class="active"><a href="#histo">Histogram</a></dd>
						<dd><a href="#qq">QQ-Plot</a></dd>
						<dd><a href="#box">Boxplot</a></dd>
						<dd><a href="#scatter">Scatterplot</a></dd>
						<dd><a href="#logratio">Symmetry of log ratios</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="histoTab">
							<p>
								The following is useful for all types of plots:
								<ul class="disc">
									<li>load("data.RData") : this is for when you want data that exists as an .RData file</li>
									<li>par(mfrow = c(#, #)) : this changes the view of the subplots, where the first number is the number of rows and second is number of columns</li>
								</ul>
							</p>
							<p>
								Here is a sample of making a histogram (where "breaks" is telling where to draw intervals):
								<div id="testimonials"><blockquote>
									hist(data, main = "Main Title", xlab = "X-Axis", ylab = "Y-Axis", breaks = seq(floor(mindata), ceiling(max(data))))
								</blockquote></div>
							</p>
							<p>
								Related to a histogram, we can use an empirical commutative distribution function to show us the frequency of values occurring below set thresholds. For a normal distribution, you would expect to see something like an S shape.
							</p>
						</li>


						<li id="qqTab">
							<p>
								A QQ-plot (aka quantile-quantile plot) is a plot that displays observed percentiles vs percentiles predicted by normal <i>qqnorm(data)</i>. We are also able to draw a <i>qqline()</i> which makes it easier to see if our data is close to normal.
							</p>
							<p>
								<div id="testimonials"><blockquote>
									qqnorm(data)<br>
									qqline(data, col = "steelblue")
								</blockquote></div>
								It would look like this:
								<center><img src="images/pages/stats/r-stats/qq-plot.png" width="300"></center>
								This data is not normal. 
							</p>
							<p>
								If our data is not normally distributed, we can have data that is <b><u>right (or positively) skewed</u></b> (long tail to right)
								<center>
									<div class="six columns">
										<b>histogram:</b><br>
										<img src="images/pages/stats/r-stats/skew-positive-hist.png" width="300">
									</div>
									<div class="six columns">
										<b>qqnorm:</b><br>
										<img src="images/pages/stats/r-stats/skew-positive-qqnorm.png" width="300">
									</div>
								</center>
								<div class="twelve columns">
								Or we could have data that is <b><u>left (or negatively) skewed</u></b> (long tail to left)
								<center>
									<div class="six columns">
										<b>histogram:</b><br>
										<img src="images/pages/stats/r-stats/skew-negative-hist.png" width="300">
									</div>
									<div class="six columns">
										<b>qqnorm:</b><br>
										<img src="images/pages/stats/r-stats/skew-negative-qqnorm.png" width="300">
									</div>
								</center>
							</div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/1.2.2_QQ-plot.R" class="saymore">Here is a basic example of a qqnorm plot.</a>
							</p>
						</li>


						<li id="boxTab">
							<p>
								Boxplots are commonly used to see distributions (especially if not normal). We call the x-axis: factors and the y-axis: values. A basic example would be:
								<div id="testimonials"><blockquote>
									boxplot(data, ylab = "Y-Axis", ylim = c(#, #))
								</blockquote></div>
								Or we can:
								<div id="testimonials"><blockquote>
									boxplot(split(values, factor))
								</blockquote></div>
								Or even:
								<div id="testimonials"><blockquote>
									boxplot(values~ factor)
								</blockquote></div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/1.2.3_Boxplot.R" class="saymore">Here is a basic coding example of a boxplot.</a>
							</p>
						</li>


						<li id="scatterTab">
							<p>
								Usually scatterplots can show correlation beter than other summary stats.
							</p>
							<p>
								It is a good idea to make a line that plots the correlation to see more data on correlation:
								<div id="testimonials"><blockquote>
									abline(0, cor(x,y))
								</blockquote></div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/4.1.1_Scatterplot.R" class="saymore">Here is a basic coding example of a scatterplot.</a>
							</p>
						</li>


						<li id="logratioTab">
							<p>
								Sometimes we have a graph that doesn't show a linear relationship but rathedr a multiplicative change - making it hard to understand whether there is a relationship in general. For instance, if we have a plot 
								<center><img src="images/pages/stats/r-stats/2x.png" width="300"></center>
								But if we take log base 2 of our function, we can see that there is a very linear relationship:
								<center><img src="images/pages/stats/r-stats/log2x.png" width="300"></center>
							</p>
						</li>

					</ul>
				</div>
			</li>

		<!-- Accordion: Random Variable and Probability Distribution -->
			<li class="">
				<div class="title">
					<b>Random Variable and Probability Distribution</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>

					<!-- Tabs -->
					<dl class="tabs">
						<dd class="active"><a href="#rv">Random variable</a></dd>
						<dd><a href="#nulldis">Null distribution</a></dd>
						<dd><a href="#probdis">Probability distributions</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="rvTab">
							<p>
								Inference statistics lets us look if our results have a significance when looking at the relationship between variables.
							</p>
							<p>
								A random variable is a variable whose values are numerical and determined by random (or by chance).
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.1.1_RandomVariables.R" class="saymore">Here are the R exercises for random variables.</a>
							</p>
						</li>


						<li id="nulldisTab">
							<p>
								A null distribution is a set of all possible realizations under null. The null hypothesis is when there is no effect between the variables being studied.
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.1.2_NullDistributions.R" class="saymore">Here are the R exercises for null distribution.</a>
							</p>
						</li>


						<li id="probdisTab">
							<p>
								A probability distribution shows the probability of each occurrence of the possible outcomes of a random variable.
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.1.3_ProbabilityDistributions.R" class="saymore">Here are the R exercises for probability distribution.</a>
							</p>
						</li>

						
					</ul>
				</div>
			</li>


		<!-- Accordion: Central Limit Theorem -->
			<li class="">
				<div class="title">
					<b>Central Limit Theorem</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>

					<!-- Tabs -->
					<dl class="tabs">
						<dd class="active"><a href="#normdis">Normal distribution</a></dd>
						<dd><a href="#popestimates">Populations, parameters and sample estimates</a></dd>
						<dd><a href="#clt">CLT</a></dd>
						<dd><a href="#ttest">t-tests</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="normdisTab">
							<p>
								As mentioned before a normal distribution is a bell shaped curve of probability. The distribution of the data between standard deviations are specific ratios:
								<center><img src="images/pages/stats/r-stats/normal-sd-percent.png" width="300"></center>
								The formula for the probability is: <center><img src="images/pages/stats/r-stats/norm-dis-formula.png" width="150"></center>
							</p>
							<p>
								We can also standardize the units (which is very useful) by subtracting the mean from each point and dividing by the standard deviation:
								<center><img src="images/pages/stats/r-stats/standardize.png" width="100"></center>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.2.1_NormalDistribution.R" class="saymore">Here are the R exercises for normal distributions.</a>
							</p>
						</li>

						

						<li id="popestimatesTab">
							<p>
								We have our population mean, mu. However, in the real world it is extremely difficult to get the whole population so we usually have to rely on a sample. We also want to make sure our sample is a good representation of the population in order for us to be able to have our results be valid. 
							</p>
							<p>
								The Central Limit Theorem (CLT) lets us see how close the sample average is to the population average.
							</p>
							<p>
								The calculation for population vs sample standard deviation varies slightly. Rafael Irizarry created some shortcuts for data exploration in his library: <i>rafalib</i>. One of the function is for calculating the population sd: <i>popsd(data)</i>. 
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.2.2.2_PopulationSamplesEstimates.R" class="saymore">Here are the R exercises for population samples estimates.</a>
							</p>
						</li>


						<li id="cltTab">
							<p>
								The Central Limit Theorem states that if the sample size is big enough that the sample distribution will approximate the normal distribution (regardless of the population's distribution). This is a very powerful result as it allows us to calculate a lot of information of our sample without knowing the distribution of the population. This is due to the fact the bigger the sample size the smaller the spread of our data (due to the denominator of our sd being bigger leading to a smaller overall sd).
							</p>
							<p>
								<b>How big is big enough?</b><br>
								On average sample sizes that are greater than 30 is sufficient - but it does depend on the data.
							</p>
							<p>
								Under the CLT, the average of the sample means and standard deviations will equal the population mean and standard deviation.
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.2.3_CentralLimitTheorem.R" class="saymore">Here are the R exercises for CLT.</a>
							</p>
						</li>


						<li id="ttestTab">
							<p>
								A t-test is used to compare the means of two groups to see if there was a change between them. This is usually done to see if the treatment group mean is actually different than the control group. In order to use this test, the data must be randomly selected and normally distributed. This is why it can be used in conjunction with the CLT.
							</p>
							<p>
								We must consider the degrees of freedom when looking at our t-test. <a href="https://www.voxco.com/blog/degrees-of-freedom-in-t-test/" class="saymore">This website</a> has one of the simplest explanations for degrees of freedom and why they are needed. Usually for t-tests our degrees of freedom with be the two population amounts minus 2: <i>df = m + n – 2</i>.
							</p>
							<p>
								For a t-test, there are several different kinds depending on the data you have. <a href="https://vitalflux.com/two-sample-t-test-formula-examples/" class="saymore">This website</a> has a really good breakdown of the different kinds you can calculate. Overall, we can have the calculation for the t-statistic between two populations be:
								<center>
									<img src="images/pages/stats/r-stats/tstat1.png" width="200"><br>
									<img src="images/pages/stats/r-stats/tstat2.png" width="200">
								</center>
							</p>
							<p>
								For programming we can use:
								<div id="testimonials"><blockquote>
									t.test(sample1, sample2)
								</blockquote></div>
								This will compute the mean difference and estimate SE
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/2.2.4_CLT-t-distribution.R" class="saymore">Here are the R exercises for the CLT and t-tests.</a>
							</p>
						</li>
						
					</ul>
				</div>
			</li>

		<!-- Accordion: Inference -->
			<li class="">
				<div class="title">
					<b>Inference</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Inference allows us to draw conclusions about our population with only sample data. However, in order to do this properly there is a lot to consider: distribution, size of sample, etc. We looked at some of these already - in particular with the CLT. Here we will look at more things to consider when trying to find out important results about our sample/population.
					</p>

					<!-- Tabs -->
					<dl class="tabs">
						<dd class="active"><a href="#pval">P-value</a></dd>
						<dd><a href="#ci">Confidence Intervals</a></dd>
						<dd><a href="#power">Power Calculations</a></dd>
						<dd><a href="#monte">Monte Carlo Sims</a></dd>
						<dd><a href="#permu">Create Null</a></dd>
						<dd><a href="#association">Association Tests</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="pvalTab">
							<p>
								P-values are important in hypothesis testing. A p-value is the probability of obtaining a result that suggests the alternate hypothesis is correct, given the fact the null hypothesis is actually true. The smaller the p-value, the stronger the evidence is to reject the null hypothesis. 
							</p>
							<p>
								There are different levels of p-values that are used. Most often p-value of 0.05 or 0.01 is used. The level to use is dependent on the data and type of question you are trying to answer. 
							</p>
							<p>
								For programming we can use: <i>pnorm(a)</i> which gives us the probability that our random variable falls below a. Example:
								<div id="testimonials"><blockquote>
									pval <- 2*(1 - pnorm(tstat))
								</blockquote></div>
								for a 2 tailed test using our t-statistic result. For a right tail test:
								<div id="testimonials"><blockquote>
									pval <- 1 - pnorm(abs(tstat))
								</blockquote></div>
								And for a left tail test:
								<div id="testimonials"><blockquote>
									pval <- pnorm( - abs(tstat))
								</blockquote></div>
							</p>
							<p>
								<i>Note: we can change the t-stat to any distribution value we want, e.g. z-score and the process still remains the same when calculating the p-value.</i>
							</p>
						</li>


						<li id="ciTab">
							<p>
								Sometimes very large sample sizes can lead to a very small p-value implying that there is a statistical significance in the results even when there is not one. This is why confidence intervals (CI) are more informative, as they include the estimate itself.
							</p>
							<p>
								We can calculate our confidence interval:
								<center>
									<img src="images/pages/stats/r-stats/ci.png" width="200">
								</center>
									Where:
								<center>
									<img src="images/pages/stats/r-stats/ci_info.png" width="200">
								</center>
							</p>
							<p>
								To calculate using R (if we have a normal distribution):
								<div id="testimonials"><blockquote>
									upper_ci <- xbar + (qnorm(1 - alpha/2) * sd)<br>
									lower_ci <- xbar - (qnorm(1 - alpha/2) * sd) <br>
									ci <- c(lower_ci, upper_ci)
								</blockquote></div>
								If our distribution is not normal or we have too small sample size for CLT, we must use the t-distribution:
								<div id="testimonials"><blockquote>
									upper_ci <- xbar + (qt(1 - alpha/2, df = #) * sd)<br>
									lower_ci <- xbar - (qt(1 - alpha/2, df = #) * sd) <br>
									ci <- c(lower_ci, upper_ci)
								</blockquote></div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.1.1_T-test.R" class="saymore">Here are the R exercises for t-tests</a> and <a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.1.2_ConfidenceIntervals.R" class="saymore">confidence interval calculations.</a>
							</p>
						</li>


						<li id="powerTab">
							<p>
								Statistical power is the probability of rejecting the null hypothesis when the alternate is actually true. The larger the sample size, the larger your power. 
							</p>
							<p>
								We have two main errors when it comes to our hypothesis testing:
								<ol>
									<li>
										<b>Type I error</b>: reject the null, when we should accept. Meaning we think that there is an effect, but there is not.
									</li>
									<li>
										<b>Type II error</b>: accept the null, when we should reject. Meaning we think there is no effect, when there actually is.
									</li>
								</ol>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.1.3_PowerCalculations.R" class="saymore">Here are the R exercises for the power calculations.</a>
							</p>
						</li>


						<li id="monteTab">
							<p>
								Monte Carlo simulations are used to model probability of different outcome in a process that cannot easily be predicted due to the intervention of a random variable.
							</p>
							<p>
								There are many different ways to generate data sets to model our experiments. The most basic building blocks for a Monte Carlo simulation is:
								<div id="testimonials"><blockquote>
									N <- 15<br>
									B <- 10000<br>
									tstats <- replicate(B,{
									<div class="tab">
									  X <- sample(c(-1,1), N, replace=TRUE)<br>
									  sqrt(N)*mean(X)/sd(X)
									</div>
									})
								</blockquote></div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.2.1_MonteCarlo.R" class="saymore">Here are the R exercises for the Monte Carlo simulations.</a>
							</p>
						</li>


						<li id="permuTab">
							<p>
								We can create a null distribution from the data we have using the following steps:
								<ol>
									<li>
										Get new null mean:
										<ul>
											<li>
												a) collect data:
												<div id="testimonials"><blockquote>
													dat <- c(dataset1, dataset2)
												</blockquote></div>
											</li>
											<li>
												b) randomize data:
												<div id="testimonials"><blockquote>
													shuffle <- sample(dat)
												</blockquote></div>
											</li>
											<li>
												c) disperse to 2 groups:
												<div id="testimonials"><blockquote>
													dataset1* <- shuffle[1 : n]<br>
													dataset2* <- shuffle[n+1 : 2n]
												</blockquote></div>
											</li>
											<li>
												d) take mean for null:
												<div id="testimonials"><blockquote>
													mean(dataset1*) - mean(dataset2*) 
												</blockquote></div>
											</li>
										</ul>
									</li>
									<li>
										replicate
										<div id="testimonials"><blockquote>
											replicate(B, null_mean(n))
										</blockquote></div>
									</li>
								</ol>
							</p>
							<p>
								We can calculate p-value given null mean and observed mean:
								<div id="testimonials"><blockquote>
									mean( abs(null_distribution) >= abs(observed) )
								</blockquote></div>
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.2.2_Permutations.R" class="saymore">Here are the R exercises for the permutations.</a>
							</p>
						</li>


						<li id="associationTab">
							<p>
								There are many different association tests that are used for just what the name suggests: finding out if there is any association between two r.v. 
							</p>
							<p>
								There are several useful methods to look at association:
								<ul class="disc">
									<li>
										<i>chisq.test(data)</i>: can tell you whether 2 variables are independent of one another. If the p-value is less than alpha then the observed is not the same as the expected.
									</li>
									<li>
										<i>fisher.test(data)</i>: can tell you if there are non-random associations between two variables (it is based on the hypergeometric distribution).
									</li>
								</ul>
							</p>
							<p>
								An example of something used in genetics is the Manhattan plots are used to plot chromosomes in the x-axis vs the association statisical significance as a <i>-log(pvalue)</i> in the y-axis. It would look something like this:
								<center><img src="images/pages/stats/r-stats/manhattan-plot.png" width="500"></center>
								The y-axis is actually plotting the p-value but the log is taken of the p-value to exaggerate any p-values that are very small (like chromosome 6, 8, 12, and 19 in the picture).
							</p>
							<p>
								<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/3.2.3_AssociationTests.R" class="saymore">Here are the R exercises for the association tests.</a>
							</p>
						</li>
						
					</ul>
				</div>
			</li>

		<!-- Accordion: Robust Summaries -->
			<li class="">
				<div class="title">
					<b>Robust Statistics</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<u>What are robust summary statistics?</u><br>
						They provide valid results even in the face of less than ideal conditions (e.g. data with outliers).
					</p>
					<p>
						Example of robust summary statistics include: median, MAD, and Spearman Correlation.
					</p>
					<p>
						<b>Median</b> isn't as sensitive to outliers as mean because it is more determined by rank of data as opposed to purely values like mean. 
					</p>
					<p>
						<b>MAD <i>(aka median absolute deviation)</i></b> is a robust estimate of the standard deviation. It is calculate by <img src="images/pages/stats/r-stats/MAD.png" width="200">. 
					</p>

					<p>
						<b>Spearman correlation</b> is a more robust summary statistic for correlation (compared to Pearson). This is because it does not just look at the values of the points - which can make it more susceptible to outlier. But rather it looks at the rank of the values slong both axes. It is calculated in two steps.
						<ol>
							<li>Compute rank of the vectors (both x and y)</li>
							<li>Compute correlation of the newly ranked vector</li>
						</ol>
					</p>
					<p> 
						For instance, we might have something that initally looks like this:
						<center><a href="http://mres.uni-potsdam.de/index.php/2017/02/14/outliers-and-correlation-coefficients/"><img src="images/pages/stats/r-stats/outlier-corr.png" width="300"></a></center>
						Here we can see that the data looks correlated. But if we take the rank of the values we might see that it actually looks something like this:
						<center><img src="images/pages/stats/r-stats/outlier-corr-rank.png" width="300"></center>
						which shows no correlation. 
					</p>
					<p>
						This is how the Spearmann correlation works. <a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/4.2.1_Median-MAD-SpearmanCorrelation.R" class="saymore">Here</a> are some exercises I completed for the robust summary statistics.
					</p>
					<p>
						<b>Mann-Whitney-Wilcoxon Test</b> (aka Mann-Whitney U Test or Wilcoxon Rank Sum Test) is used to determine if two samples are likely to be dervied from the same population. 
					</p>
					<p>
						Usually we have our hypothesis testing to check equality of means between two independent samples for a large enough population that is either normally distributed or large enough to be approximately normal. However, in cases were we have a small sample size not normally distributed we want to use a nonparametric test. The Mann-Whitney-Wilcoxon test is a popular nonparametric test with the hypotheses being:
						<ul>
							<li>H0: The two populations are equal.</li>
							<li>H1: The two populations are not equal.</li>
						</ul>
					</p>
					<p>
						It first ranks the y-values between the two groups. Then it looks at how many values are smaller in 1 group compared to the other:
						<center><img src="images/pages/stats/r-stats/m-w-w-test.png" width="200"></center>
						After the sum has been taken of how many are smaller then it divides it by the total number of points within the group. This is then averaged to get an overal score to determine if the two populations are equal or not. 
					</p>
					<p>
						<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/r-stats/exercises/4.2.2_Mann-Whitney-WilcoxonTest.R" class="saymore">Here</a> are some exercises I completed for the Mann-Whitney-Wilcoxon Test.
					</p>
				</div>
			</li>




		</ul>
	</div>

	<div class="hr">
	</div>

	<!-- Visualization -->
	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>Visuals
			</div>
		</div>
		<h4 id="visual">Visualization</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-visualization" class="saymore">here</a>. Within this course we look at different R functions for visualizing data and what scenarios to use them in.
		</p>
		<!-- Accordion -->
		<ul class="accordion">

		<!-- Accordion: Introduction to Data Visualization  -->
			<li class="">
				<div class="title">
					<b>Introduction to Data Visualization</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Data Types</h6>
					<p>
						We have two different types of data: categorial and numerical. Categorical can be futher divided into ordinal (ordered) or non-ordinal. While numerical can be further divided into discrete (finite) or continous.
					</p>
					<h6>Practice Problems</h6>
					<ol class="disc">
						<li>
							<b><u>
								The type of data we are working with will often influence the data visualization technique we use. We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous. 
								We will review data types using some of the examples provided in the dslabs package. For example, the heights dataset.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br><br>


								names(heights)<br>
								## "sex"    "height"
							</blockquote></div>
						</li>
						<li>
							<b><u>
								We saw that sex is the first variable. We know what values are represented by this variable and can confirm this by looking at the first few entires:
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								head(heights)<br>
							</blockquote></div>
							What data type is the sex variable? Categorical 
						</li>
						<li>
							<b><u>
								Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. The height variable could be ordinal if, for example, we report a small number of values such as short, medium, and tall. Let's explore how many unique values are used by the heights variable. For this we can use the unique function:
							</u></b>
							<div id="testimonials"><blockquote>
								x <- c(3, 3, 3, 3, 4, 4, 2)<br>
								unique(x)<br><br>


								library(dslabs)<br>
								data(heights)<br>
								x <- heights$height<br><br>


								length(unique(x))<br>
								## 139
							</blockquote></div>
						</li>
						<li>
							<b><u>
								One of the useful outputs of data visualization is that we can learn about the distribution of variables. For categorical data we can construct this distribution by simply computing the frequency of each unique value. This can be done with the function table. 
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								x <- heights$height<br><br>

								tab <- table(x)
							</blockquote></div>
						</li>
						<li>
							<b><u>
								To see why treating the reported heights as an ordinal value is not useful in practice we note how many values are reported only once.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								tab <- table(heights$height)<br><br>


								sum(tab==1)<br>
								## 63
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Since there are a finite number of reported heights and technically the height can be considered ordinal, what is true:
							</u></b><br>
							It is more effective to consider heights to be numerical given the number of unique values we observe and the fact that if we keep collecting data even more will be observed.
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Introduction to Distributions -->
			<li class="">
				<div class="title">
					<b>Introduction to Distributions</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Cumulative Distribution Function</h6>
					<p>
						Every continous distribution has a cumulative distribtion funciton which defines the proportion of the data below a given value. It is written as: 
						<img src="images/pages/stats/visualization/1.2_cdf.png" width="100">
					</p>
					<p>
						For datasets that are not normal, the CDF can be calculated manually by defining a function to compute the probability above. Like this:
						<div id="testimonials"><blockquote>
								# define range of values spanning the dataset:<br>
								a <- seq(min(my_data), max(my_data), length = 100)<br><br>

								# computes prob. for a single value<br>
								cdf_function <- function(x) { 
									<div class="tab">
								    mean(my_data <= x)
								  </div>
								}<br>
								cdf_values <- sapply(a, cdf_function)<br>
								plot(a, cdf_values)
						</blockquote></div>
					</p>
					<h6>Smooth Density Plots</h6>
					<p>
						Smooth density plots are similar to histoplots but more appealing because they go through the frequency scale vs count scale and at the top of the small histogram buckets. Histogram is assumption free, whereas the smooth density is based on assumptions/choices you make (i.e. you can control the smoothness using ggplot). The Smooth Density is scaled so that the area under the density curve adds up to 1, meaning it gives us the proportion within a range. This all makes it easier to compare two datasets.
					</p>
					<h6>Practice Problems: Distributions</h6>
					<ol class="disc">
						<li>
							<b><u>
								You may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. So, for example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list? We are going to learn when these 2 numbers are enough and when we need more elaborate summaries and plots to describe the data.<br>
								Our first data visualization building block is learning to summarize lists of factors or numeric vectors. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as distribution, there are several data visualization techniques to effectively relay this information. In later assessments we will practice to write code for data visualization. Here we start with some multiple choice questions to test your understanding of distributions and related basic plots.<br>
								In the murders dataset, the region is a categorical variable and on the right you can see its distribution. To the closet 5%, what proportion of the states are in the North Central region?
								<center><img src="images/pages/stats/visualization/1.2_a1_q1.png" width="450"></center>
							</u></b><br>
							25%<br>
						</li>
						<li>
							<b><u>
								In the murders dataset, the region is a categorical variable and to the right is its distribution. What is true:
							</u></b><br>
							The graph shows only four numbers with a bar plot.<br>
						</li>
						<li>
							<b><u>
								The plot shows the eCDF for male heights. Based on the plot, what percentage of males are shorter than 75 inches?
								<center><img src="images/pages/stats/visualization/1.2_a1_q3.png" width="450"></center>
							</u></b>
							95%<br>
						</li>
						<li>
							<b><u>
								The plot shows the eCDF for male heights. To the closest inch, what height m has the property that 1/2 of the male students are taller than m and 1/2 are shorter?
							</u></b><br>
							69 in<br>
						</li>
						<li>
							<b><u>
								Here is an eCDF of the murder rates across states. Knowing that there are 51 states (counting DC) and based on this plot, how many states have murder rates larger than 10 per 100,000 people?
								<center><img src="images/pages/stats/visualization/1.2_a1_q5.png" width="450"></center>
							</u></b><br>
							1 <br>
						</li>
						<li>
							<b><u>
								What is true:
							</u></b><br>
							with the exception of 4 states, the murder rates are below 5 per 100,000.<br>
						</li>
						<li>
							<b><u>
								Here is a histogram of male heights in our heights dataset. Based on this plot, how many males are between 62.5 and 65.5?
								<center><img src="images/pages/stats/visualization/1.2_a1_q7.png" width="450"></center>
							</u></b><Br>
							58<br>
						</li>
						<li>
							<b><u>
								From above, About what percentage are shorter than 60 inches?
							</u></b><br>
							1%<br>
						</li>
						<li>
							<b><u>
								Based on this density plot, about what proportion of US states have populations larger than 10 million?
								<center><img src="images/pages/stats/visualization/1.2_a1_q9.png" width="450"></center>
							</u></b><br>
							0.15<br>
						</li>
						<li>
							<b><u>
								Here are three density plots. Is it possible that they are from the same dataset? What is true:
								<center><img src="images/pages/stats/visualization/1.2_a1_q10.png" width="750"></center>
							</u></b><br>
							They are the same dataset, but the first does not have the x-axis in the log scale, the second undersmooths and the third oversmooths.<br>
						</li>
					</ol>

					<h6>Normal Distribution aka Guassian distribution</h6>
					<p>
						The normal distribution has the classic bell-shaped curve where: 1 standard deviation contains 68% of the data, 2sds contain 95%, and 3sds contain 99.7% of the data.
						<center><img src="images/pages/stats/visualization/1.2_normdist.png" width="250"></center>
					</p>
					<p>
						The equation for the distribution is given by <img src="images/pages/stats/visualization/1.2_normdist-eqn.png" width="195">. While the equation to convert from a random variable (X) to the normal (Z) is given by <img src="images/pages/stats/visualization/1.2_norm-eqn.png" width="60">
					</p>
					<p>
						The scale function converts a vector of approximately normally distributed values into z-scores:
						<div id="testimonials"><blockquote>
								z <- scale(x)
						</blockquote></div>
					</p>
					<p>
						You can compute the proportion of observations that are within 2 standard deviations of the mean like this:
						<div id="testimonials"><blockquote>
								mean(abs(z) < 2)
						</blockquote></div>
					</p>
					<h6>The Normal CDF and pnorm</h6>
					<p>
						Discretization: although true height distribution is continuous, the reported heights tend to be more common at discrete values (usually due to rounding) meaning that the pnorm is not as accurate when looking over ranges not including an integer.
						<div id="testimonials"><blockquote>
								F(a) <- pnorm(a, mu, sigma)
						</blockquote></div>
					</p>
					
					<h6>Practice Problems: Normal Distribution</h6>
					<ol class="disc">
						<li>
							<b><u>
								Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.<br>
								The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations. Examples include gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. Often data visualization is needed to confirm that our data follows a normal distribution.<br>
								Here we focus on how the normal distribution helps us summarize data and can be useful in practice.<br>
								One way the normal distribution is useful is that it can be used to approximate the distribution of a list of numbers without having access to the entire list. We will demonstrate this with the heights dataset.
								Load the height data set and create a vector x with just the male heights:
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								x <- heights$height[heights$sex == "Male"]<br><br>


								mean(x<=72) - mean(x <= 69)<br>
								## 0.3337438

							</blockquote></div>
						</li>
						<li>
							<b><u>
								Suppose all you know about the height data from the previous exercise is the average and the standard deviation and that its distribution is approximated by the normal distribution. Suppose you only have avg and stdev below, but no access to x, can you approximate the proportion of the data that is between 69 and 72 inches?
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								x <- heights$height[heights$sex=="Male"]<br>
								avg <- mean(x)<br>
								stdev <- sd(x)<br><br>


								pnorm(72, avg, stdev) - pnorm(69, avg, stdev)<br>
								## 0.3061779

							</blockquote></div>
						</li>
						<li>
							<b><u>
								Notice that the approximation calculated in the second question is very close to the exact calculation in the first question. The normal distribution was a useful approximation for this case.
								However, the approximation is not always useful. An example is for the more extreme values, often called the "tails" of the distribution. Let's look at an example. We can compute the proportion of heights between 79 and 81.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								x <- heights$height[heights$sex == "Male"]<br>
								exact <- mean(x > 79 & x <= 81)<br>
								approx <- pnorm(81, mean(x), sd(x)) - pnorm(79, mean(x), sd(x))<br><br>


								exact/approx<br>
								## 1.614261
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Someone asks you what percent of seven footers are in the National Basketball Association (NBA). Can you provide an estimate? Let's try using the normal approximation to answer this question.<br>
								First, we will estimate the proportion of adult men that are taller than 7 feet.<br>
								Assume that the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches.
							</u></b>
							<div id="testimonials"><blockquote>
								# use pnorm to calculate the proportion over 7 feet (7*12 inches)<br>
								1 - pnorm(7*12, 69, 3)<br>
								## 2.866516e-07
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now we have an approximation for the proportion, call it p, of men that are 7 feet tall or taller.
								We know that there are about 1 billion men between the ages of 18 and 40 in the world, the age range for the NBA.
								Can we use the normal distribution to estimate how many of these 1 billion men are at least seven feet tall?
							</u></b>
							<div id="testimonials"><blockquote>
								p <- 1 - pnorm(7*12, 69, 3)<br><br>


								round(10^9*p)<br>
								## 287
							</blockquote></div>
						</li>
						<li>
							<b><u>
								There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher.
							</u></b>
							<div id="testimonials"><blockquote>
								p <- 1 - pnorm(7*12, 69, 3)<br>
								N <- round(10^9*p)<br><br>


								10/N<br>
								## 0.03484321
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In the previous exercise we estimated the proportion of seven footers in the NBA using this simple code. Repeat the calculations performed in the previous question for Lebron James' height: 6 feet 8 inches. There are about 150 players, instead of 10, that are at least that tall in the NBA.
							</u></b>
							<div id="testimonials"><blockquote>
								## Change the solution to previous answer<br>
								p <- 1 - pnorm((6*12)+8, 69, 3)<br>
								N <- round(p * 10^9)<br><br>

								150/N<br>
								## 0.001220842
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations?
							</u></b><br>
							As seen in exercise 3, the normal approximation tends to underestimate the extreme values. It's possible that there are more seven footers than we predicted.
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Quantiles, Percentiles, and Boxplots -->
			<li class="">
				<div class="title">
					<b>Quantiles, Percentiles, and Boxplots</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Definition of quantiles</h6>
					<p>
						Quantiles are "cutoff points that divide a dataset into intervals with set probabilities - where the qth quantile is the value at which q% of the observations are equal to or less than that value".
					</p>
					<p>
						We can get the quanitile by simply using <i>quantile(data, desired_quantile)</i>. Furthermore, we can get percentiles - which are the quantiles that divide a dataset into 100 intervals each with 1% probability: 
						<div id="testimonials"><blockquote>
							p <- seq(0.01, 0.99, 0.01)<br>
							quantile(data, p)
						</blockquote></div>
					</p>
					<h6>Finding quantiles with qnorm</h6>
					<p>
						We can determine the theoretical quantiles of a dataset: the theoretical value of quantiles assuming that a dataset follows a normal distribution by using <i>qnorm(p, mu, sigma)</i>.
					</p>
					<h6>Quantile-Quantile Plots</h6>
					<p>
						We can use <i>abline(0,1)</i> to show if the distribution is normally distributed.
					</p>
					<p>
						For normal distribution, mean and median are the same.
					</p>
					<h6>Boxplots</h6>
					<p>
						Boxplots are five number summaries: range, Q1, Q2, Q3, Q4. The interquartile range is calculated using Q3 and Q1: <i>IQR = Q3 - Q1</i>
					</p>

					<h6>Practice Problems: Quantiles, percentiles, and boxplots</h6>
					<ol class="disc">
						<li>
							<b><u>
								When analyzing data it's often important to know the number of measurements you have for each category.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								male <- heights$height[heights$sex=="Male"]<br>
								female <- heights$height[heights$sex=="Female"]<br><br>


								length(male)<br>
								## 812<br>
								length(female)<br>
								## 238
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Suppose we can't make a plot and want to compare the distributions side by side. If the number of data points is large, listing all the numbers is impractical. A more practical approach is to look at the percentiles. 
							</u></b>
							<div id="testimonials"><blockquote>
								library(dslabs)<br>
								data(heights)<br>
								male <- heights$height[heights$sex=="Male"]<br>
								female <- heights$height[heights$sex=="Female"]<br><br>


								percentiles <- c(0.1, 0.3, 0.5, 0.7, 0.9)<br><br>


								female_percentiles <- quantile(female, percentiles)<br>
								male_percentiles <- quantile(male, percentiles)<br><br>


								df <- data.frame(female = female_percentiles, male = male_percentiles)<br>
								df
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/1.3_q2_df.png" width="150"></center>
						</li>
						<li>
							<b><u>
								Study the boxplots summarizing the distributions of populations sizes by country.
								<center><img src="images/pages/stats/visualization/1.3_q3_boxplot.png" width="600"></center>
								Which continent has the country with the largest population size?
							</u></b><br>
							Asia<br>
						</li>
						<li>
							<b><u>
								Which continent has the largest median population?
							</u></b><br>
							Africa
						</li>
						<li>
							<b><u>
								To the nearest million, what is the median population size for Africa?
							</u></b><br>
								10 million
						</li>
						<li>
							<b><u>
								Approximately what proportion of countries in Europe have populations below 14 million.
							</u></b><br>
							0.75
						</li>
						<li>
							<b><u>
								Which continent shown below has the largest interquartile range for log(population)?
							</u></b><br>
							Americas
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Exploratory Data Analysis -->
			<li class="">
				<div class="title">
					<b>Exploratory Data Analysis</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Practice Problems: Robust Summaries with Outliers</h6>
					<ol class="disc">
						<li>
							<b><u>
								For this chapter, we will use height data collected by Francis Galton for his genetics studies. Here we just use height of the children in the dataset. Compute the average and median of these data.
							</u></b>
							<div id="testimonials"><blockquote>
								library(HistData)<br>
								data(Galton)<br>
								x <- Galton$child<br><br>


								mean(x)<br>
								## 68.08847<br>
								median(x)<br>
								## 68.2
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now for the same data compute the standard deviation and the median absolute deviation (MAD).
							</u></b>
							<div id="testimonials"><blockquote>
								library(HistData)<br>
								data(Galton)<br>
								x <- Galton$child<br><br>


								sd(x)<br>
								## 2.517941<br><br>


								#median absolute deviation:<br>
								mad(x)<br>
								## 2.9652
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In the previous exercises we saw that the mean and median are very similar and so are the standard deviation and MAD. This is expected since the data is approximated by a normal distribution which has this property. Now suppose that Galton made a mistake when entering the first value, forgetting to use the decimal point. The data now has an outlier that the normal approximation does not account for. Let's see how this affects the average.
							</u></b>
							<div id="testimonials"><blockquote>
								library(HistData)<br>
								data(Galton)<br>
								x <- Galton$child<br>
								x_with_error <- x<br>
								x_with_error[1] <- x_with_error[1]*10<br><br>


								mean(x_with_error) - mean(x)<br>
								## 0.5983836
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In the previous exercise we saw how a simple mistake in 1 out of over 900 observations can result in the average of our data increasing more than half an inch, which is a large difference in practical terms. Now let's explore the effect this outlier has on the standard deviation.
							</u></b>
							<div id="testimonials"><blockquote>
								x_with_error <- x<br>
								x_with_error[1] <- x_with_error[1]*10<br><br>


								sd(x_with_error) - sd(x)<br>
								## 15.6746
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In the previous exercises we saw how one mistake can have a substantial effect on the average and the standard deviation. Now we are going to see how the median and MAD are much more resistant to outliers. For this reason we say that they are robust summaries.
							</u></b>
							<div id="testimonials"><blockquote>
								x_with_error <- x<br>
								x_with_error[1] <- x_with_error[1]*10<br><br>


								median(x_with_error) - median(x)<br>
								##0
							</blockquote></div>
						</li>
						<li>
							<b><u>
								We saw that the median barely changes. Now let's see how the MAD is affected.
							</u></b>
							<div id="testimonials"><blockquote>
								x_with_error <- x<br>
								x_with_error[1] <- x_with_error[1]*10<br><br>


								mad(x_with_error) - mad(x)<br>
								## 0
							</blockquote></div>
						</li>
						<li>
							<b><u>
								How could you use exploratory data analysis to detect that an error was made?
							</u></b><br>
							A boxplot, histogram, or qq-plot would reveal a clear outlier.
						</li><br>
						<li>
							<b><u>
								We have seen how the average can be affected by outliers. But how large can this effect get? This of course depends on the size of the outlier and the size of the dataset. To see how outliers can affect the average of a dataset, let's write a simple function that takes the size of the outlier as input and returns the average.
							</u></b>
							<div id="testimonials"><blockquote>
								x <- Galton$child<br>
								error_avg <- function(k){
									<div class="tab">
								    x_error <- x<br>
								    x_error[1] <- k<br>
								    mean(x_error)
								  </div>
								}<br><br>

								error_avg(10000)<br>
								## 78.79784<br>
								error_avg(-10000)<br>
								## 57.24612
							</blockquote></div>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Basics of ggplot2  -->
			<li class="">
				<div class="title">
					<b>Basics of ggplot2</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>ggplot</h6>
					<p>
						“Grammar of graphics” (gg) lets you use simple base cases to do a lot of stuff. Where you are working with data tables: rows have to be observations and columns have to be variables. There is a really good <a href="https://github.com/rstudio/cheatsheets/blob/main/data-visualization-2.1.pdf" class="saymore">cheatsheet for graphing</a>. Tidyverse is a package that contains dplyr and ggplot2, which makes it easier to work with dataframes and create graphic.
					</p>
					<h6>Graph Components</h6>
					<p>
						Break graph into components: data (what are you summarizing), geometry (scatterplot, bar plot, histograms, smooth densities, q-q plots, and box plots), aesthetic mappings, scale component, labels, titles, legends, etc.
					</p>
					<h6>Creating a New Plot</h6>
					<p>
						Need ggplot object: <i>ggplot(data = dataset)</i> OR pipe the data: <i>dataset %>% ggplot()</i>.
					</p>
				</div>
			</li>

		<!-- Accordion:  Customizing Plots -->
			<li class="">
				<div class="title">
					<b>Customizing Plots</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Layers</h6>
					<p>
						In order to make graph you add layers (added component by component). Order of layers matters!! Adds layers in order from top down You add a layer using “<i>+</i>”:
						<div id="testimonials"><blockquote>
							Data %>% ggplot() %>% layer1 + layer 2 +... +layer n
						</blockquote></div>
					<ul class="disc">
						<li>Geometry: geom_point will let you create the actual graph using the loaded data</li>
						<li><i>geom_abline()</i> gives us intercept a and slope b</li>
						<li>
							The most common one used is aes (i.e. aesthetic mappings)
							<div id="testimonials"><blockquote>
								aes(x = … , y = …)
							</blockquote></div>
						</li>
						<li>
							Geom_label adds label to plot with little rectangle and geom_text adds text to plot. E.g. <i>geom_text(aes(x, y, label = column))</i>
						</li>
						<li>
							Nudge_x moves labels to the right; used outside of aes
						</li>
					</ul>
					<h6>Tinkering</h6>
					<p>
						Through tinkering around with the settings the following was found:
						<ul class="disc">
							<li>When you affect something outside of aes all the sizes change not based on the data given</li>
							<li>By putting in the aes in ggplot() then this is a global aes (meaning that you do not need to call x and y each time for geom_… )</li>
							<li>The local mappings can still override the global mapping</li>
						</ul>
					</p>

					<h6>Scales, Labels, and Colors</h6>
					<p>
						It is benefical to change scales to logarithmic to spread clusters of data when values are high. We can do this using: <i>scale_x_continous(trans = “log10”)</i> for different logs or <i>scale_x_log10()</i> function within ggplot (because log base 10 is used so often).
					</p>
					<p>
						In order to label the axes and title we use: <i>xlab(), ylab(), ggtitle()</i>.
					</p>
					<p>
						For colours, we use the <i>color</i> argument in <i>geom_point</i>. Aes needs to be used to create a mapping: <i>geom_point(aes(col=region))</i>. And to change the legend title: <i>scale_color_discrete()</i>
					</p>
					<h6>Add-on Packages</h6>
					<p>
						There are more add-on packages that make graphing more complex:
						<ul class="disc">
							<li>
								<i>ggthemes()</i> which has themes that can be set using dslabs package: <i>ds_theme_set()</i> and <i>theme_economist()</i> being one of the types of themes.
							</li>
							<li>
								<i>ggrepel()</i> has a part that adds labels but ensures they don’t fall on top of each other. In order to use it, change <i>geom_text()</i> to <i>geom_text_repel()</i>.
							</li>
							<li>
								gridExtra package let's you create plots then populate this function <i>grid.arrange()</i> which helps layout the graphs.
							</li>
						</ul>
					</p>
					<h6>Other Examples</h6>
					<p>
						There are other examples of types of types of graphs, the most commonly used ones being: <i>geom_histogram()</i>, <i>geom_density()</i>, <i>geom_qq()</i>.
					</p>

					<h6>Practice Problems</h6>
					<ol class="disc">
						<li>
							<b><u>
								Start by loading the dplyr and ggplot2 libraries as well as the murders data.
								With ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this</u></b>
								<div id="testimonials"><blockquote>p <- ggplot(data = murders)</blockquote></div>
								<b><u>Because data is the first argument we don't need to spell it out. So we can write this instead:</u></b>
								<div id="testimonials"><blockquote>p <- ggplot(murders)</blockquote></div>
								<b><u>or, if we load dplyr, we can use the pipe:</u></b>
								<div id="testimonials"><blockquote>p <- murders %>% ggplot()</blockquote></div>
								<b><u>Remember the pipe sends the object on the left of %>% to be the first argument for the function the right of %>%.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br><br>
								data(heights)<br>
								data(murders)<br><br>
								p <- ggplot(murders)<br>
								class(p)<br>
								## "gg"     "ggplot"
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Remember that to print an object you can use the command print or simply type the object. For example, instead of</u></b>
								<div id="testimonials"><blockquote>x <- 2<br>
								print(x)</blockquote></div>
								<b><u>you can simply type</u></b>
								<div id="testimonials"><blockquote>x <- 2<br>
								x</blockquote></div>
								<b><u>Print the object p defined in exercise one</u></b>
								<div id="testimonials"><blockquote>p <- ggplot(murders)</blockquote></div>
								<b><u>and describe what you see.
							</u></b><br>
							A blank slate plot.
						</li><br>
						<li>
							<b><u>
								Now we are going to review the use of pipes by seeing how they can be used with ggplot. Using the pipe %>%, create an object p associated with the heights dataset instead of with the murders dataset as in previous exercises.
							</u></b>
							<div id="testimonials"><blockquote>
								data(heights)<br>
								# define ggplot object called p like in the previous exercise but using a pipe <br>
								p <- heights %>% ggplot()
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now we are going to add layers and the corresponding aesthetic mappings. For the murders data, we plotted total murders versus population sizes in the videos. Explore the murders data frame to remind yourself of the names for the two variables (total murders and population size) we want to plot and select the correct answer.
							</u></b><br>
							total and population
						</li><br>
						<li>
							<b><u>
								To create a scatter plot, we add a layer with the function geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables respectively. 
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(x = population, y = total)) +
								<div class="tab">
								  geom_point()
								</div>
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Switch order
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(total, population)) +
								<div class="tab">
								  geom_point()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q6.png" width="400"></center>
						</li>
						<li>
							<b><u>
								If instead of points we want to add text, we can use the geom_text() or geom_label() geometries. However, note that the following code
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(population, total)) +
								<div class="tab">
								  geom_label()
								</div>
							</blockquote></div>
							<b><u>
								will give us the error message: Error: geom_label requires the following missing aesthetics: label. Why is this?
							</u></b><br>
							We need to map a character to each point through the label argument in aes
						</li><br>
						<li>
							<b><u>
								 You can also add labels to the points on a plot.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br><br>
								data(murders)<br>
								# Add the label<br>
								murders %>% ggplot(aes(population, total, label = abb)) +
								<div class="tab">
								  geom_label()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q8.png" width="400"></center>
						</li>
						<li>
							<b><u>
								geom_point colors. Now let's change the color of the labels to blue. How can we do this?
							</u></b><br>
							By using the color argument in geom_label because we want all colors to be blue so we do not need to map colors
						</li><br>
						<li>
							<b><u>
								Now let's go ahead and make the labels blue. 
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(population, total,label= abb)) +
								<div class="tab">
								  geom_label(color="blue")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q10.png" width="400"></center>
						</li>
						<li>
							<b><u>
								Now suppose we want to use color to represent the different regions. So the states from the West will be one color, states from the Northeast another, and so on. In this case, which of the following is most appropriate:
							</u></b><br>
							Mapping the colors through the color argument of aes because each label needs a different color
						</li><br>
						<li>
							<b><u>
								We previously used this code to make a plot using the state abbreviations as labels:
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(population, total, label = abb)) +
								<div class="tab">
								  geom_label()
								</div>
							</blockquote></div>
							<b><u>
								We are now going to add color to represent the region.
							</u></b>
							<div id="testimonials"><blockquote>
								murders %>% ggplot(aes(population, total, label = abb, color = region)) +
								  <div class="tab">
								  	geom_label()
								  </div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q12.png" width="400"></center>
						</li>
						<li>
							<b><u>
								Now we are going to change the axes to log scales to account for the fact that the population distribution is skewed. Let's start by defining an object p that holds the plot we have made up to now:
							</u></b>
							<div id="testimonials"><blockquote>
								p <- murders %>% ggplot(aes(population, total, label = abb, color = region)) +
									<div class="tab">
								  geom_label() 
								</div>
							</blockquote></div>
							<b><u>
								To change the x-axis to a log scale we learned about the scale_x_log10() function. We can change the axis by adding this layer to the object p to change the scale and render the plot using the following code <i>p + scale_x_log10()</i>
							</u></b>
							<div id="testimonials"><blockquote>
								p <- murders %>% ggplot(aes(population, total, label = abb, color = region)) + geom_label()<br>
								# add log scale to p <br>
								p + scale_x_log10() +scale_y_log10()
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q13.png" width="400"></center>
						</li>
						<li>
							<b><u>
								In the previous exercises we created a plot using the following code:
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br><br>
								data(murders)<br><br>
								p<- murders %>% ggplot(aes(population, total, label = abb, color = region)) +
									<div class="tab">
								  	geom_label()
								  </div>
								p + scale_x_log10() + scale_y_log10()
							</blockquote></div>
							<b><u>
								We are now going to add a title to this plot. We will do this by adding yet another layer, this time with the function ggtitle.
							</u></b>
							<div id="testimonials"><blockquote>
								p <- murders %>% ggplot(aes(population, total, label = abb, color = region)) +
									<div class="tab">
								  	geom_label()
								  </div>
								# add a layer to add title to the next line <br>
								p + scale_x_log10() + 
								<div class="tab">
								  scale_y_log10() +<br>
								  ggtitle("Gun murder data")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q14.png" width="400"></center>
						</li>
						<li>
							<b><u>
								We are going to shift our focus from the murders dataset to explore the heights dataset.<br>
								We use the geom_histogram function to make a histogram of the heights in the heights data frame. When reading the documentation for this function we see that it requires just one mapping, the values to be used for the histogram.<br>
								What is the variable containing the heights in inches in the heights data frame?
							</u></b><br>
							height
						</li><br>
						<li>
							<b><u>
								We are now going to make a histogram of the heights so we will load the heights dataset.
							</u></b>
							<div id="testimonials"><blockquote>
								p <- heights %>% ggplot(aes(height)) +
									<div class="tab">
								    geom_histogram()
								  </div>
								p
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q16.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Note that when we run the code from the previous exercise we get the following warning:
								stat_bin() using bins = 30. Pick better value with binwidth.
							</u></b>
							<div id="testimonials"><blockquote>
								p <- heights %>% 
									<div class="tab">
								 	 ggplot(aes(height))
									</div>
								# add the geom_histogram layer but with the better binwidth<br>
								p + geom_histogram(binwidth = 1)
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q17.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now instead of a histogram we are going to make a smooth density plot. In this case, we will not make an object p. Instead we will render the plot using a single line of code. In the previous exercise, we could have created a histogram using one line of code like this:
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height)) +<br>
								  geom_histogram()
								</div>
							</blockquote></div>
							<b><u>
								Now instead of geom_histogram we will use geom_density to create a smooth density plot.
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height)) +<br>
								  geom_density()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q18.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now we are going to make density plots for males and females separately. We can do this using the group argument within the aes mapping. Because each point will be assigned to a different density depending on a variable from the dataset, we need to map within aes.
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height, group = sex)) + <br>
								  geom_density()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q19.png" width="350"></center>
						</li>
						<li>
							<b><u>
								In the previous exercise we made the two density plots, one for each sex, using:
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height, group = sex)) + <br>
								  geom_density()
								</div>
							</blockquote></div>
							<b><u>
								We can also assign groups through the color or fill argument. For example, if you type color = sex ggplot knows you want a different color for each sex. So two densities must be drawn. You can therefore skip the group = sex mapping. Using color has the added benefit that it uses color to distinguish the groups.
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height, color = sex)) +<br>
								  geom_density()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q20.png" width="350"></center>
						</li>
						<li>
							<b><u>
								We can also assign groups using the fill argument. When using the geom_density geometry, color creates a colored line for the smooth density plot while fill colors in the area under the curve. We can see what this looks like by running the following code:
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height, fill = sex)) +<br> 
								  geom_density() 
								</div>
							</blockquote></div>
							<b><u>
								However, here the second density is drawn over the other. We can change this by using something called alpha blending.
							</u></b>
							<div id="testimonials"><blockquote>
								heights %>% 
								<div class="tab">
								  ggplot(aes(height, fill = sex)) + <br>
								  geom_density(alpha=0.2) 
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/2.2_q21.png" width="350"></center>
						</li>
						
					</ol>
				</div>
			</li>

		<!-- Accordion: Introduction to Gapminder -->
			<li class="">
				<div class="title">
					<b>Introduction to Gapminder</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						Gapminder.org is an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. You can access it from the <i>Dslabs library</i> using <i>data(gapminder)</i>. Started because views on the world based on wealthy was cemented about 50 years ago (1962) and it has not been updated - Gapminder looks to inform people with data about the truth of now (using data).
					</p>

				</div>
			</li>

		<!-- Accordion:  Using the Gapminder Dataset -->
			<li class="">
				<div class="title">
					<b>Using the Gapminder Dataset</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Faceting</h6>
					<p>
						The faceting variables allows you to make side by side comparisons for varying data. It keeps the scale the same through all plots to allow comparisons easily to occur. You can add layers and stratify by up to 2 variables: <i>facet_grid()</i>. Example: <i>facet_grid(continent ~ year)</i> divides it by year and continent.
					</p>
					<center><img src="images/pages/stats/visualization/3.2_faceting-grid1.png" width="400"></center>
					<p>
						We can also only divide by 1 variable: <i>facet_grid(. ~ year)</i>:
					</p>
					<center><img src="images/pages/stats/visualization/3.2_faceting-grid2.png" width="450"></center>
					<p>
						If we want to analyze more than 2 years, we can but we should use <i>facet_wrap()</i>, which will automatically wraps accordingly (<i>facet_grid(facet_wrap(~year))</i>.
					</p>
					<center><img src="images/pages/stats/visualization/3.2_faceting-grid2.png" width="450"></center>
					<h6>Time Series Plots</h6>
					<p>
						When you want to look at data across time, your time should be on the x-axis and what you want to look at in y-axis. You can use <i>group=category</i> or <i>color = category</i> within <i>ggplot</i> to break the data up into categories over time:
						<div id="testimonials"><blockquote>
								geom_text(data = labels, aes(x, y, color = country), size 5) +
								<div class="tab">
									theme(legend.position="none")
								</div>
						</blockquote></div>
					</p>
					<h6>Transformations</h6>
					<p>
						The way data is given maybe useful when looking at the numbers but you might need a transformation in order to make the graph clear to use. For instance, to get GDP per day is more useful to <i>mutate(dollar_per_day = gdp/population/365)</i> and then take <i>log2(dollars_per_day)</i> (or you could use <i>scale_x_continuous(trans = “log2”)</i>).
					</p>
					<h6>Stratify and Boxplot</h6>
					<p>
						When you are creating a graph - particularly boxplots - it is useful to know how to rearrange the boxplots so that there is some sort of ordering that makes sense to the data you are trying to convey. We can to this by using <i>reorder()</i> based on a numeric vector. 
						<div id="testimonials"><blockquote>
							mutate(region = reorder(region, dollar_per_day, FUN = median)) %>% <br>geom_boxplot() + 
							<div class="tab">
								theme(axis.text.x = element_text(angle = 90, hjust =1))	
							</div>						
						</blockquote></div>
					</p>
					<h6>Comparing Distributions</h6>
					<p>
						Within the data you can separate data into new groupings using mutate: <i>mutate(group = ifelse(region%in%west, “West”, “Developing”))</i>. You can also find the intersection between two lists using <i>intersect()</i> (tidyverse has a simpler version to do this). And a third way you can compare is by adding a factor for year (<i>fill = factor(year)</i>) in your <i>aes</i>
					</p>
					<h6>Density Plots</h6>
					<p>
						To have the areas of the densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group within <i>geom_density</i>. <b>To access geom features use ".."</b>. 
						<div id="testimonials"><blockquote>
							geom_density(aes(x=dollars_per_day, y = ..count..)) 
						</blockquote></div>
					</p>
					<p>
						We can also use <i>case_when()</i> to group certain aspects together:
						<div id="testimonials"><blockquote>
							mutate(group = case_when(
							<div class="tab">
								.$region %in% west ~ "West"<br>
								.$region %in% c("Eastern Asia", "South-Eastern Asia") ~ "East Asia", <br>
								.$region %in% c("Caribbean", "Central America", "South America") ~ "Latin America" <br>
								.$continent == "Africa" & .$region != "Northern Africa" ~ "Sub-Saharan Africa",<br>
								TRUE ~ "Others"
							</div>
							))
						</blockquote></div>
					</p>
					<p>
						A couple more useful arguments to know:
						<ul class="disc">
							<li>
								<i>levels = c()</i> lets us dictate the ordering in which to plot data
							</li>
							<li>
								<i>position = "stack"</i> shows the different factors on top of one another to see differences 
							</li>
							<li>
								weight argument based on specific section of your data let's you take into account the number of observations within a specific section of data
							</li>
						</ul>
					</p>
					<h6>Ecological Fallacy</h6>
					<p>
						Be careful about making assumtions at an individual level when it could be a trend that just occurs at the population level. We can soemtimes check this using logit transformations.to have the areas of the densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. 
					</p>
					<h6>Practice Problems: Exploring the Gapminder Dataset </h6>
					<ol class="disc">
						<li>
							<b><u>
								Using ggplot and the points layer, create a scatter plot of life expectancy versus fertility for the African continent in 2012.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>
								
								gapminder %>% filter(continent == "Africa" & year == "2012") %>%
								<div class="tab">
								  ggplot(aes(life_expectancy, fertility)) +<br>
								  geom_point()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q1.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Note that there is quite a bit of variability in life expectancy and fertility with some African countries having very high life expectancies. There also appear to be three clusters in the plot.
								Remake the plot from the previous exercises but this time use color to distinguish the different regions of Africa to see if this explains the clusters. Remember that you can explore the gapminder data to see how the regions of Africa are labeled in the data frame!
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>
								
								gapminder %>% filter(continent == "Africa" & year == "2012") %>%
								<div class="tab">
								  ggplot(aes(y= life_expectancy, x= fertility, color = region)) +<br>
								  geom_point()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q2.png" width="350"></center>
						</li>
						<li>
							<b><u>
								While many of the countries in the high life expectancy/low fertility cluster are from Northern Africa, three countries are not. 
								Create a table showing the country and region for the African countries (use select) that in 2012 had fertility rates of 3 or less and life expectancies of at least 70.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>


								df <- gapminder %>% filter(continent=="Africa", year == "2012", fertility <= 3, life_expectancy >=70) %>% select(country, region) %>% data.frame()
							</blockquote></div>
						</li>
						<li>
							<b><u>
								The Vietnam War lasted from 1955 to 1975. Do the data support war having a negative effect on life expectancy? We will create a time series plot that covers the period from 1960 to 2010 of life expectancy for Vietnam and the United States, using color to distinguish the two countries. In this start we start the analysis by generating a table.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>


								tab <- gapminder %>% filter(year %in% 1960:2010, country %in% c("Vietnam", "United States"))
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now that you have created the data table in Exercise 4, it is time to plot the data for the two countries.
							</u></b>
							<div id="testimonials"><blockquote>
								p <- tab %>% ggplot(aes(x=year,y=life_expectancy,color=country)) + 
								<div class="tab">
									geom_line()
								</div>
								p
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q5.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Cambodia was also involved in this conflict and, after the war, Pol Pot and his communist Khmer Rouge took control and ruled Cambodia from 1975 to 1979. He is considered one of the most brutal dictators in history. Does the data support this claim?
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>


								gapminder %>% filter(year %in% 1960:2010, country == "Cambodia") %>% 
								<div class="tab">
								  ggplot(aes(year, life_expectancy)) +<br>
								  geom_line()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q6.png" width="350"></center>
							Yes, it does.
						</li><br>
						<li>
							<b><u>
								Now we are going to calculate and plot dollars per day for African countries in 2010 using GDP data. In the first part of this analysis, we will create the dollars per day variable.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>
								daydollars <- gapminder %>% mutate(dollars_per_day = gdp/population/365) %>% filter(continent == "Africa", year == "2010", !is.na(dollars_per_day))


								daydollars
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now we are going to calculate and plot dollars per day for African countries in 2010 using GDP data. In the second part of this analysis, we will plot the smooth density plot using a log (base 2) x axis.
							</u></b>
							<div id="testimonials"><blockquote>
								daydollars %>% ggplot(aes(dollars_per_day)) +
								<div class="tab"> 
									geom_density() +<br>
									scale_x_continuous(trans = "log2")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q8.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now we are going to combine the plotting tools we have used in the past two exercises to create density plots for multiple years.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>


								gapminder %>% filter(continent == "Africa", year %in% c(1970,2010)) %>%
								<div class="tab">
									mutate(dollars_per_day = gdp/population/365) %>% <br>
									filter(!is.na(dollars_per_day)) %>% <br>
									ggplot(aes(dollars_per_day)) +<br>
									geom_density() + <br>
									scale_x_continuous(trans = "log2")  + <br>
									facet_grid(year~.)
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q9.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now we are going to edit the code from Exercise 9 to show a stacked density plot of each region in Africa.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>




								dat <- gapminder %>% filter(continent == "Africa", year %in% c(1970,2010)) %>% mutate(dollars_per_day = gdp/population/365) %>% 
									<div class="tab">
										filter(!is.na(dollars_per_day)) %>%<br>
										ggplot(aes(dollars_per_day, color = region, fill = region)) + <br>geom_density(bw=0.5, position = "stack" ) + <br>
										scale_x_continuous(trans = "log2")  + <br>
										facet_grid(year~.)
									</div>


								dat
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q10.png" width="350"></center>
						</li>
						<li>
							<b><u>
								We are going to continue looking at patterns in the gapminder dataset by plotting infant mortality rates versus dollars per day for African countries.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>

								gapminder_Africa_2010 <- gapminder %>% filter(continent == "Africa", year == "2010") %>% mutate(dollars_per_day = gdp/population/365) %>% filter(!is.na(dollars_per_day)) <br><br>

								# scatter plot<br>
								gapminder_Africa_2010 %>% ggplot(aes(dollars_per_day, infant_mortality, color = region)) + 
								<div class="tab">
									geom_point()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q11.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now we are going to transform the x axis of the plot from the previous exercise.
							</u></b>
							<div id="testimonials"><blockquote>
								gapminder_Africa_2010 %>% ggplot(aes(dollars_per_day, infant_mortality, color = region)) + 
								<div class="tab">
									geom_point() + <br>
									scale_x_continuous(trans = "log2")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q12.png" width="350"></center>
						</li>

						<li>
							<b><u>
								Note that there is a large variation in infant mortality and dollars per day among African countries. As an example, one country has infant mortality rates of less than 20 per 1000 and dollars per day of 16, while another country has infant mortality rates over 10% and dollars per day of about 1. In this exercise, we will remake the plot from Exercise 12 with country names instead of points so we can identify which countries are which.
							</u></b>
							<div id="testimonials"><blockquote>
								gapminder_Africa_2010 %>% ggplot(aes(dollars_per_day, infant_mortality, color = region,label = country)) + 
								<div class="tab">
									geom_point() +<br>
									scale_x_continuous(trans = "log2") +<br>
									geom_text()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q13.png" width="500"></center>
						</li>

						<li>
							<b><u>
								Now we are going to look at changes in the infant mortality and dollars per day patterns African countries between 1970 and 2010.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(gapminder)<br><br>


								africa_year_comp <- gapminder %>% 
								<div class="tab"> 
									filter(continent == "Africa", year %in% c(1970,2010)) %>% <br>
									mutate(dollars_per_day = gdp/population/365) %>% <br>
									filter(!is.na(dollars_per_day)& !is.na(infant_mortality)) 
								</div><br>


								africa_year_comp %>% ggplot(aes(dollars_per_day, infant_mortality, color = region,label = country)) + 
								<div class="tab">
									geom_point() + <br>
									scale_x_continuous(trans = "log2") +<br>
									geom_text() + facet_grid(year~.)
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/3.2_q14.png" width="500"></center>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Data Visualization Principles-->
			<li class="">
				<div class="title">
					<b>Data Visualization Principles</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Encoding Data Using Visual Cues</h6>
					<p>
						Some useful suggests:
						<ul class="disc">
							<li>Position, aligned lengths, angles, area, brightness, and colour hue</li>
							<li>Area and angles are a lot harder to read than length and position</li>
							<li>Know When to Include Zero: it can be dishonest not to start at 0</li>
							<li>Do Not Distort Quantities: when comparing differences better to make circle comparisons using radius proportions rather than area (ggplto automatically uses area)</li>
							<li>Order by a Meaningful Value: reorder()</li>
						</ul>
					</p>

					<h6>Practice Problems: Data Visualization Principles 1</h6>
					<ol class="disc">
						<li>
							<b><u>
								Pie charts are appropriate:
							</u></b><br>
							Never
						</li><br>
						<li>
							<b><u>
								What is the problem with this plot?
							</u></b><br>
							<center><img src="images/pages/stats/visualization/4.1_q2.png" width="300"></center>
							The axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when in fact it was about 30% more.
						</li><br>
						<li>
							<b><u>
								Take a look at the following two plots. They show the same information: rates of measles by state in the United States for 1928.
								<center><img src="images/pages/stats/visualization/4.1_q3.png" width="600"></center>
							</u></b><br>
							The plot on the right is better because it orders the states by disease rate so we can quickly see the states with highest and lowest rates.
						</li>
					</ol>
					<h6>Show the Data</h6>
					<p>
						Two features that can be added that are helpful to make your data more clear, especially if points are overlapped:
						<ul class="disc">
							<li>
								jitter() adds spaces between point
							</li>
							<li>
								Alpha blending stronger colour when more points overlapped
							</li>
						</ul>
						<center><img src="images/pages/stats/visualization/4.2_showdata1.png" width="350"></center>
					</p>
					<p>
						We can also put plots on a common axes to make ease of comparison. Align plots vertical to see horizontal changes and vice versa.
						<center><img src="images/pages/stats/visualization/4.2_showdata2.png" width="350"></center>
					</p>
					<p>
						Default colours in ggplot are usually hard for colour blind people, so you can select your own colours:
						<div id="testimonials"><blockquote>
							colour_blind_friendly_cols <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
						</blockquote></div>
						Add it with <i>+ scale_color_manual(values = colour_blind_friendly_cols)</i>.
					</p>


					<h6>Practice Problems: Data Visualization Principles 2</h6>
					<ol class="disc">
						<li>
							<b><u>
								To make the plot on the right in the exercise from the last set of assessments, we had to reorder the levels of the states' variables. Redefine the state object so that the levels are re-ordered by rate. Print the new object state and its levels (using levels) so you can see that the vector is now re-ordered by the levels.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								dat <- us_contagious_diseases %>%
								<div class="tab">
									filter(year == 1967 & disease=="Measles" & !is.na(population)) %>% <br>
									mutate(rate = count / population * 10000 * 52 / weeks_reporting)
								</div>
								state <- dat$state<br>
								state <- reorder(state, rate)<br>
								rate <- dat$count/(dat$population/10000)*(52/dat$weeks_reporting)<br><br>


								print(state)<br>
								levels(state)
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Now we are going to customize this plot a little more by creating a rate variable and reordering by that variable instead.<br>
								Add a single line of code to the definition of the dat table that uses mutate to reorder the states by the rate variable.<br>
								The sample code provided will then create a bar plot using the newly defined dat.<br>
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data(us_contagious_diseases)<br><br>

								dat <- us_contagious_diseases %>% 
								<div class="tab">
									filter(year == 1967 & disease=="Measles" & count>0 & !is.na(population)) %>%<br>
								  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %>% <br>mutate(state = reorder(state, rate))
								</div>
								dat %>% ggplot(aes(state, rate)) +
								<div class="tab">
								  geom_bar(stat="identity") +<br>
								  coord_flip()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.2_q2.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data("murders")<br><br>

								murders %>% mutate(rate = total/population*100000) %>%
								<div class="tab">
								  group_by(region) %>%<br>
								  summarize(avg = mean(rate)) %>%<br>
								  mutate(region = factor(region)) %>%<br>
								  ggplot(aes(region, avg)) +<br>
								  geom_bar(stat="identity") +<br>
								  ylab("Murder Rate Average")
								</div>

							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.2_q3.png" width="350"></center>
							<b><u>and decide to move to a state in the western region. What is the main problem with this interpretation?</u></b><br>
							It does not show all the data. We do not see the variability within a region and it's possible that the safest states are not in the West.
						</li>	<br>
						<li>
							<b><u>
								To further investigate whether moving to the western region is a wise decision, let's make a box plot of murder rates by region, showing all points. Order the regions by their median murder rate by using mutate and reorder. Make a box plot of the murder rates by region. Show all of the points on the box plot.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								data("murders")<br><br>

								m <- murders %>% 
								<div class="tab">
									mutate(rate = total/population*100000) %>% <br>
									mutate(region = reorder(region, rate)) 
								</div>
								m %>% ggplot(aes(region, rate)) +
								<div class="tab">
									geom_boxplot() + geom_point()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.2_q4.png" width="350"></center>
						</li>	
					</ol>

					<h6>Slope Charts</h6>
					<p>
						We can make slope charts using geom_line:
						<div id="testimonials"><blockquote>
							west <- c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")<br>
							dat <- gapminder
							<div class="tab">
								filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population > 10^7)
							</div>
							dat %>% 
							<div class="tab">
								mutate(location = ifelse(year == 2010, 1, 2),<br>
								location = ifelse(year == 2015 & country %in% c("United Kingdom", "Portugal"), location +0.22, location),
								hjust = ifelse(year == 2010, 1, 0) %>% <br>
								mutate(year = as.factor(year)) %>% <br>
								ggplot(aes(year, life_expectancy, group = country)) +<br>
								geom_line(aes(color = country), show.legend = FALSE) + <br>
								geom_text(aes(x=location, label = country, hjust = hjust), show.legend = FALSE) + <br>
								xlab("") + <br>
								ylab("Life Expectancy")
						</blockquote></div>
						<center><img src="images/pages/stats/visualization/4.3_slopechart1.png" width="350"></center>
					</p>
					<p>
						Bland-Altman plot (aka Tukey Mean Difference, and MA): shows the difference and average between values:
						<center><img src="images/pages/stats/visualization/4.3_slopechart2.png" width="350"></center>
					</p>
					<h6>Encoding a Third Variable</h6>
					<p>
						Using shape() or color or hue, is very useful in getting through many different data points:
						<center><img src="images/pages/stats/visualization/4.3_3rdvar.png" width="350"></center>
						We can pick between many, but here are some popular ones:
						<center><img src="images/pages/stats/visualization/4.3_3rdvar2.png" width="250"></center>
						For colours - sequential palettes (library(RColorBrewer)) : used to go from low to high values 
						<center><img src="images/pages/stats/visualization/4.3_vaccine1.png" width="350"></center>
						Diverging palettes: used to represent colours that verge from a centre  - emphasis on both ends
						<center><img src="images/pages/stats/visualization/4.3_vaccine2.png" width="350"></center>
					</p>
					<h6>Case Study: Vaccines</h6>
					<p>
						You can look at data collected in the US using <i>data(data(us_contagious_diseases))</i>.
					</p>


					<h6>Practice Problems: Data Visualization Principles 3</h6>
					<ol class="disc">
						<li>
							<b><u>
								The sample code given creates a tile plot showing the rate of measles cases per population. We are going to modify the tile plot to look at smallpox cases instead.Modify the tile plot to show the rate of smallpox cases instead of measles cases. Exclude years in which cases were reported in fewer than 10 weeks from the plot.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(RColorBrewer)<br>
								library(dslabs)<br>
								data(us_contagious_diseases)<br><br>


								the_disease = "Smallpox"<br>
								dat <- us_contagious_diseases %>% 
									<div class="tab">
								   filter(!state%in%c("Hawaii","Alaska") & disease == the_disease & weeks_reporting>=10) %>% <br>
								   mutate(rate = count / population * 10000) %>% <br>
								   mutate(state = reorder(state, rate))
								 </div><br>


								dat %>% ggplot(aes(year, state, fill = rate)) + 
								<div class="tab">
								  geom_tile(color = "grey50") + <br>
								  scale_x_continuous(expand=c(0,0)) + <br>
								  scale_fill_gradientn(colors = brewer.pal(9, "Reds"), trans = "sqrt") + <br>
								  theme_minimal() + <br>
								  theme(panel.grid = element_blank()) + <br>
								  ggtitle(the_disease) + <br>
								  ylab("") + <br>
								  xlab("")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.3_q1.png" width="350"></center>
						</li>
						<li>
							<b><u>
								The sample code given creates a time series plot showing the rate of measles cases per population by state. We are going to again modify this plot to look at smallpox cases instead. Modify the sample code for the time series plot to plot data for smallpox instead of for measles. Once again, restrict the plot to years in which cases were reported in at least 10 weeks.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								library(RColorBrewer)<br>
								data(us_contagious_diseases)<br>
								head(us_contagious_diseases)<br><br>
								the_disease = "Smallpox"<br>

								dat <- us_contagious_diseases %>%
								<div class="tab">
								  filter(!state%in%c("Hawaii","Alaska") & disease == the_disease & weeks_reporting>=10) %>%<br>
								  mutate(rate = count / population * 10000) %>%<br>
								  mutate(state = reorder(state, rate))
								</div><br>


								avg <- us_contagious_diseases %>%
								<div class="tab">
								  filter(disease==the_disease) %>% group_by(year) %>%<br>
								  summarize(us_rate = sum(count, na.rm=TRUE)/sum(population, na.rm=TRUE)*10000)
								</div>


								dat %>% ggplot() +
								<div class="tab">
								  geom_line(aes(year, rate, group = state),  color = "grey50", 
								            show.legend = FALSE, alpha = 0.2, size = 1) +<br>
								  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1, color = "black") +<br>
								  scale_y_continuous(trans = "sqrt", breaks = c(5,25,125,300)) + <br>
								  ggtitle("Cases per 10,000 by state") + <br>
								  xlab("") + <br>
								  ylab("") +<br>
								  geom_text(data = data.frame(x=1955, y=50), mapping = aes(x, y, label="US average"), color="black") + <br>
								  geom_vline(xintercept=1963, col = "blue")
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.3_q2.png" width="350"></center>
						</li>
						<li>
							<b><u>
								Now we are going to look at the rates of all diseases in one state. Again, you will be modifying the sample code to produce the desired plot. For the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease. Include your aes function inside of ggplot rather than inside your geom layer.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								library(RColorBrewer)<br>
								data(us_contagious_diseases)<br><br>


								us_contagious_diseases %>% filter(state=="California" & weeks_reporting>=10) %>% 
								<div class="tab">
								  group_by(year, disease) %>%<br>
								  summarize(rate = sum(count)/sum(population)*10000) %>%<br>
								  ggplot(aes(year, rate, color = disease)) + <br>
								  geom_line()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.3_q3.png" width="350"></center>
						</li>	
						<li>
							<b><u>
								Now we are going to make a time series plot for the rates of all diseases in the United States. For this exercise, we have provided less sample code - you can take a look at the previous exercise to get you started. Compute the US rate by using summarize to sum over states. Call the variable rate. The US rate for each disease will be the total number of cases divided by the total population. Remember to convert to cases per 10,000. You will need to filter for !is.na(population) to get all the data. Plot each disease in a different color.
							</u></b>
							<div id="testimonials"><blockquote>
								library(dplyr)<br>
								library(ggplot2)<br>
								library(dslabs)<br>
								library(RColorBrewer)<br>
								data(us_contagious_diseases)<br><br>


								us_contagious_diseases %>% filter(!is.na(population)) %>% 
								<div class="tab">
								  group_by(year, disease) %>%<br>
								  summarize(rate = sum(count)/sum(population)*10000) %>%<br>
								  ggplot(aes(year, rate, color = disease)) + <br>
								  geom_line()
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/visualization/4.3_q4.png" width="350"></center>
						</li>	
					</ol>
				</div>
			</li>

		<!-- Accordion: Assessment: Titanic Survival  -->
			<li class="">
				<div class="title">
					<b>Assessment: Titanic Survival</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6><a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/visualization" class="saymore">Practice Problems</a></h6>
					<p>
						Find R code on GitHub.
					</p>
				</div>
			</li>

		</ul>

	</div>

	<div class="hr">
	</div>



	<!-- Probability -->
	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>Prob
			</div>
		</div>
		<h4 id="prob">Probability</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-probability" class="saymore">here</a>. Within this course we look at probability theory.
		</p>

		<!-- Accordion -->
		<ul class="accordion">

		<!-- Accordion: 1.1 Intro to Discrete -->
			<li class="">
				<div class="title">
					<b>Intro to Discrete</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Discrete Probability</h6>
					<p>
						In probability, we talk about <u>events</u>. An event is something that occurs by chance. The <u>probability of an event</u> is the proportion of times the event occurs when we repeat the experiment over and over, independently and under the same conditions. <u>Discrete probability</u> is when there is a finite number of outcomes.
					</p>
					<h6>Monte Carlo Simulations</h6>
					<p>
						A Monte Carlo simulation is a simulation in which an experiment is run a certain amount of times to determine the experimental probability of our event occurring.
					</p>
					<p>

					</p>
					<h6>Probability Distribution</h6>
					<p>
						The probability of outcomes is drawn as a distribution (can be discrete or continuous random variable). This shows the proportion of each outcome occurring.
					</p>
					<h6>Independence</h6>
					<p>
						Two events are independent if the outcome of one does not affect the outcome of the other.
					</p>
					<p>
						We know that the conditional probability between two events will not be affected:
						
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?</b></u>
							<p>
								<b><i>Mathematically:</b></i>
							</p>
							<p>
								3/(3+5+7) = 0.2
							</p>
							<p>
								<b><i>R:</b></i>
							</p>
							<div id="testimonials"><blockquote>
								cyan <- 3<br>
								magenta <- 5<br>
								yellow <- 7<br><br>


								# Assign a variable `p` as the probability of choosing a cyan ball from the box<br>
								p <- cyan/sum(cyan, magenta, yellow)<br><br>


								# Print the variable `p` to the console<br>
								p<br>
								## 0.2
							</blockquote></div>
						</li>
						<li>
							<b><u>One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will not be cyan? </b></u>
							<p>
								<b><i>Mathematically:</b></i>
							</p>
							<p>
								1 - cyan (0.2) = 0.8 
							</p>
							<p>
								<b><i>R:</b></i>
							</p>
							<div id="testimonials"><blockquote>
								# `p` is defined as the probability of choosing a cyan ball from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls.<br>
								# Using variable `p`, calculate the probability of choosing any ball that is not cyan from the box<br>
								1- p<br>
								##0.8

							</blockquote></div>
						</li>
						<li>
							<b><u>Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? Provide at least 3 significant digits.</b></u>
							<p>
								<b><i>Mathematically:</b></i>
							</p>
							<p>
								(cyan/total)(not cyan/total-1) = (3/15)(12/14) = (6/35) = 0.171428571
							</p>
							<p>
								<b><i>R:</b></i>
							</p>
							<div id="testimonials"><blockquote>
								cyan <- 3<br>
								magenta <- 5<br>
								yellow <- 7<br><br>


								# The variable `p_1` is the probability of choosing a cyan ball from the box on the first draw.<br>
								p_1 <- cyan / (cyan + magenta + yellow)<br><br>


								# Assign a variable `p_2` as the probability of not choosing a cyan ball on the second draw without replacement.<br>
								p_2 <- sum(magenta, yellow)/ (cyan + magenta + yellow - 1)<br><br>


								# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.<br>
								p_1*p_2<br>
								## 0.1714286<br>

							</blockquote></div>
						</li>
						<li>
							<b><u>Now repeat the experiment, but this time, after taking the first draw and recording the color, return it back to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan? </b></u>
							<p>
								<b><i>Mathematically:</b></i>
							</p>
							<p>
								(cyan/total)(not cyan/total) = (3/15)(12/15) = (6/35) = 0.16
							</p>
							<p>
								<b><i>R:</b></i>
							</p>
							<div id="testimonials"><blockquote>
								cyan <- 3<br>
								magenta <- 5<br>
								yellow <- 7<br><br>


								# The variable 'p_1' is the probability of choosing a cyan ball from the box on the first draw.<br>
								p_1 <- cyan / (cyan + magenta + yellow)<br><br>


								# Assign a variable 'p_2' as the probability of not choosing a cyan ball on the second draw with replacement.<br>
								p_2 <- sum(magenta, yellow) / (cyan + magenta + yellow)<br><br>


								# Calculate the probability that the first draw is cyan and the second draw is not cyan using `p_1` and `p_2`.<br>
								p_1*p_2<br>
								## 0.16
							</blockquote></div>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 1.2 Combination and Permutation -->
			<li class="">
				<div class="title">
					<b>Combination and Permutation</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Setting up card deck</h6>
					<p>
						In order to set up a card deck you can use two different functions: <i>paste()</i> and <i>expand.grid()</i>.
					</p>
					<p>
						<i>paste()</i> is used to put two strings together. In the case of the deck, we want to group numbers 1-10 and J,Q,K with suits - heart, diamond, spade, clover. An example of how we could use paste: 
						<div id="testimonials"><blockquote>
							paste(letters[1:5], as.character(1:5))
						</blockquote></div>
						This would return <i>a1 b2 c3 d4 e5</i>
					</p>
					<p>
						<i>expand.grid()</i> gives a combinations of all items listed between two lists. This would allow us to match every suit with a number. An example of this would be:
						<div id="testimonials"><blockquote>
							expand.grid(pants=c(“blue”, “black”), shirt = c(“white”, “grey”, “plaid”)) 
						</blockquote></div>
						This would return 
						<table>
							<tr>
								<th>pants</th>
								<th>shirt</th>
							</tr>
							<tr>
								<td>blue</td>
								<td>white</td>
							</tr>
							<tr>
								<td>black</td>
								<td>white</td>
							</tr>
							<tr>
								<td>blue</td>
								<td>grey</td>
							</tr>
							<tr>
								<td>black</td>
								<td>grey</td>
							</tr>
							<tr>
								<td>blue</td>
								<td>plaid</td>
							</tr>
							<tr>
								<td>black</td>
								<td>plaid</td>
							</tr>
						</table>
					</p>
					<p>
						Now to apply it to our cards:
						<div id="testimonials"><blockquote>
							Suits = c( list all )<br>
							Numbers = c( list all as strings )<br>
							Deck <- expand.grid( number = numbers, suit=suits) %>% paste(number, suit)
						</blockquote></div>
					</p>
					<p>
						Now we can check our probabilities:
						<div id="testimonials"><blockquote>
							kings <- paster(“King”, suits”)<br>
							mean(deck %in% kings)<br>
						</blockquote></div>
					</p>
					<h6>gtools package</h6>
					<p>
						This package is very useful because it has combinations and permutations operations. <b>Combinations</b> are elements from a subset where order does not matter - selecting "r" items from "n" (i.e. 1 then 3 same as 3 then 1). <b>Permutations</b> are combinations of items where order matters (i.e. 1 then 3 different from 3 then 1). We can create all the different 2 card hands from a deck of cards using permutations:
						<div id="testimonials"><blockquote>
							hands <- permutations(52, 2, v= deck)
						</blockquote></div>
						Then we can select the first and second hand by specifying location:
						<div id="testimonials"><blockquote>
							first_card <- hands[,1]<br>
							second_card <- hands[,2]
						</blockquote></div>
					</p>
					<h6>Checking for duplicates</h6>
					<p>
						<i>duplicated()</i> checks if an element of a vector has already appeared in that vector; returns true or false. Then we can check if there exists any duplicates in a list: <i>any(duplicated())</i> (which will return true if there are any TRUEs listed). 
					</p>
					<p>
						Note: Often for loops are less preferred in R; usually prefer operations on entire vectors. <i>sapply()</i> allows applying an operation to every single element of a vector.
					</p>
					<h6>How many Monte Carlo Experiments are Enough?</h6>
					<p>
						When we want to run simulations it is important to get the right number of Monte Carlo sims otherwise the results won't be correct. In order to determine a "good enough" amount of sims, we want to check the stability of the experiment. You do this by looking at varying sizes in your sample and graph it to see where stabilizing might occur:
						<center><img src="images/pages/stats/1.2_monte-stabilize.png" width="400"></center>
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>Imagine you draw two balls from a box containing colored balls. You either replace the first ball before you draw the second or you leave the first ball out of the box when you draw the second ball. Under which situation are the two draws independent of one another?</u></b><br>
							Replace
						</li>
						<li>
							<b><u>Say you’ve drawn 5 balls from a box that has 3 cyan balls, 5 magenta balls, and 7 yellow balls, with replacement, and all have been yellow. What is the probability that the next one is yellow? </u></b>
							<div id="testimonials"><blockquote>
								cyan <- 3<br>
								magenta <- 5<br>
								yellow <- 7<br><br>


								# Assign the variable 'p_yellow' as the probability that a yellow ball is drawn from the box.<br>
								p_yellow <- yellow/sum(cyan, magenta, yellow)<br><br>

								# Using the variable 'p_yellow', calculate the probability of drawing a yellow ball on the sixth draw. Print this value to the console.<br>
								P_yellow<br>
								## 0.4666667
							</blockquote></div>
						</li>
						<li>
							<b><u>If you roll a 6-sided die once, what is the probability of not seeing a 6? If you roll a 6-sided die six times, what is the probability of not seeing a 6 on any of those rolls?</u></b>
							<div id="testimonials"><blockquote>
								# Assign the variable 'p_no6' as the probability of not seeing a 6 on a single roll. <br>
								p_no6 <- 5/6<br><br>

								# Calculate the probability of not seeing a 6 on six rolls using `p_no6`. Print your result to the console: do not assign it to a variable.<br>
								p_no6^6<br>
								## 0.334898
							</blockquote></div>
						</li>
						<li>
							<b><u>Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game? Remember that the Celtics must win one of the first four games, or the series will be over!</u></b>
							<div id="testimonials"><blockquote>
								# Assign the variable `p_cavs_win4` as the probability that the Cavs will win the first four games of the series.<br>
								p_cavs_win4 <- (.6)^4<br><br>

								# Using the variable `p_cavs_win4`, calculate the probability that the Celtics win at least one game in the first four games of the series.<br>
								1 - p_cavs_win4<br>
								## 0.8704
							</blockquote></div>
						</li>
						<li>
							<b><u>Create a Monte Carlo simulation to confirm your answer to the previous problem by estimating how frequently the Celtics win at least 1 of 4 games. Use B <- 10000 simulations. The provided sample code simulates a single series of four random games, simulated_games.</u></b>
							<div id="testimonials"><blockquote>
								# This line of example code simulates four independent random games where the Celtics either lose or win. Copy this example code to use within the `replicate` function.<br>
								simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))<br>
								# The variable 'B' specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.<br>
								B <- 10000<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.<br>
								set.seed(1)<br>


								# Create an object called `celtic_wins` that replicates two steps for B iterations: (1) generating a random four-game series `simulated_games` using the example code, then (2) determining whether the simulated series contains at least one win for the Celtics.<br>
								# celtic_wins <- replicate(B, any(simulated_games=="win"))<br>
								celtic_wins <- replicate(B, any({simulated_games <- sample(c("lose","win"), 4, replace = TRUE, prob = c(0.6, 0.4))}=="win"))<br><br>


								# Calculate the frequency out of B iterations that the Celtics won at least one game. Print your answer to the console.<br>
								mean(celtic_wins)<br>
								## 0.8757
							</blockquote></div>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 1.3 Addition Rule and Monty Hall -->
			<li class="">
				<div class="title">
					<b>Addition Rule and Monty Hall</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Addition rule (A or B) : P(A or B) = P(A) + P(B) - P(A and B)
					</p>
					<p>
						Monty Hall problem is a counter-intuitive stats problem that gives you are choice of 3 doors (one of which has a prize). After you pick a door one of the other doors opens to reveal no prize - the question is do you switch doors from your intial pick?
					</p>
					<p>
						When you work of the statistics behind it - you should switch your choice due to an increase knowledge.
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games wins the series. The teams are equally good, so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'n' as the number of remaining games.<br>
								n <- 6<br><br>

								# Assign a variable `outcomes` as a vector of possible game outcomes, where 0 indicates a loss and 1 indicates a win for the Cavs.<br>
								outcomes <- c(0,1)<br><br>

								# Assign a variable `l` to a list of all possible outcomes in all remaining games. Use the `rep` function on `list(outcomes)` to create list of length `n`.<br>
								l <- rep(list(outcomes), n)<br><br>

								# Create a data frame named 'possibilities' that contains all combinations of possible outcomes for the remaining games.<br>
								possibilities <- expand.grid(l)<br><br>

								# Create a vector named 'results' that indicates whether each row in the data frame 'possibilities' contains enough wins for the Cavs to win the series.<br>
								results <- rowSums(possibilities) >=4<br><br>

								# Calculate the proportion of 'results' in which the Cavs win the series. Print the outcome to the console.<br>
								mean(results)<br>
								## 0.34375
							</blockquote></div>
						</li>
						<li>
							<b><u>Confirm the results of the previous question with a Monte Carlo simulation to estimate the probability of the Cavs winning the series after losing the first game.</u></b>
							<div id="testimonials"><blockquote>
								# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.<br>
								B <- 10000<br><br>

								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.<br>
								set.seed(1) <br><br>

								# Create an object called `results` that replicates for `B` iterations a simulated series and determines whether that series contains at least four wins for the Cavs.<br>
								results <- replicate(B, {sum(sample(c(0,1), 6, replace = TRUE))>=4})<br><br>

								# Calculate the frequency out of `B` iterations that the Cavs won at least four games in the remainder of the series. Print your answer to the console.<br>
								mean(results)<br>
								## 0.3453
							</blockquote></div>
						</li>
						<li>
							<b><u>Two teams A and B are playing a seven series game series. Team A is better than team B and has a p> 0.5 chance of winning each game.</u></b>
							<div id="testimonials"><blockquote>
								# Let's assign the variable 'p' as the vector of probabilities that team A will win.<br>
								p <- seq(0.5, 0.95, 0.025)<br>

								# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:<br>
								prob_win <- function(p){
									<div class="tab">
								  	B <- 10000<br>
								  result <- replicate(B, {
								  	<div class="tab">
								    b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))<br>
								    sum(b_win)>=4
								  </div>
								  })<br>
								  mean(result)
								</div>
								}<br><br>

								# Apply the 'prob_win' function across the vector of probabilities that team A will win to determine the probability that team B will win. Call this object 'Pr'.<br>
								Pr <- sapply(p, prob_win)<br><br>

								# Plot the probability 'p' on the x-axis and 'Pr' on the y-axis.<br>
								plot(p, Pr)
							</blockquote></div>
							<center><img src="images/pages/stats/1.3_addition-q3.png" width="400"></center>
						</li>
						<li>
							<b><u>Repeat the previous exercise, but now keep the probability that team A wins fixed at p <- 0.75 and compute the probability for different series lengths. For example, wins in best of 1 game, 3 games, 5 games, and so on through a series that lasts 25 games.</u></b>
							<div id="testimonials"><blockquote>
								# Given a value 'p', the probability of winning the series for the underdog team B can be computed with the following function based on a Monte Carlo simulation:<br>
								prob_win <- function(N, p=0.75){
									<div class="tab">
								    B <- 10000<br>
								    result <- replicate(B, {
								    <div class="tab">
								      b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))<br>
								      sum(b_win)>=(N+1)/2<br>
								    </div>
								    })<br>
								    mean(result)
								  </div>
								  }<br><br>

								# Assign the variable 'N' as the vector of series lengths. Use only odd numbers ranging from 1 to 25 games.<br>
								N <- seq(from=1, to=25, by=2)<br><br>

								# Apply the 'prob_win' function across the vector of series lengths to determine the probability that team B will win. Call this object `Pr`.<br>
								Pr <- sapply(N, prob_win)<br><br>

								# Plot the number of games in the series 'N' on the x-axis and 'Pr' on the y-axis.<br>
								plot(N, Pr)
							</blockquote></div>
							<center><img src="images/pages/stats/1.3_addition-q4.png" width="400"></center>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 1.4 Discrete Probability Assessment -->
			<li class="">
				<div class="title">
					<b>Discrete Probability Assessment</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6><a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/probability_exercises" class="saymore">Practice Problems</a></h6>
					<p>
						Find R code on GitHub.
					</p>
				</div>
			</li>

		<!-- Accordion: 2.1 Continuous Probability  -->
			<li class="">
				<div class="title">
					<b>Continuous Probability </b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Continuous probability</h6>
					<p>
						Continuous probability functions operate on intervals rather than singular values. We have difference functions: eCDF (empirical cumulative distribution function) and CDF (cumulative distribution function) - which is a basic summary of a list of numerical values. They represent the proportion/count of observations falling below a specific value; the eCDF is an estimator of the CDF
						Create function: function(a) mean(x>=a). The CDF defines a probability distribution for our variables.
					</p>
					<h6>Theoretical Distribution</h6>
					<p>
						Approximation to the normal distribution (pnorm()) pnorm(a, avg, s) is very useful if we can use but it is only used for continuous data.
					</p>
					<p>
						Data scientist use data which technically speaking is discrete; so we look at intervals of probabilities instead of exact values. This allows us to use the normal approximation (with large enough data set).
					</p>
					<p>
						Discretization: using continuous distribution (by way of intervals) even though the data is technically discrete
					</p>
					<h6>Probability Density</h6>
					<center><img src="images/pages/stats/2.1_prob-density.png" width="300"></center>
					<p>
						You get the probability density function for the normal distribution using <i>dnorm(z, mu, sigma)</i>. This allow us to fit models to data for which predefined functions are not available.
					</p>
					<p>
						We can use <i>dnorm()</i> to plot the density curve for the normal distribution. <i>dnorm(z)</i> gives the probability density f(z)  of a certain z-score, so we can draw a curve by calculating the density over a range of possible values of z.
					</p>
					<p>
						Since we know 99.7% of observations will be within -3 <= z <= 3, we can use a value of z slightly larger than 3 and this will cover most likely values of the normal distribution. Then, we calculate f(z), which is <i>dnorm()</i> of the series of z-scores. Last, we plot z against f(z).
						<div id="testimonials"><blockquote>
							# to use %>% we must import tidyverse<br>
							library(tidyverse)<br><br>
							x <- seq(-4, 4, length = 100)<br>
							# creating dataframe for the normal distribution for z values between -4 and 4, then plotting it<br>
							data.frame(x, f = dnorm(x)) %>%
							<div class="tab">
						    ggplot(aes(x, f)) + <br>
						    geom_line()
						  </div>
						</blockquote></div>
					</p>
					<h6>Monte Carlo Simulation</h6>
					<p>
						How to generate Monte Carlo simulations: use <i>rnorm()</i> which takes size, average (default 0), standard deviation (default 1)
					</p>
					<p>
					<div id="testimonials"><blockquote>
						x <- heights <br>
						n <- length(x)<br>
						avg <- mean(x)<br>
						s <- sd(x)<br>
						simulated _heights <- rnorm(n, avg, s)
					</blockquote></div>
						This generates normally generated data
					</p>
					<h6>Other continuous distribution</h6>
					<ul class="disc">
						<li>Norm (norm), student -t (t), chi-squared, exponential, gamma, and beta distributions</li>
						<li>R provides functions to computer: density (dblah), quantiles (qblah), probability density function (pblah), and random (rblah),  cumulative distribution</li>
					</ul>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'female_avg' as the average female height.<br>
								female_avg <- 64<br><br>

								# Assign a variable 'female_sd' as the standard deviation for female heights.<br>
								female_sd <- 3<br><br>

								# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is shorter than 5 feet. Print this value to the console.<br>
								pnorm(60, female_avg, female_sd)<br>
								## 0.09121122
							</blockquote></div>
						</li>
						<li>
							<b><u>Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'female_avg' as the average female height.<br>
								female_avg <- 64<br><br>

								# Assign a variable 'female_sd' as the standard deviation for female heights.<br>
								female_sd <- 3<br><br>

								# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is 6 feet or taller. Print this value to the console.<br>
								1- pnorm(72, female_avg, female_sd)<br>
								## 0.003830381
							</blockquote></div>
						</li>
						<li>
							<b><u>Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'female_avg' as the average female height.<br>
								female_avg <- 64<br><br>

								# Assign a variable 'female_sd' as the standard deviation for female heights.<br>
								female_sd <- 3<br><br>

								# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.<br>
								pnorm(67, female_avg, female_sd) - pnorm(61, female_avg, female_sd)<br>
								## 0.6826895
							</blockquote></div>
						</li>
						<li>
							<b><u>Repeat the previous exercise, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'female_avg' as the average female height. Convert this value to centimeters.<br>
								female_avg <- 64*2.54<br><br>

								# Assign a variable 'female_sd' as the standard deviation for female heights. Convert this value to centimeters.<br>
								female_sd <- 3*2.54<br><br>

								# Using variables 'female_avg' and 'female_sd', calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.<br>
								pnorm(67*2.54, female_avg, female_sd) - pnorm(61*2.54, female_avg, female_sd)<br>
								## 0.6826895
							</blockquote></div>
						</li>
						<li>
							<b><u>Compute the probability that the height of a randomly chosen female is within 1 SD from the average height.</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'female_avg' as the average female height.<br>
								female_avg <- 64<br><br>

								# Assign a variable 'female_sd' as the standard deviation for female heights.<br>
								female_sd <- 3<br><br>

								# To a variable named 'taller', assign the value of a height that is one SD taller than average.<br>
								taller <- female_avg + female_sd<br><br>

								# To a variable named 'shorter', assign the value of a height that is one SD shorter than average.<br>
								shorter <- female_avg - female_sd<br><br>

								# Calculate the probability that a randomly selected female is between the desired height range. Print this value to the console.<br>
								pnorm(taller, female_avg, female_sd) - pnorm(shorter, female_avg, female_sd)<br>
								## 0.6826895
							</blockquote></div>
						</li>
						<li>
							<b><u>Imagine the distribution of male adults is approximately normal with an average of 69 inches and a standard deviation of 3 inches. How tall is a male in the 99th percentile?</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable 'male_avg' as the average male height.<br>
								male_avg <- 69<br><br>

								# Assign a variable 'male_sd' as the standard deviation for male heights.<br>
								male_sd <- 3<br><br>

								# Determine the height of a man in the 99th percentile of the distribution.<br>
								qnorm(.99, male_avg, male_sd)<br>
								## 75.97904
							</blockquote></div>
						</li>
						<li>
							<b><u>The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the person with the highest IQ in your school district, where 10,000 people are born each year. Generate 10,000 IQ scores 1,000 times using a Monte Carlo simulation. Make a histogram of the highest IQ scores.</u></b>
							<div id="testimonials"><blockquote>
								# The variable `B` specifies the number of times we want the simulation to run.<br>
								B <- 1000<br><br>

								# Use the `set.seed` function to make sure your answer matches the expected result after random number generation.<br>
								set.seed(1)<br><br>

								avg <- 100<br>
								s <- 15<br><br>

								# Create an object called `highestIQ` that contains the highest IQ score from each random distribution of 10,000 people.<br>
								highestIQ <- replicate(B, {max(rnorm(10000, avg, s))}) <br><br>

								# Make a histogram of the highest IQ scores.<br>
								hist(highestIQ)<br>
							</blockquote></div>
							<center><img src="images/pages/stats/2.1_hist-q7.png" width="550"></center>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 2.2 Continuous Probability Assessment  -->
			<li class="">
				<div class="title">
					<b>Continuous Probability Assessment </b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6><a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/probability_exercises" class="saymore">Practice Problems</a></h6>
					<p>
						Find R code on GitHub.
					</p>
				</div>
			</li>

		<!-- Accordion: 3.1 Random Variables and Sampling Models -->
			<li class="">
				<div class="title">
					<b>Random Variables and Sampling Models</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Random Variable</h6>
					<p>
						A random variable is a variable that has different outcomes left to chance.
					</p>
					<p>
						We can check the distribution of our data by doing the following code:
					<div id="testimonials"><blockquote>
						<b># first we sequence the data</b><br>
						s <- seq(min(data), max(data), length = 100)<br><br>
						<b># next we want to create a data frame that has the normal distribution from our data set:</b><br>
						Normal_density <- data.frame(s = s, f = dnorm(s, mean(data), sd(data)))<br><br>
						<b># finally, we can use our original data to create a histogram with a line graph representing the distribution if it was a normal distribution. This will help us see if our distribution is approximately normal.</b><br>
						data.frame(data=data) %>% ggplot(aes(data, ..density..)) + 
						<div class="tab">
							geom_histogram(color = “black”, binwidth = 10) + <br>
							ylab(“Probability”) + <br>
							geom_line(data = normal_density, mapping=aes(s, f), color= “blue”)
						</div>
					</blockquote></div>
					<h6>Sampling Models</h6>
					<p>
						Sampling models create ways to randomly sample populations to answer questions. We want to model random behaviour of a population with sample draws.
					</p>
					<ul class="disc">
						The following functions help us sample our data:
						<li><i>rep(c(“B”, “R”, “G”), c(18, 18, 2)))</i> repeats each one the amount of times specified </li>
						<li><i>R.V. X <- sample(ifelse (color == “Red”, -1, 1), n, replace = True)</i> OR if you know the probability: <i>X <- sample(c( -1, 1), n, replace = True, prob=c(9/19, 10/19))</i> </li>
					</ul>
					<h6>Distributions vs. Probability Distributions</h6>
					<p>
						Distribution of a list of numbers: what proportion of the list is less than or equal to a value. The probability distribution of a random variable is probability of the observed value falling in any given interval. Probability distribution: what is the probability that X is less than or equal to a; with expected value and standard error
					</p>
					<p>
						The average of a random sample is called the expected value and the standard deviation is call the standard error of the random variable. 
						<div id="testimonials"><blockquote>
							avg <- sum(x)/length(x)<br>
							sd <- sqrt(sum(x-avg)^2)/length(x)
						</blockquote></div>
					</p>
					<h6>Notation for Random Variables</h6>
					<p>
						Capital letters are for RV and lower case are for observed values.
					</p>
					<h6>CLT</h6>
					<p>
						The Central Limit Theorem states that when the sample size is large, the sum of the independent trials is approximately normal. This is a big result because if it is approximately normal we only need the mean and sd to have the distribution.
					</p>
					<p>
						Where we have:
						<ul class="disc">
							<li>
								<i>E[X] = mu</i> (expected value = mean): number of draws x avg of numbers in urn
							</li>
							<li>
								Standard Error: <i>SE[X] = sqrt(number of draws) x sd</i> OR <i>|b - a|sqrt(p(1-p))</i> to give us the range of possibilities.
							</li>
						</ul>
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>
								An American roulette wheel has 18 red, 18 black, and 2 green pockets. Each red and black pocket is associated with a number from 1 to 36. The two remaining green slots feature "0" and "00". Players place bets on which pocket they think a ball will land in after the wheel is spun. Players can bet on a specific number (0, 00, 1-36) or color (red, black, or green).
								What are the chances that the ball lands in a green pocket?
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables `green`, `black`, and `red` contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green/sum(green, black, red)<br><br>

								# Print the variable `p_green` to the console<br>
								p_green<br>
								## 0.05263158
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In American roulette, the payout for winning on green is $17. This means that if you bet $1 and it lands on green, you get $17 as a prize.
								Create a model to predict your winnings from betting on green one time.
							</u></b>
							<div id="testimonials"><blockquote>
								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.<br>
								set.seed(1)<br><br>


								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1 - p_green<br><br>


								# Create a model to predict the random variable `X`, your winnings from betting on green. Sample one time.<br>
								X <- sample(c(17, -1), 1, replace = TRUE, prob=c(p_green, p_not_green))<br><br>


								# Print the value of `X` to the console<br>
								X<br>
								## -1
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In American roulette, the payout for winning on green is $17. This means that if you bet $1 and it lands on green, you get $17 as a prize.In the previous exercise, you created a model to predict your winnings from betting on green.
								Now, compute the expected value of X, the random variable you generated previously.
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Calculate the expected outcome if you win $17 if the ball lands on green and you lose $1 if the ball doesn't land on green<br>
								17*p_green + (-1)*p_not_green<br>
								## -0.05263158
							</blockquote></div>
						</li>
						<li>
							<b><u>
								The standard error of a random variable X tells us the difference between a random variable and its expected value. You calculated a random variable X in exercise 2 and the expected value of that random variable in exercise 3.
								Now, compute the standard error of that random variable, which represents a single outcome after one spin of the roulette wheel.
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Compute the standard error of the random variable<br>
								abs(-1-17)*sqrt(p_green*p_not_green)<br>
								## 4.019344
							</blockquote></div>
						</li>
						<li>
							<b><u>
								You modeled the outcome of a single spin of the roulette wheel, X, in exercise 2. Now create a random variable S that sums your winnings after betting on green 1,000 times.
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling<br>
								set.seed(1)<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 1000<br><br>


								# Create a vector called 'X' that contains the outcomes of 1000 samples<br>
								X <- sample(c(17, -1), n, replace = TRUE, prob=c(p_green, p_not_green))<br><br>


								# Assign the sum of all 1000 outcomes to the variable 'S'<br>
								S <- sum(X)<br><br>


								# Print the value of 'S' to the console<br>
								S<br>
								## -10
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In the previous exercise, you generated a vector of random outcomes, X, after betting on green 1,000 times. What is the expected value of S?
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 1000<br><br>


								# Calculate the expected outcome of 1,000 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green<br>
								X <- sample(c(17, -1), n, replace = TRUE, prob=c(p_green, p_not_green))<br>
								E_S <- n*(17*p_green + -1*p_not_green)<br>
								E_S <br>
								## -52.63158
							</blockquote></div>
						</li>
						<li>
							<b><u>
								You generated the expected value of S, the outcomes of 1,000 bets that the ball lands in the green pocket, in the previous exercise. What is the standard error of S?
							</u></b>
							<div id="testimonials"><blockquote>
								# The variables 'green', 'black', and 'red' contain the number of pockets for each color<br>
								green <- 2<br>
								black <- 18<br>
								red <- 18<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- green / (green+black+red)<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 1000<br><br>


								# Compute the standard error of the sum of 1,000 outcomes<br>
								se_S <- sqrt(n)*(abs(-1-17)*sqrt(p_green*p_not_green))<br>
								se_S<br>
								## 127.1028
							</blockquote></div>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 3.2 The Central Limit Theorem Continued -->
			<li class="">
				<div class="title">
					<b>The Central Limit Theorem Continued</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Averages and Proportions</h6>
					<p>
						Rules for expected value and standard error:
						<ul class="disc">
							<li>
								The expected value of the sum of RV is the sum of the <i>E(X)</i> of the individual RV
							</li>
							<li>
								<i>E(X)</i> of RV times constant is the <i>c*E(X)</i>
							</li>
							<li>
								<i>SE^2</i> of the sum of independent RV is the sum of the square of the SE of each random variable
							</li>
							<li>
								<i>SE</i> of RV times a constant is the <i>c*SE(X)</i>
							</li>
							<li>
								<i>SE(X) = sigma/sqrt(n)</i>
							</li>
							<li>
								<i>X~ norm</i> then <i>aX +b ~norm</i> also
							</li>
						</ul>
					</p>
					<h6>Law of Large Numbers</h6>
					<p>
						When <i>n</i> is large enough then the mean converges to the true <i>E(X)</i>.
					</p>
					<h6>How Large is Large in CLT</h6>
					<p>
						30  is a good rule of thumb, but highly dependent on your data.
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>
								The exercises in the previous chapter explored winnings in American roulette. In this chapter of exercises, we will continue with the roulette example and add in the Central Limit Theorem.
								In the previous chapter of exercises, you created a random variable  S that is the sum of your winnings after betting on green a number of times in American Roulette. What is the probability that you end up winning money if you bet on green 100 times?
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- 2 / 38<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 100<br><br>


								# Calculate 'avg', the expected outcome of 100 spins if you win $17 when the ball lands on green and you lose $1 when the ball doesn't land on green<br>
								avg <- n * (17*p_green + -1*p_not_green)<br><br>


								# Compute 'se', the standard error of the sum of 100 outcomes<br>
								se <- sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)<br><br>


								# Using the expected value 'avg' and standard error 'se', compute the probability that you win money betting on green 100 times.<br>
								1- pnorm(0, avg, se)<br>
								## 0.4479091
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Create a Monte Carlo simulation that generates 10,000 outcomes of S, the sum of 100 bets. 
								Compute the average and standard deviation of the resulting list and compare them to the expected value (-5.263158) and standard error (40.19344) for S that you calculated previously.
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- 2 / 38<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1-p_green<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 100<br><br>


								# The variable `B` specifies the number of times we want the simulation to run. Let's run the Monte Carlo simulation 10,000 times.<br>
								B <- 10000<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.<br>
								set.seed(1)<br><br>


								# Create an object called `S` that replicates the sample code for `B` iterations and sums the outcomes.<br>
								S <- replicate(B, {
									<div class="tab">
								    X <- sample(c(17, -1), n, replace = TRUE, prob=c(p_green, p_not_green))<br>
								    sum(X)
								  </div>
								})<br><br>




								# Compute the average value for 'S'<br>
								mean(S)<br>
								## 40.30608<br><br>


								# Calculate the standard deviation of 'S'<br>
								sd(S)<br>
								## -5.9086

							</blockquote></div>
						</li>
						<li>
							<b><u>
								In this chapter, you calculated the probability of winning money in American roulette using the CLT. 
								Now, calculate the probability of winning money from the Monte Carlo simulation. The Monte Carlo simulation from the previous exercise has already been pre-run for you, resulting in the variable S that contains a list of 10,000 simulated outcomes.
							</u></b>
							<div id="testimonials"><blockquote>
								# Calculate the proportion of outcomes in the vector `S` that exceed $0
								mean(S>0)<br>
								## 0.4232
							</blockquote></div>
						</li>
						<li>
							<b><u>
								The Monte Carlo result and the CLT approximation for the probability of losing money after 100 bets are close, but not that close. What could account for this?
							</u></b><br>
							The CLT does not work as well when the probability of success is small.<br>
						</li>
						<li>
							<b><u>
								Now create a random variable Y that contains your average winnings per bet after betting on green 10,000 times.
							</u></b>
							<div id="testimonials"><blockquote>
								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling.<br>
								set.seed(1)<br><br>


								# Define the number of bets using the variable 'n'<br>
								n <- 10000<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- 2 / 38<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1 - p_green<br><br>


								# Create a vector called `X` that contains the outcomes of `n` bets<br>
								X <- sample(c(17, -1), n, replace=TRUE, prob=c(p_green, p_not_green))<br><br>


								# Define a variable `Y` that contains the mean outcome per bet. Print this mean to the console.<br>
								Y <- mean(X)<br>
								Y<br>
								## 0.008
							</blockquote></div>
						</li>
						<li>
							<b><u>
								What is the expected value of Y, the average outcome per bet after betting on green 10,000 times?
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- 2 / 38<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1 - p_green<br><br>


								# Calculate the expected outcome of `Y`, the mean outcome per bet in 10,000 bets<br>
								E_Y <- (17*p_green + (-1)*p_not_green)<br>
								E_Y<br>
								## -0.05263158
							</blockquote></div>
						</li>
						<li>
							<b><u>
								What is the standard error of Y, the average result of 10,000 spins?
							</u></b>
							<div id="testimonials"><blockquote>
								# Define the number of bets using the variable 'n'<br>
								n <- 10000<br><br>


								# Assign a variable `p_green` as the probability of the ball landing in a green pocket<br>
								p_green <- 2 / 38<br><br>


								# Assign a variable `p_not_green` as the probability of the ball not landing in a green pocket<br>
								p_not_green <- 1 - p_green<br><br>


								# Compute the standard error of 'Y', the mean outcome per bet from 10,000 bets.<br>
								SE_Y <- abs(-1-17)*sqrt(p_green*p_not_green)/sqrt(10000)<br>
								SE_Y<br>
								## 0.04019344
							</blockquote></div>
						</li>
						<li>
							<b><u>
								What is the probability that your winnings are positive after betting on green 10,000 times?
							</u></b>
							<div id="testimonials"><blockquote>
								# We defined the average using the following code<br>
								avg <- 17*p_green + -1*p_not_green<br><br>


								# We defined standard error using this equation<br>
								se <- 1/sqrt(n) * (17 - -1)*sqrt(p_green*p_not_green)<br><br>


								# Given this average and standard error, determine the probability of winning more than $0. Print the result to the console.<br>
								1 - pnorm(0, avg, se)<br>
								## 0.0951898
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Create a Monte Carlo simulation that generates 10,000 outcomes of S, the average outcome from 10,000 bets on green. 
								Compute the average and standard deviation of the resulting list to confirm the results from previous exercises using the Central Limit Theorem.
							</u></b>
							<div id="testimonials"><blockquote>
								# The variable `n` specifies the number of independent bets on green<br>
								n <- 10000<br><br>


								# The variable `B` specifies the number of times we want the simulation to run<br>
								B <- 10000<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random number generation<br>
								set.seed(1)<br><br>


								# Generate a vector `S` that contains the the average outcomes of 10,000 bets modeled 10,000 times<br>
								S <- replicate(B, {
									<div class="tab">
								    X <- sample(c(17,-1), n, replace=TRUE, prob=c(p_green, p_not_green))<br>
								    mean(X)
								  </div>
								})<br><br>




								# Compute the average of `S`<br>
								mean(S)<br>
								## 0.03996168<br><br>


								# Compute the standard deviation of `S`<br>
								sd(S)<br>
								## -0.05223142
							</blockquote></div>
						</li>
						<li>
							<b><u>
								In a previous exercise, you found the probability of winning more than $0 after betting on green 10,000 times using the Central Limit Theorem. Then, you used a Monte Carlo simulation to model the average result of betting on green 10,000 times over 10,000 simulated series of bets.
								What is the probability of winning more than $0 as estimated by your Monte Carlo simulation? The code to generate the vector S that contains the the average outcomes of 10,000 bets modeled 10,000 times has already been run for you.
							</u></b>
							<div id="testimonials"><blockquote>
								# Compute the proportion of outcomes in the vector 'S' where you won more than $0<br>
								mean(S>0)<br>
								## 0.0977
							</blockquote></div>
						</li>
						<li>
							<b><u>
								The Monte Carlo result and the CLT approximation are now much closer than when we calculated the probability of winning for 100 bets on green. What could account for this difference?
							</u></b><br>
							The CLT works better when the sample size is larger.
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: 3.3 Assessment: Random Variables, Sampling Models, and the Central Limit Theorem -->
			<li class="">
				<div class="title">
					<b>Assessment: Random Variables, Sampling Models, and the Central Limit Theorem</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6><a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/probability_exercises" class="saymore">Practice Problems</a></h6>
					<p>
						Find R code on GitHub.
					</p>
				</div>
			</li>

		<!-- Accordion: 4.1 The Big Short -->
			<li class="">
				<div class="title">
					<b>The Big Short</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Interest Rates Explained</h6>
					<p>
						Banks want to cover for the amount of people the predict will not pay back the loan. This is why they have different interest rates. Due to CLT, since our losses are a sum of independent draws we have <i>E(X) = n*(p*loss + q*noloss)</i> and <i>sd(X) = √(n) *abs(loss - noloss)*√(p*q)</i>. Now, if our expected value is zero, it means we breakeven. This leads to out equation of <i>loss*p +x*q = 0</i>
					</p>
					<p>
						Next, we want to take into account that we want a 99% confidence interval (meaning 99% of the time we breakeven). We do this by getting <i>P(X<0) = 0.01</i>. When we standardize this, we get <i>P(Z < -n*(loss*p + x*q/abs(x-loss)sqrt(n*p*q)) =0.01</i>. Using <i>qnorm()</i>, we can see that: <i>qnorm(0.01) = -2.32</i>. This means we are looking for our z-value to be -2.32.
					</p>
					<p>
						This gives us the equation: <i>-n*(loss*p + x*q/abs(x-loss)sqrt(n*p*q) = -2.32</i> which will determine our values needed.
					</p>
					<h6>Big Short</h6>
					<p>
						The reason the big short happened was it assumed that the chance of defaulting on high risk mortgage was independent from person to person. However, when there is a global event (recession) many people are impacted and the chances of defaulting are not 50-50. Financial experts assumed independence when there was none.
					</p>
					<h6>Practice Problems</h6>
					<ol>
						<li>
							<b><u>
								Say you manage a bank that gives out 10,000 loans. The default rate is 0.03 and you lose $200,000 in each foreclosure. 
								Create a random variable S that contains the earnings of your bank. Calculate the total amount of money lost in this scenario.
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the number of loans to the variable `n`<br>
								n <- 10000<br><br>


								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling<br>
								set.seed(1)<br><br>


								# Generate a vector called `defaults` that contains the default outcomes of `n` loans<br>
								defaults <- sample(c(0,1), n, replace=TRUE, prob = c(1-p_default, p_default))<br><br>


								# Generate `S`, the total amount of money lost across all foreclosures. Print the value to the console.<br>
								S <- sum(defaults)*loss_per_foreclosure<br>
								S <br>
								## -6.3e+07
							</blockquote></div>
						</li>
						<li>
							<b><u>
								Run a Monte Carlo simulation with 10,000 outcomes for S, the sum of losses over 10,000 loans. Make a histogram of the results.
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the number of loans to the variable `n`<br>
								n <- 10000<br><br>


								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Use the `set.seed` function to make sure your answer matches the expected result after random sampling<br>
								set.seed(1)<br><br>


								# The variable `B` specifies the number of times we want the simulation to run<br>
								B <- 10000<br><br>


								# Generate a list of summed losses 'S'. Replicate the code from the previous exercise over 'B' iterations to generate a list of summed losses for 'n' loans.  Ignore any warnings for now.<br>
								S <- replicate(B, {
									<div class="tab">
								    X <- sample(c(0,1), n, replace=TRUE, prob = c(1-p_default, p_default))<br>
								    sum(X)*loss_per_foreclosure
								  </div>
								})<br><br>




								# Plot a histogram of 'S'.  Ignore any warnings for now.<br>
								hist(S)
							</blockquote></div>
							<center><img src="images/pages/stats/4.1_histo1.png" width="300"></center>
						</li>
						<li>
							<b><u>
								What is the expected value of S, the sum of losses over 10,000 loans? For now, assume a bank makes no money if the loan is paid.
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the number of loans to the variable `n`<br>
								n <- 10000<br><br>


								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Calculate the expected loss due to default out of 10,000 loans<br>
								n*p_default*loss_per_foreclosure<br>
								## -6e+07
							</blockquote></div>
						</li>
						<li>
							<b><u>
								 What is the standard error of S?
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the number of loans to the variable `n`<br>
								n <- 10000<br><br>


								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Compute the standard error of the sum of 10,000 loans<br>
								abs(loss_per_foreclosure)*sqrt(n*p_default*(1-p_default))<br>
								## 3411744
							</blockquote></div>
						</li>
						<li>
							<b><u>
								So far, we've been assuming that we make no money when people pay their loans and we lose a lot of money when people default on their loans. Assume we give out loans for $180,000. How much money do we need to make when people pay their loans so that our net loss is $0? 
								In other words, what interest rate do we need to charge in order to not lose money?
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Assign a variable `x` as the total amount necessary to have an expected outcome of $0<br>
								#  lp + x(1-p) = 0<br>
								x <- -p_default*loss_per_foreclosure/(1-p_default)<br><br>




								# Convert `x` to a rate, given that the loan amount is $180,000. Print this value to the console.<br>
								x/180000<br>
								## 0.03436426
							</blockquote></div>
						</li>
						<li>
							<b><u>
								With the interest rate calculated in the last example, we still lose money 50% of the time. What should the interest rate be so that the chance of losing money is 1 in 20? 
								In math notation, what should the interest rate be so that P(S<0) = 0.05
								<center><img src="images/pages/stats/4.1_prob-loss.png" width="200"></center>
								Let z = qnorm(0.05) give us the value of z for which P(Z < z)=0.05;
								<center><img src="images/pages/stats/4.1_x-value.png" width="200"></center>
							</u></b>
							<div id="testimonials"><blockquote>
								# Assign the number of loans to the variable `n`<br>
								n <- 10000<br><br>


								# Assign the loss per foreclosure to the variable `loss_per_foreclosure`<br>
								loss_per_foreclosure <- -200000<br><br>


								# Assign the probability of default to the variable `p_default`<br>
								p_default <- 0.03<br><br>


								# Generate a variable `z` using the `qnorm` function<br>
								z <- qnorm(0.05)<br><br>


								# Generate a variable `x` using `z`, `p_default`, `loss_per_foreclosure`, and `n`<br>
								x <- -loss_per_foreclosure*( ( n*p_default - z*sqrt(n*p_default*(1-p_default)) ) / (n*(1-p_default) + z*sqrt(n*p_default*(1-p_default))) )<br><br>




								# Convert `x` to an interest rate, given that the loan amount is $180,000. Print this value to the console.<br>
								x/180000<br>
								## 0.03768738

							</blockquote></div>
						</li>
						<li>
							<b><u>
								 The bank wants to minimize the probability of losing money. Which of the following achieves their goal without making interest rates go up?
							</u></b><br>
							A reduced default rate
						</li>
					</ol>
				</div>
			</li>

		</ul>

	</div>

	<div class="hr">
	</div>

	<!-- Inference and Modeling -->
	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>Inference
			</div>
		</div>
		<h4 id="inference">Inference and Modeling</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-inference-and-modeling" class="saymore">here</a>. Within this course we look at how to model data and draw inferences from our samples. This section will be updated soon!
		</p>
		<p>
			
		</p>
		<!-- Accordion -->
		<ul class="accordion">

		<!-- Accordion: Parameters and Estimates -->
			<li class="">
				<div class="title">
					<b>Parameters and Estimates</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Central Limit Theorem in Practice -->
			<li class="">
				<div class="title">
					<b>Central Limit Theorem in Practice</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Confidence Intervals and p-Values -->
			<li class="">
				<div class="title">
					<b>Confidence Intervals and p-Values</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Statistical Models -->
			<li class="">
				<div class="title">
					<b>Statistical Models</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Bayesian Statistics  -->
			<li class="">
				<div class="title">
					<b>Bayesian Statistics</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Election Forecasting  -->
			<li class="">
				<div class="title">
					<b>Election Forecasting</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		<!-- Accordion: Association Tests -->
			<li class="">
				<div class="title">
					<b>Association Tests</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		</ul>

	</div>

	<div class="hr">
	</div>




	<!-- Linear Regression -->
	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>Regression
			</div>
		</div>
		<h4 id="linreg">Linear Regression</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-linear-regression" class="saymore">here</a>. Within this course we look at linear models and how to understand relationships between various factors within an analysis. 
		</p>
		<!-- Accordion -->
		<ul class="accordion">

		<!-- Accordion: Baseball as a Motivating Example-->
			<li class="">
				<div class="title">
					<b>Baseball as a Motivating Example</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Baseball Basics</h6>
					<p>
						Some information to know about baseball and how things are denoted in this section are:
						<ul class="disc">
							<li>9 innings with 3 outs per team per inning</li>
							<li>There is a set order for 9 batters</li>
							<li>Bases form a cycle</li>
							<li>
								There are 6 ways to succeed:
								<ol>
									<li>BB (base on balls aka "walk")</li>
									<li>SB (stolen bases)</li>
									<li>1B (single base run)</li>
									<li>2B (double bases run)</li>
									<li>3B (triple bases run)</li>
									<li>HR (home run)</li>
								</ol>
							</li>
							<li>We define a hit with any of the four outcomes (1B, 2B, 3B, HR) and at bat (AB) to have just two outcomes (get a hit or out)</li>
							<li>PA (stands for plate appearance): up to bat; has a binary outcome - either they are out or not out.</li>
							<li>Batting average has been considered for a long time the most important offensive stat. Where batting average is defined as H/AB but it ignores BB.</li>
							<li>The statistics analyzed are team-level because players are not independent from one another.</li>
						</ul>
					</p>
					<p>
						We can make a scatterplot of the home runs vs runs per game to futher see a relationship by:
						<div id="testimonials"><blockquote>
							library(Lahman)<br><br>

							ds_theme_set()<br>
							Teams %>% filter(yearID %in% 1961:2001) %>% 
							<div class="tab"> 
								mutate(HR_per_game = HR/G, R_per_game = R/G) %>% <br>
								ggplot(aes(HR_per_game, R_per_game)) +<br>
								geom_point(alpha=0.5)
							</div>
						</blockquote></div>
					</p>
					<h6>Assessment: Baseball as a Motivating Example</h6>
					<ol>
						<li>
							<b><u>
								What is the application of statistics and data science to baseball called?
							</u></b><br>
							Sabermetrics
						</li><br>
						<li>
							<b><u>
								Which of the following outcomes is not included in the batting average?
							</u></b><br>
							A base on balls
						</li><br>
						<li>
							<b><u>
								Why do we consider team statistics as well as individual player statistics?
							</u></b><br>
							The success of any individual player also depends on the strength of their team.
						</li><br>
						<li>
							<b><u>
								You want to know whether teams with more at-bats per game have more runs per game. What R code below correctly makes a scatter plot for this relationship?
							</u></b>
							<div id="testimonials"><blockquote>
								Teams %>% filter(yearID %in% 1961:2001 ) %>%
								<div class="tab">
							    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%<br>
							    ggplot(aes(AB_per_game, R_per_game)) + <br>
							    geom_point(alpha = 0.5)
								</div>
							</blockquote></div>
						</li>
						<li>
							<b><u>
								What does the variable “SOA” stand for in the Teams table?
							</u></b><br>
							Strikeouts by pitchers
						</li><br>
						<li>
							<b><u>
								Load the Lahman library. Filter the Teams data frame to include years from 1961 to 2001. Make a scatterplot of runs per game versus at bats (AB) per game.
							</u></b>
							<div id="testimonials"><blockquote>
								Teams %>% filter(yearID %in% 1961:2001 ) %>%
								<div class="tab">
							    mutate(AB_per_game = AB/G, R_per_game = R/G) %>%<br>
							    ggplot(aes(AB_per_game, R_per_game)) + <br>
							    geom_point(alpha = 0.5)
								</div>
							</blockquote></div>
							
							<center><img src="images/pages/stats/linreg/lr_1-q6.png" width="200"></center>
							As the number of at bats per game increases, the number of runs per game tends to increase.
						</li><br>
						<li>
							<b><u>
								Use the filtered Teams data frame from Question 6. Make a scatterplot of win rate (number of wins per game) versus number of fielding errors (E) per game.
								Which of the following is true?
							</u></b>
							<div id="testimonials"><blockquote>
								Teams %>% filter(yearID %in% 1961:2001 ) %>%
								<div class="tab">
							    mutate(W_per_game = W/G, E_per_game = E/G) %>%<br>
							    ggplot(aes(W_per_game, E_per_game)) + <br>
							    geom_point(alpha = 0.5)
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/linreg/lr_1-q7.png" width="200"></center>
							As the number of errors per game increases, the win rate tends to decrease.
						</li><br>
						<li>
							<b><u>
								Use the filtered Teams data frame from Question 6. Make a scatterplot of triples (X3B) per game versus doubles (X2B) per game. 
								Which of the following is true?
							</u></b>
							<div id="testimonials"><blockquote>
								Teams %>% filter(yearID %in% 1961:2001 ) %>%
								<div class="tab">
							    mutate(X3_per_game = X3B/G, X2_per_game = X2B/G) %>%<br>
							    ggplot(aes(X3_per_game, X2_per_game)) + <br>
							    geom_point(alpha = 0.5)
								</div>
							</blockquote></div>
							<center><img src="images/pages/stats/linreg/lr_1-q8.png" width="200"></center>
							There is no clear relationship between doubles per game and triples per game.
						</li>
					</ol>
					<p>
						<a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/linear_regression" class="saymore">RStudio on GitHub: 1.1_BaseballExample_Assessment.R</a>
					</p>
				</div>
			</li>

		<!-- Accordion: Correlation -->
			<li class="">
				<div class="title">
					<b>Correlation</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Correlation</h6>
					<p>
						For univariate data, each observation includes just one value. However, in most data anaylsis cases we will not be just working with one variable.
					</p>
					<p>
						For instance, we can look at the library <i>HistData</i> that contains heights of people by gender and familial connections. Here is how to filter for fathers and sons:
						<div id="testimonials"><blockquote>
							library(HistData)><br>
							data("GaltonFamilies")<br>
							set.seed(1983)<br><br>

							galton_heights <- GaltonFamilies %>%
							<div class="tab">
								filter(gender == "male") %>%<br>
								group_by(family) %>%<br>
								sample_n(1) %>%<br>
								ungroup() %>%<br>
								select(father, childHeight) %>%<br>
								rename(son = childHeight) %>%
							</div>
						</blockquote></div>
					</p>
					<p>
						Usually, correlation coefficients summarize the trend effectively.
					</p>

					<h6>Correlation Coefficient</h6>
					<p>
						We usually refer to the correlation coefficient with the Greek letter rho. The equation is:
						<center><img src="images/pages/stats/linreg/lr_2-rho.png" width="200"></center>
						Here we have xi and yi being two different variable values from the data set. 
					</p>
					<p>
						If xi and yi are unrelated then the product will be positive as often as it is negative, giving us 0.
					</p>
					<p>
						The correlation coefficient range is -1 to 1. The value telling you how strong the relationship and the positive/negative telling you the directionaility of the relationship.
					</p>

					<h6>Sample Correlation is a Random Variable</h6>
					<p>
						Sample correlation is the most commonly used estimate of the population correlation. When the CLT is applied we can have the following distribution:
						<center><img src="images/pages/stats/linreg/lr_2-rho-CLT.png" width="150"></center>
					</p>

					<h6>Assessment: Correlation</h6>
					<ol>
						<li>
							<b><u>
								While studying heredity, Francis Galton developed what important statistical concept?
							</u></b><br>
							Correlation
						</li><br>
						<li>
							<b><u>
								The correlation coefficient is a summary of what?
							</u></b><br>
							The trend between two variables
						</li><br>
						<li>
							<b><u>
								Below is a scatter plot showing the relationship between two variables, x and y. From this figure, the correlation between x and y appears to be about:
							</u></b>
							<center><img src="images/pages/stats/linreg/lr_2-q3.png" width="300"></center>
							-0.9
						</li><br>
						<li>
							<b><u>
								Instead of running a Monte Carlo simulation with a sample size of 25 from the 179 father-son pairs described in the videos, imagine we now run the simulation with a sample size of 50. Note: You do not need to run any code to determine the answer to this exercise. 
								Would you expect the mean of the sample correlation to increase, decrease, or stay approximately the same?
							</u></b><br>
							Stay approximately the same; Because the expected value of the sample correlation is the population correlation, it should stay approximately the same even if the sample size is increased.
						</li><br>
						<li>
							<b><u>
								Instead of running a Monte Carlo simulation with a sample size of 25 from the 179 father-son pairs described in the videos, imagine we now run the simulation with a sample size of 50. Note: You do not need to run any code to determine the answer to this exercise. 
								Would you expect the standard deviation of the sample correlation to increase, decrease, or stay approximately the same?
							</u></b><br>
							Decrease; As the sample size N increases, the standard deviation of the sample correlation should decrease.
						</li>
						<li>
							<b><u>
								If X and Y are completely independent, what do you expect the value of the correlation coefficient to be?
							</u></b><br>
							0
						</li><br>
						<li>
							<a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/linear_regression" class="saymore">RStudio on GitHub: 1.2_Correlation_assement.R</a>
						</li>
						<li>
							<a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/linear_regression" class="saymore">RStudio on GitHub: 1.2_Correlation_assement.R</a>
						</li>
					</ol>
				</div>
			</li>

		<!-- Accordion: Stratification and Variance Explained -->
			<li class="">
				<div class="title">
					<b>Stratification and Variance Explained</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Anscombe's Quartet</h6>
					<p>
						Although correlation is sometimes a good summary, it isn't always. An example of why it is not always the best is called Anscombe’s Quartet and it has four data sets that have near identical simple descriptive statistics:
						<center><img src="images/pages/stats/linreg/lr_3-a-quartet.png" width="300"></center>
						All of the above have rho of 0.816, but as can be see are very different!
					</p>

					<h6>Stratification</h6>
					<p>
						Correlation is only meaningful in a particular context, so we use stratification. Stratifying lets you take a conditional average: looking at a specific subgroup of your data. The following gives us an equation for regression:
						<center><img src="images/pages/stats/linreg/lr_3-regress-eqn.png" width="200"></center>
						This equation lets us say if there is a perfect correlation than the increase for both is by their sd.
					</p>
					<p>We can model both the slope:
						<center><img src="images/pages/stats/linreg/lr_3-slope.png" width="100"></center>
						and the intercept:
						<center><img src="images/pages/stats/linreg/lr_3-intercept.png" width="150"></center>
					</p>
					<p>
						<i>Fun fact:</i> it is called regression because the son “regresses” to the average height as it is not a perfect correlation. In general, we see heights regress to mediocrity.
					</p>

					<h6>Bivariate Normal Distribution</h6>
					<p>
						Looking at scatterplots we can also see if there is correlation:
						<center><img src="images/pages/stats/linreg/lr_3-scatter-corr.png" width="200"></center>
					<p>
						If X ~Norm() and Y~Norm(). Then we can say, for any stratum (X=x), Y ~ Norm() in the stratum and the pair (Y, x) is approximately bivariate normal; we call is the conditional distribution of Y (Y | X = x).
					</p>
					<p>
						If there are 3 or more variables in which each pair is bivariate normal, we say it follows a multivariate normal distribution (i.e. jointly normal).
					</p>
					<p>
						Under the stratum, we can get the expected value given by this formula:
						<center><img src="images/pages/stats/linreg/lr_3-exp-strat.png" width="275"></center>
						and variance explained:
						<center><img src="images/pages/stats/linreg/lr_3-var-strat.png" width="250"></center>
					</p>
					<p>
						Some things to note:
						<ul class="disc">
							<li>Regression lines based on strata matter. </li>
							<li>E(Y|X=x) cannot be rearranged to find the regression line going the other way, but rather E(X|Y=y) must be calculated separately</li>
						</ul>
					</p>


					<h6>Assessment: </h6>
					<ol>
						<li>
							<b><u>
								The slope of the regression line in this figure is equal to what, in words?
							</u></b>
							<center><img src="images/pages/stats/linreg/lr_3-q1.png" width="250"></center>
							Slope = (correlation coefficient of son and father heights) * (standard deviation of sons’ heights / standard deviation of fathers’ heights)
						</li><br>
						<li>
							<b><u>
								Why does the regression line simplify to a line with intercept zero and slope rho when we standardize our x and y variables?
							</u></b><br>
							When we standardize variables, both x and y will have a mean of zero and a standard deviation of one. When you substitute this into the formula for the regression line, the terms cancel out until we have the following equation: yi=rho*xi
						</li><br>
						<li>
							<b><u>
								What is a limitation of calculating conditional means?
							</u></b><br>
							Each stratum we condition on (e.g., a specific father’s height) may not have many data points.<br>
							Because there are limited data points for each stratum, our average values have large standard errors.<br>
							Conditional means are less stable than a regression line.
						</li><br>
						<li>
							<b><u>
								A regression line is the best prediction of Y given we know the value of X when
							</u></b><br>
							X and Y follow a bivariate normal distribution.
						</li><br>
						<li>
							<b><u>
								Which one of the following scatterplots depicts an x and y distribution that is NOT well-approximated by the bivariate normal distribution?
							</u></b>
							<center><img src="images/pages/stats/linreg/lr_3-q5.png" width="250"></center>
							The v-shaped distribution of points from the first plot means that the x and y variables do not follow a bivariate normal distribution.<br>
							When a pair of random variables is approximated by a bivariate normal, the scatter plot looks like an oval (as in the 2nd, 3rd, and 4th plots) - it is okay if the oval is very round (as in the 3rd plot) or long and thin (as in the 4th plot).
						</li>
						<li>
							<b><u>
								We previously calculated that the correlation coefficient rho between fathers’ and sons’ heights is 0.5. Given this, what percent of the variation in sons’ heights is explained by fathers’ heights?
							</u></b><br>
							25%<br><br>
							When two variables follow a bivariate normal distribution, the variation explained can be calculated as 100*rho^2.
						</li><br>
						<li>
							<b><u>
								Suppose the correlation between father and son’s height is 0.5, the standard deviation of fathers’ heights is 2 inches, and the standard deviation of sons’ heights is 3 inches. Given a one inch increase in a father’s height, what is the predicted change in the son’s height?
								</u></b><br>
								rho = 0.5<br>
								sd_f = 2<br>
								sd_s = 3<br>
								f = +1<br>
								s = ?<br>
								3/2*0.5 = 0.75<br><br>


						 		The slope of the regression line is calculated by multiplying the correlation coefficient by the ratio of the standard deviation of son heights and standard deviation of father heights: sigma_son/sigma_father.
						</li>
						
					</ol><br>
					<p>
						<a href="https://github.com/anjawu/edx-harvard-ds-courses/tree/main/linear_regression" class="saymore">RStudio on GitHub questions 8-11: 1.3_Stratification-VarianceExplained_assessment.R</a>
					</p>
				</div>
			</li>

		<!-- Accordion: Introduction to Linear Models -->
			<li class="">
				<div class="title">
					<b>Introduction to Linear Models</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Using the <i>tidy()</i>, <i>glance()</i>, and <i>augment()</i> functions from the broom package will be helpful in data analysis.
					</p>

					<h6>Confounding: Are BBs More Predictive?</h6>
					<p>
						A really important fact is that <b>association is not causation</b>!
					</p>
					<p>
						In baseball, there is a confounding variable for BB and HR:
						<center><img src="images/pages/stats/linreg/lr_4-bb-confound.png" width="300"></center>
						BB are confound with HR.
					</p>
					<p>
						In real-life this happens because pitchers sometimes tend to avoid throwing strikes to home run hitters leading to more home run hitters having more bases on balls.
					</p>
					<p>
						Regression can help use adjust the confounding we see to see if there is still an affect.
					</p>

					<h6>Stratification and Multivariate Regression</h6>
					<p>
						First approach we can take is to keep HR fixed and then examine relationship between R and BB. After stratifying we can check if there is an effect. Here we can get the expected value:
						<center><img src="images/pages/stats/linreg/lr_4-exp-r.png" width="400"></center>
						Where beta is the slope by strata.
					</p>

					<h6>Linear Models</h6>
					<p>
						Second, we can look at a linear model with regression. Where a linear model is a model that describes the relationship between two or more variables. And regression allows us to find relationships between two variables while adjusting for others. This is particularly useful when we are not able to randomly assign groups in treatment vs. control.
					</p>
					<p>
						For our Galton height example, we can predict any son's height (Yi) from this equation:
						<center><img src="images/pages/stats/linreg/lr_4-Yi-son.png" width="300"></center>
						The epsilon is there as an error buffer for accounting for things like mother’s genetic effect, environmental factors, and other biological randomness. Epsilon assume independence E(X) =0 and sd not dependent on i (same for each individual). 
					</p>
					<p>
						Linear models are meant to be interpretable. So we can go back and change our equation to make it a bit more interpretable: 
						<center><img src="images/pages/stats/linreg/lr_4-Yi-son-new.png" width="350"></center>
						This would give us the beta_0 is the predicted height for the son of the average father.
					</p>

					<h6>Assessment: </h6>
					<ol>
						<li>
							<b><u>
								As described in the videos, when we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened?
							</u></b><br>
							The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.
						</li><br>
						<li>
							<b><u>
								We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results:
							</u></b>
							<div id="testimonials"><blockquote>
								> lm(son ~ father, data = galton_heights)<br><br>


								Call:<br>
								lm(formula = son ~ father, data = galton_heights)<br><br>


								Coefficients:<br>
								(Intercept)    father  <br>
								    35.71       0.50  
							</blockquote></div>
							<b><u>Interpret the numeric coefficient for "father."</u></b><br>
							For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches.
						</li><br>
						<li>
							<b><u>
								We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero.
							</u></b>
							<div id="testimonials"><blockquote>
								galton_heights <- galton_heights %>%
								<div class="tab">
							    mutate(father_centered=father - mean(father))
							  </div>
							</blockquote></div>
						  <b><u>We run a linear model using this centered fathers’ height variable.</u></b>							
							<div id="testimonials"><blockquote>
								> lm(son ~ father, data = galton_heights)<br><br>


								Call:<br>
								lm(formula = son ~ father, data = galton_heights)<br><br>


								Coefficients:<br>
								(Intercept)    father  <br>
								    70.45        0.50  
							</blockquote></div>
							<b><u>Interpret the numeric coefficient for the intercept.</u></b><br>
							The height of a son of a father of average height is 70.45 inches.
						</li><br>
						<li>
							<b><u>
								Suppose we fit a multivariable regression model for expected runs based on BB and HR:
							</u></b>
							<center><img src="images/pages/stats/linreg/lr_4-Yi-son-new.png" width="350"></center>
							<b><u>
								Suppose we fix BB = x_1. Then we observe a linear relationship between runs and HR with intercept of:
							</u></b><br>
							If x_1 is fixed, then Beta_1*x_1 is fixed and acts as the intercept for this regression model. This is the basis of stratification.
						</li><br>
						<li>
							<b><u>
								Which of the following are assumptions for the errors epsilon_i  in a linear regression model?
							</u></b><br>
							The epsilon_i  are independent of each other<br>
							The  epsilon_i  have expected value 0<br>
							The variance of epsilon_i  is a constant
						</li><br>
					</ol>
				</div>
			</li>

		<!-- Accordion: Least Squares Estimates  -->
			<li class="">
				<div class="title">
					<b>Least Squares Estimates <i>(LSE)</i></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Least Squares Estimates (LSE)</h6>
					<p>
						For linear models to be useful we need to estimate the unknown parameters. When looking for parameters, we also want to find the parameters that minimize the residual sum of squares:
						<center><img src="images/pages/stats/linreg/lr_5-rss.png" width="200"></center>
					</p>
					<p>
						The best way to minimize the RSS, is to use partial derivatives, set them equal to zero and find the minimums.
					</p>
					<p>
						A useful fact to know is that beta and SE(beta) both have t-distribution with N- p (number of parameters in model) degrees of freedom.
					</p>
					<p>
						In order to create a linear model prediction we can use the <i>lm()</i> function. Where lm( value predicting ~ value used for prediction ). Also do not forget that summary(): gives more information; estimate, se, tvalue, p-values (Pr(>|t|).
					</p>
					<p>
						An example of calculating LSE for a repeated experiment would be:
						<div id="testimonials"><blockquote>
							lse %>% summarize(cor(beta_0, beta_1))<br>
							B <- 1000<br>
							N <- 50<br>
							lse <- replicate(B, {
								<div class="tab"> 
						      sample_n(galton_heights, N, replace = TRUE) %>%<br>
						      mutate(father = father - mean(father)) %>%<br>
						      lm(son ~ father, data = .) %>% .$coef 
						    </div>
							})<br>
							cor(lse[1,], lse[2,]) 
						</blockquote></div>
					</p>

					<h6>Predicted Variables are Random Variables</h6>
					<p>
						Plugging estimates into our model gives our predicted values <i>predict()</i>. Then from there we can construct a confidence interval. Using ggplot2: 
						<div id="testimonials"><blockquote>
							geom_smooth(method = “lm”)
						</blockquote></div>
						we plot the CI around predicted Y.
					</p>


					<h6>Assessment: Least Squares Estimates</h6>
					<p>
						<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/linear_regression/2.2_LSE_assessment.R" class="saymore">RStudio on GitHub: 2.2_LSE_assessment.R</a>
					</p>
				</div>
			</li>

		<!-- Accordion: Advanced dplyr: summarize with functions and broom   -->
			<li class="">
				<div class="title">
					<b>Advanced dplyr: summarize with functions and broom</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Important to know: <i>lm()</i> ignores the <i>group_by()</i> because <i>lm()</i> is not part of <i>tidyverse</i>. So we can work around it by: 
						<div id="testimonials"><blockquote>
							dat %>% group_by(HR) %>% summarize(slope = lm(R ~ BB)$coef[2])
						</blockquote></div>
					</p>
					<p>
						What is really great about the <i>broom</i> package is that it allows the connection of lm() to tidyverse and compute things like CI.
					</p>
					<p>
						Three main functions to know and use:
						<ul class="disc">
							<li>
								tidy() returns estimates and related information as a data frame.<br>
								tidy(fit) returns estimate, se, stat and p-value; can add more i.e. tidy(fit, conf.int = TRUE) returns CI. Combine this with summarize to allow functionalities like group_by
							</li>
							<li>glance()</li>
							<li>augment()</li>
						</ul>
					</p>]
					<p>
						Useful to know for lm():<br>
						Using the dot operator instead of the across() function will lead R to ignore our grouping. You may notice that the estimate for both lgID values is the same as a result
					</p>

					<h6>Assessment: Advanced dplyr</h6>
					<ol>
						<li>
							<b><u>
								As seen in the videos, what problem do we encounter when we try to run a linear model on our baseball data, grouping by home runs?
							</u></b><br>
							The lm() function does not know how to handle grouped tibbles.
						</li><br>
						<li>
							<b><u>
								Tibbles are similar to what other class in R?
							</u></b><br>
							Dataframes
						</li><br>
						<li>
							<b><u>
								What are some advantages of tibbles compared to data frames?
							</u></b><br>
							All of the listed answers are advantages of tibbles when compared to data frames: tibbles display better, they always return tibbles when subsetted, they can have complex entries, and they can be grouped.
						</li><br>
						<li>
							<b><u>
								What are two advantages of the summarize() command, when applied to the tidyverse?
							</u></b><br>
							Correct. The summarize function can understand grouped tibbles.<br>
							Correct. The sumarize function always returns a type of data frame (tibble or data.frame)
						</li><br>
						<li>
							<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/linear_regression/2.3_Advanced-dplyr_assessment.R" class="saymore">RStudio on GitHub questions 5: 2.3_Advanced-dplyr_assessment.R</a>
						</li><br>
						<li>
							<b><u>
								 The output of a broom function is always what?
							</u></b><br>
							A tibble
						</li><br>
					</ol><br>
					<p>
						<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/linear_regression/2.3_Advanced-dplyr_assessment.R" class="saymore">RStudio on GitHub questions 7-10: 2.3_Advanced-dplyr_assessment.R</a>
					</p>
				</div>
			</li>

		<!-- Accordion: Regression and Baseball -->
			<li class="">
				<div class="title">
					<b>Regression and Baseball</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Building a Better Offensive Metric for Baseball</h6>
					<p>
						To let lm() know there are two predictor variables use “+”
						<div id="testimonials"><blockquote>
							fit <- Teams %>%
								<div class="tab">
									filter(yearID %in% 1961:2001) %>%<br>
									mutate(BB =BB/G, HR = HR/G, R = R/G) %>%<br>
									lm(R ~ BB + HR, data = .)
								</div>
						</blockquote></div>
						<i>tidy</i> library let’s us see the summary of the model (in this case <i>lm()</i> let us see it).
					</p>
					<p>
						<i>Side note:</i> Jointly normal means if the other variables are held constant then the remain predictor will have a linear relationship with the outcome without the slope changing based on the other predictive values
					</p>
					<p>
						When you use the <i>predict()</i> it should be used on data that hasn’t been used to create the model. In order to have a better prediction, it is beneficial to filter out players who have a low plate appearance - it gives us more data to work with. For example to give us number of runs we predict a team would score if all batters were the exact same as that one player:
						<div id="testimonials"><blockquote>
							players <- Batting %>% filter(yearID %in% 1961:2001) %>%
							<div class="tab">
								group_by(playerID) %>%<br>
								mutate(PA = BB +AB) %>% <br>
								summarize(G = sum (PA)/pa_per_game, 
								<div class="tab">
									BB = sum(BB)/G,<br>
									singles = sum(H-X2B-X3B-HR)/G,<br>
									doubles = sum(X2B)/G,<br>
									triples = sum(X3B)/G,<br>
									HR = sum(HR)/G,<br>
									AVG = sum(H)/sum(AB),<br>
									PA = sum(PA)) %>%
								</div>
								filter(PA >= 300) %>% <br>
								select(-G) %>%<br>
								mutate(R_hat = predict(fit, newdata = .))
							</div>
						</blockquote></div>
					</p>
					<p>
						A useful example to know is a way to actually pick the players for the team can be done using what computer scientists call linear programming:
						<div id="testimonials"><blockquote>
							library(reshape2)<br>
							library(lpSolve)<br><br>


							players <- players %>% filter(debut <= "1997-01-01" & debut > "1988-01-01")<br>
							constraint_matrix <- acast(players, POS ~ playerID, fun.aggregate = length)<br>
							npos <- nrow(constraint_matrix)<br>
							constraint_matrix <- rbind(constraint_matrix, salary = players$salary)<br>
							constraint_dir <- c(rep("==", npos), "<=")<br>
							constraint_limit <- c(rep(1, npos), 50*10^6)<br>
							lp_solution <- lp("max", players$R_hat,
							<div class="tab">
                constraint_matrix, constraint_dir, <br>constraint_limit,<br>
                all.int = TRUE) 
              </div><br>
            </blockquote></div>
            Then we can us this algorithm to choose 9 players:
            <div id="testimonials"><blockquote>
							our_team <- players %>%
								<div class="tab"> 
							  filter(lp_solution$solution == 1) %>%<br>
							  arrange(desc(R_hat))
							</div>
							our_team %>% select(nameFirst, nameLast, POS, salary, R_hat)
						</blockquote></div>
					</p>
					<p>
						We can actually see that these players all have above average BB and HR rates while the same is not true for singles:
						<div id="testimonials"><blockquote>
							my_scale <- function(x) (x - median(x))/mad(x)<br>
							players %>% mutate(BB = my_scale(BB), 
							<div class="tab"> 
	               singles = my_scale(singles),<br>
	               doubles = my_scale(doubles),<br>
	               triples = my_scale(triples),<br>
	               HR = my_scale(HR),<br>
	               AVG = my_scale(AVG),<br>
	               R_hat = my_scale(R_hat)) %>%
	             </div>
							    filter(playerID %in% our_team$playerID) %>%<br>
							    select(nameFirst, nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) %>%<br>
							    arrange(desc(R_hat))

						</blockquote></div>
					</p>

					<h6>Regression Fallacy</h6>
					<p>
						Sophomore slump: refers to an instance in which a second effort fails to live up to the standard of the first effort. 
					</p>
					<p>
						When we look at correlation for performance in two separate years is high but not perfect. Through futher analysis we see that the sophomore slump is basically the rule of regression to the mean. 
					</p>
					<h6>Measurement Error Models</h6>
					<p>
						Sometimes we will have non-random covariates (e.g. time) leading to randomness from measurement error. To check if the estimated points fit the data, we use augment() (from broom).
						<div id="testimonials"><blockquote>
							augment(fit) %>%
							<div class="tab">
								ggplot() +<br>
								geom_point(aes(time, y)) + <br>
								geom_line(aes(time, .fitted))
							</div>
						</blockquote></div>
					</p>

					<h6>Assessment: Regression and Baseball</h6>
					<ol>
						<li>
							<b><u>
								What is the final linear model (in the video "Building a Better Offensive Metric for Baseball") we used to predict runs scored per game?
							</u></b>
							<div id="testimonials"><blockquote>
								lm(R ~ BB + singles + doubles + triples + HR)
							</blockquote></div>
						</li>
						<li>
							<b><u>
								We want to estimate runs per game scored by individual players, not just by teams. What summary metric do we calculate to help estimate this?
							</u></b>
							<div id="testimonials"><blockquote>
								pa_per_game <- Batting %>% 
								<div class="tab"> 
								  filter(yearID == 2002) %>% <br>
								  group_by(teamID) %>%<br>
								  summarize(pa_per_game = sum(AB+BB)/max(G)) %>% <br>
								  .$pa_per_game %>% <br>
								  mean
								</div>
							</blockquote></div>
							pa_per_game: the number of plate appearances per team per game, averaged across all teams
						</li><br>
						<li>
							<b><u>
								Imagine you have two teams. Team A is comprised of batters who, on average, get two bases on balls, four singles, one double, no triples, and one home run. Team B is comprised of batters who, on average, get one base on balls, six singles, two doubles, one triple, and no home runs. (For convenience, the coefficients for the model are as follows: BB 0.371, singles 0.519, doubles 0.771, triples 1.24, and home runs 1.44.). Which team scores more runs, as predicted by our model?
							</u></b><br>
							<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/linear_regression/2.4_RegressionBaseball_assessment.R" class="saymore">RStudio on GitHub question 3: 2.4_RegressionBaseball_assessment.R</a><br>
							Team B 
							
						</li><br>
						<li>
							<b><u>
								The formula for on-base-percentage plus slugging percentage (OPS) is: 
								<img src="images/pages/stats/linreg/lr_7-q4.png" width="300">. The OPS metric gives the most weight to:
							</u></b><br>
							HR
						</li><br>
						<li>
							<b><u>
								What statistical concept properly explains the "sophomore slump"?
							</u></b><br>
							Regression to the mean
						</li><br>
						<li>
							<b><u>
								In our model of time vs. observed_distance in the video "Measurement Error Models," the randomness of our data was due to:
							</u></b><br>
							measurement error
						</li><br>
						<li>
							<b><u>
								Which of the following are important assumptions about the measurement errors in the experiment presented in the video "Measurement Error Models"?
							</u></b><br>
							The measurement error is random<br>
							The measurement error is independent<br>
							The measurement error has the same distribution for each time 
						</li><br>
						<li>
							<b><u>
								Which of the following scenarios would violate an assumption of our measurement error model?
							</u></b><br>
							There was one position where it was particularly difficult to see the dropped ball.
						</li><br>
					</ol>
					<a href="https://github.com/anjawu/edx-harvard-ds-courses/blob/main/linear_regression/2.4_RegressionBaseball_assessment.R" class="saymore">RStudio on GitHub questions 9-11: 2.4_RegressionBaseball_assessment.R</a>
				</div>
			</li>

		<!-- Accordion: Correlation is Not Causation -->
			<li class="">
				<div class="title">
					<b>Correlation is Not Causation</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Spurious Correlation</h6>
					<p>
						Spurious correlations refers to a connection being seen between two variables that appears to be causal but it is not. This can lead to misinterpreting associations. 
					</p>
					<p>
						These kinds of associations can arise from data dredging, data phishing, or data snooping (i.e. cherry picking). P-hacking is a  particular form of data dredging (reporting experiments that report only the experiments that resulted in small p-values). There are methods that can be done to adjust for this.
					</p>
					<h6>Outliers</h6>
					<p>
						Outliers allow us to see high correlation without an actual relationship. Spearman correlation computes the correlation on the ranks of the values; thereby making it robust to outliers.
					</p>
					<h6>Reversing Cause and Effect</h6>
					<p>
						Cause and effect reversal: meaning that you assume the wrong directionality of effect (e.g. sons height affects dad’s height).
					</p>
					<h6>Confounders</h6>
					<p>
						Confounders cause changes in variables that are correlated. The way to analyze confounders is by stratifying data over various aspects to see if there is any change in a foreseen correlation.
					</p>
					<h6>Simpson's Paradox</h6>
					<p>
						This is when we see the correlation flipped when analyzing a whole population compared to the stratified population. A visual example being: 
						<center><img src="images/pages/stats/linreg/lr_8-corr_all.png" width="400"></center>
						But when stratified we see:
						<center><img src="images/pages/stats/linreg/lr_8-corr_strat.png" width="400"></center>
					</p>


					<h6>Assessment: Correlation is Not Causation</h6>
					<ol>
						<li>
							<b><u>
								In the videos, we ran one million tests of correlation for two random variables, X and Y. 
								How many of these correlations would you expect to have a significant p-value (p<=0.05), just by chance?
							</u></b><br>
							In this example, the chance of finding a correlation when none exists is 0.05*1,000,000 chances.
						</li><br>
						<li>
							<b><u>
								Which of the following are examples of p-hacking?
							</u></b><br>
							Looking for associations between an outcome and several exposures and only reporting the one that is significant.<br>
							Trying several different models and selecting the one that yields the smallest p-value.<br>
							Repeating an experiment multiple times and only reporting the one with the smallest p-value.
						</li><br>
						<li>
							<b><u>
								The Spearman correlation coefficient is robust to outliers because:
							</u></b><br>
							It calculates correlation between ranks, not values.
						</li><br>
						<li>
							<b><u>
								What can you do to determine if you are misinterpreting results because of a confounder?
							</u></b><br>
							More closely examine the results by stratifying and plotting the data.<br>
							Although you can sometimes use linear models, you can't always and exploratory data analysis (stratifying and plotting data) will help determine if there is a confounder.
						</li><br>
						<li>
							<b><u>
								Look again at the admissions data presented in the confounders video using ?admissions. 
								What important characteristic of the table variables do you need to know to understand the calculations used in this video?
							</u></b><br>
							The column admitted is the percent of students admitted, while the column applicants is the total number of applicants.
						</li><br>
						<li>
							<b><u>
								In the example in the confounders video, major selectivity confounds the relationship between UC Berkeley admission rates and gender because:
							</u></b><br>
							Major selectivity is associated with both admission rates and with gender, as women tended to apply to more selective majors.
						</li><br>
						<li>
							<b><u>
								Admission rates at UC Berkeley are an example of Simpson’s Paradox because:
							</u></b><br>
							It appears that men have higher a higher admission rate than women, however, after we stratify by major, we see that on average women have a higher admission rate than men.
						</li><br>

					</ol>
				</div>
			</li>

		</ul>

	</div>

	<div class="hr">
	</div>


	<!-- Wrangling -->
<!-- 	<div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>Wrangle
			</div>
		</div>
		<h4 id="wrangle">Wrangling</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-wrangling" class="saymore">here</a>. This section will be updated soon!
		</p> -->
		<!-- Accordion -->
		<!-- <ul class="accordion"> -->

		<!-- Accordion:  -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion: -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>
 -->
		<!-- Accordion:  -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion: -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion: -->
	<!-- 		<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion: -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>
 -->
		<!-- Accordion: -->
<!-- 			<li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- </ul> -->

	<!-- </div> -->

	<div class="hr">
	</div>

<!-- Machine Learning -->
	<!-- <div class="twelve columns">
		<div class="circledate">
			<div class="textcircle">
				<br>ML
			</div>
		</div>
		<h4 id="ml">Machine Learning</h4>
		<div class="dots blogdots">
		</div>
		<p>
			The course link can be found <a href="https://www.edx.org/course/data-science-machine-learning" class="saymore">here</a>. Within this course 
		</p> -->
		<!-- Accordion -->
		<!-- <ul class="accordion"> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion: -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li> -->

		<!-- Accordion:  -->
			<!-- <li class="">
				<div class="title">
					<b></b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
				</div>
			</li>

		</ul>

	</div>

	<div class="hr">
	</div> -->






	<!-- Statistical Test-->
		<!-- <div class="circledate">
			<div class="textcircle">
				<br>Stats Tests
			</div>
		</div>
		<h4>Statistical Tests</h4>
		<div class="dots blogdots">
		</div>
		<p>
			Here I will be looking at different statistical tests, when to use them and how. Stay tuned!
		</p>
		<p>
			
		</p> -->

		<!-- Accordion: Setting Up -->
<!-- 		<ul class="accordion">
			<li class="">
				<div class="title">
					<b>Setting up Pandas</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						To download, in the terminal enter:
						<div id="testimonials"><blockquote><p>pip install pandas</p></blockquote></div>
						To use within a .py (python) file:
						<div id="testimonials"><blockquote><p>import pandas as pd</p></blockquote></div>
						Here you are importing the Pandas library and giving it a variable pd, this way when you want to call on a function from Pandas, you do not need to type pandas each time, but can just write pd.
					</p>
				</div>
			</li> -->
		<!-- Accordion: Working with dataframe -->
<!-- 			<li class="">
				<div class="title">
					<b>Working with Dataframes</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p> -->
					<!-- Tabs -->
<!-- 					<dl class="tabs">
						<dd class="active"><a href="#rk-sort">Rank / Sort Values</a></dd>
						<dd><a href="#df-functions">Applying Functions</a></dd>
						<dd><a href="#add-row">Adding Row</a></dd>
						<dd><a href="#delete">Delete Row or Column</a></dd>
						<dd><a href="#compare-df">Compare Dataframes</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="rk-sortTab">
							<p>
								Rank / Sort Values
							</p>
						</li>
						<li id="df-functionsTab">
							<p>
								Applying functions
							</p>
						</li>
						<li id="add-rowTab">
							<p>
								Adding row
							</p>
						</li>
						<li id="deleteTab">
							<p>
								Delete Column or Row
							</p>
						</li>
						<li id="compare-dfTab">
							<p>
								Compare Dataframes
							</p>
						</li>
					</ul>
				</div>
			</li> -->
		<!-- Accordion: Calculating using dataframe -->
<!-- 			<li class="">
				<div class="title">
					<b>Calculating using Dataframes</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>
					 Tabs -->
					<!-- <dl class="tabs">
						<dd class="active"><a href="#operations">Pandas Operations</a></dd>
						<dd><a href="#counting">Counting</a></dd>
						<dd><a href="#mean">Mean</a></dd>
						<dd><a href="#sd">Standard Deviation</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="operationsTab">
							<p>
								Operations tab
							</p>
						</li>
						<li id="countingTab">
							<p>
								Counting tab
							</p>
						</li>
						<li id="meanTab">
							<p>
								Mean tab
							</p>
						</li>
						<li id="sdTab">
							<p>
								SD tab
							</p>
						</li>
					</ul>
				</div>
			</li> --> 
		<!-- Accordion: Visuals with Dataframes -->
<!-- 			<li class="">
				<div class="title">
					<b>Visuals with Dataframes</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						
					</p>
					<dl class="tabs">
						<dd class="active"><a href="#pandasbar">Pandas Bar Graph</a></dd>
					</dl>
					<ul class="tabs-content">
						<li class="active" id="pandasbarTab">
							<p>
								Pandas Bar Graph
							</p>
						</li>
					</ul>
				</div>
			</li>
		</ul> -->




		<!-- <br/> -->
		<!-- Pages -->
<!-- 		<ul class="pagination">
			<li class="arrow unavailable"><a href="">&laquo;</a></li>
			<li class="current"><a href="">1</a></li>
			<li><a href="">2</a></li>
			<li><a href="">3</a></li>
			<li><a href="">4</a></li>
			<li class="arrow"><a href="">&raquo;</a></li>
		</ul> -->
	</div>
</div>
<div class="hr">
</div>

<!-- FOOOTER 
================================================== -->
<div id="footer">
	<footer class="row">
	<p class="back-top floatright">
		<a href="#top"><span></span></a>
	</p>
	<center>
		<div class="twelve columns">
			<h1>Connect with Me</h1>
			<a class="social github" href="https://github.com/anjawu"></a>
			<a class="social linkedin" href="https://www.linkedin.com/in/anja-wu/"></a>
		</div>
	</center>
	</footer>
</div>
<div class="copyright">
	<div class="row">
		<div class="six columns">
			 &copy;<span class="small"> Copyright 2021 Anja Wu</span>
		</div>
		<div class="six columns">
			<span class="small floatright"> Template by <a href="www.wowthemes.net">WowThemes.net</a></span>
		</div>
	</div>
</div>
<!-- JAVASCRIPTS 
================================================== -->
<!-- Javascript files placed here for faster loading -->
<script src="javascripts/foundation.min.js"></script>
<script src="javascripts/jquery.easing.1.3.js"></script>
<script src="javascripts/elasticslideshow.js"></script>
<script src="javascripts/jquery.carouFredSel-6.0.5-packed.js"></script>
<script src="javascripts/jquery.cycle.js"></script>
<script src="javascripts/app.js"></script>
<script src="javascripts/modernizr.foundation.js"></script>
<script src="javascripts/slidepanel.js"></script>
<script src="javascripts/scrolltotop.js"></script>
<script src="javascripts/hoverIntent.js"></script>
<script src="javascripts/superfish.js"></script>
<script src="javascripts/responsivemenu.js"></script>
</body>
</html>