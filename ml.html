<!DOCTYPE html>
<!-- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->
<head>
<meta charset="utf-8"/>
<!-- Set the viewport width to device width for mobile -->
<meta name="viewport" content="width=device-width"/>
<title>Anja's Machine Learning</title>
<!-- CSS Files-->
<link rel="stylesheet" href="stylesheets/style.css">

<link rel="stylesheet" href="stylesheets/skins/blue.css">
<!-- skin color -->
<link rel="stylesheet" href="stylesheets/responsive.css">
<!-- IE Fix for HTML5 Tags -->
<!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-216360630-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-216360630-1');
</script>

<!-- creating buckets for time spent on page -->
<script type="text/javascript">
function timer11(){gtag('event','1', {'event_category':'ml_TimeOnPage','event_label':'11-30 seconds','non_interaction':true});}
function timer31(){gtag('event','2', {'event_category':'ml_TimeOnPage','event_label':'31-60 seconds','non_interaction':true});}
function timer61(){gtag('event','3', {'event_category':'ml_TimeOnPage','event_label':'61-180 seconds','non_interaction':true});}
function timer181(){gtag('event','4', {'event_category':'ml_TimeOnPage','event_label':'181-360 seconds','non_interaction':true});}
function timer361(){gtag('event','5', {'event_category':'ml_TimeOnPage','event_label':'361-600 seconds','non_interaction':true});}
function timer601(){gtag('event','6', {'event_category':'ml_TimeOnPage','event_label':'601-1800 seconds','non_interaction':true});}
function timer1801(){gtag('event','7', {'event_category':'ml_TimeOnPage','event_label':'1801+ seconds','non_interaction':true});}
gtag('event','0', {'event_category':'ml_TimeOnPage','event_label':'0-10 seconds','non_interaction':true});
setTimeout(timer11,11000);
setTimeout(timer31,31000);
setTimeout(timer61,61000);
setTimeout(timer181,181000);
setTimeout(timer361,361000);
setTimeout(timer601,601000);
setTimeout(timer1801,1801000);
</script>

</head>
<body>
<!-- HIDDEN PANEL 
================================================== -->
<div id="panel">
	<div class="row">
		<div class="twelve columns">
			<img src="http://www.wowthemes.net/demo/studiofrancesca/images/info.png" class="pics" alt="info">
			<div class="infotext">
				 Thank you for visiting my website!
			</div>
		</div>
	</div>
</div>
<p class="slide">
	<a href="#" class="btn-slide"></a>
</p>
<!-- HEADER
================================================== -->
<div class="row">
	<div class="headerlogo four columns">
		<div class="logo">
			<a href="index.html">
			<h4>Anja Wu</h4>
			</a>
		</div>
	</div>
	<div class="headermenu eight columns noleftmarg">
		<nav id="nav-wrap">
		<ul id="main-menu" class="nav-bar sf-menu">
			<li class="current">
			<a href="index.html">Home</a>
			</li>
			<li>
			Coding Info
			<ul>
				<li><a href="sql.html">SQL</a></li>
				<li><a href="libraries.html">Python Libraries</a></li>
				<!-- <li><a href="stats.html">Statistics</a></li> -->
				<li><a href="ml.html">Machine Learning</a></li>
				<li><a href="github.html">GitHub</a></li>
				<li><a href="ganalytics.html">Google Analytics</a></li>
				<li><a href="funlearn.html">Fun Things Learnt</a></li>
				<li><a href="versus.html">Versus</a></li>
				<li><a href="environmentsetup.html">Environment Setup</a></li>
			</ul>
			</li>
			<li>
			<a href="projects.html">Projects</a>
			<ul>
				<li><a href="redlight.html">Redlight</a></li>
				<li><a href="costco.html">Costco</a></li>
				<li><a href="website.html">Website</a></li>
				<li><a href="indeed.html">Indeed</a></li>
				<li><a href="imdb.html">IMDb</a></li>
				<li><a href="hangman.html">Hangman</a></li>
			</ul>
			</li>
<!-- 			<li>
			<a href="contact.html">Contact</a>
			</li> -->
		</ul>
		</nav>
	</div>
</div>
<div class="clear">
</div>
<!-- SUBHEADER
================================================== -->
<div id="subheader">
	<div class="row">
		<div class="twelve columns">
			<p class="left">
				Machine Learning
			</p>
		</div>
	</div>
</div>
<div class="hr">
</div>
<!-- CONTENT 
================================================== -->
<div class="row">
	<p>
		I recently completed the <a href ="https://vectorinstitute.ai/mothers-and-machine-learning/" class = saymore>Introduction to Machine Learning</a> run by the Vector Institute with Juan Felipe Carrasquilla √Ålvarez as the professor. I really enjoyed the course and learnt so much!
	</p>
	<p>
		I have broken down the things I have learnt by my assignment, lectures, and material I have researched. 
	</p>
	<p>
		<b>I have begun to fill out the sections slowly! I have provided a link to my GitHub for all of the assignments and created a tab for every topic that was covered in the course. I plan on adding to each tab bit by bit!</b>
	</p>
</div>


<div class="row">
	<div class="twelve columns">

		<!-- Assignments -->
		<div class="circledate">
			<div class="textcircle">
				<br>Projects
			</div>
		</div>
		<h4>Assignments</h4>
		<div class="dots blogdots">
		</div>
		<p>
			This section has the best take aways from each assignment as opposed to just posting the whole assignment. I do link to the GitHub repo that has all of my code posted. I will be updating the learning from each assignment shortly.
		</p>

		<!-- Accordion: Assignment 1-->
			<ul class="accordion">
			<li class="">
				<div class="title">
					<b>k-Nearest Neighbours Assignment</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H1" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
							<li>"Anja_H1_Question_2_KNN_2Features.ipynb": has the analysis and selection of the "k" hyperparameter in the Iris database from sklearn focusing on just two features (the sepal ones) to predict classification of three species.  </li>
							<li>"Anja_H1_Question_2_KNN_4Features.ipynb": has the above k-Nearest Neighbours analysis but done using all four features for the prediction of the species classification.</li>
						</ul>
					</p>
					 
					<h6>Highlights of Learning</h6> 
					Steps
					<ol>
						<li>
							<b>Defining a data set from the Iris dataset (did 2 then 4 features):</b><br>
							Taking the first 2 features from the data matrix:
							<div id="testimonials"><blockquote>
								X = iris.data[:, :2]<br>
								y = iris.target # The class labels
							</blockquote></div>
							Extending the matrix to all four features:
							<div id="testimonials"><blockquote>								
								X = iris.data[:, :4]<br>
								y = iris.target
							</blockquote></div>
						</li><br>

						<li>
							<b>Splitting training and test set:</b><br>
							<div id="testimonials"><blockquote>								
								from sklearn.model_selection import train_test_split
							</blockquote></div>
							We set 20% of the dataset as the test set, and 80% as the training set
							<div id="testimonials"><blockquote>								
								X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
							</blockquote></div>
						</li><br>

						<li>
							<b>Creating scatter plot to visualize (two different sets):</b><br>
							<u>For the first two features</u><br>
							Making a scatterplot for two features (sepal) and three species:
							<div id="testimonials"><blockquote>								
								f, axs = plt.subplots(figsize=(8,6))<br>
								the_scatter = axs.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')
							</blockquote></div>

							Change the legend name of the class to be Iris species like Setosa, Versicolour, and Virginica:
							<div id="testimonials"><blockquote>								
								lines, legend_names = the_scatter.legend_elements()<br>
								legend1 = axs.legend(lines, ['Setosa', 'Versicolour', 'Virginica'], title="Classes")<br>
								axs.add_artist(legend1)
							</blockquote></div>
							<img src="images/pages/ml/a1-g1-sepal.png" width = "385" height = "auto" alt=""><br>

							<u>For the last two features</u><br>
							Making a scatterplot for two features (petal) and three species:
							<div id="testimonials"><blockquote>								
								f, axs = plt.subplots(figsize=(8,6))<br>
								the_scatter = axs.scatter(X[:, 2], X[:, 3], c=y, cmap=plt.cm.Set1, edgecolor='k')
							</blockquote></div>
							<img src="images/pages/ml/a1-g2-petal.png" width = "385" height = "auto" alt=""><br><br>

							As can be seen two of the features (petal) give a lot more information in the boundary decisions. We would expect when we include them for our predictions to be done well.
						</li><br>

						<li>
							<b>Creating function that will print out the features from the integers in the dataset:</b><br>
							Due to the fact that the categorical variables needed to be encoded, we must pull out the type in order for the values to be interpretable in our data. So we define a function <i>print_features(x,y)</i> that will print out the Iris type and sepal width/length, in a clean format.
							<div id="testimonials"><blockquote>								
							def print_features(x,y):
							<div class="tab">
								print('Iris type:', ['Setosa', 'Versicolour', 'Virginica'][y-1])<br>
								print('Sepal Length: %.1f \t Sepal Width: %.1f'%(x[0], x[1]))  
							</div>
						</blockquote></div>
						</li><br>

						<li>
							<b>Creating Euclidean distance function:</b><br>
							This was done for fun to check if the same neighbours would be returned as using the k-nearest neighbour function from sklearn (to further understand what was occuring "under the hood" for the algorithm.
							<div id="testimonials"><blockquote>								
								def euclidean_distance(x1, x2):
								<div class="tab">
									distance = sqrt(np.sum((x2-x1)**2))<br>
									return distance
								</div>
							</blockquote></div>
						</li><br>

						<li>
							<b>Printing out nearest neighbours:</b><br>
							<u>Using my Euclidean distance formula</u><br>
							First, we randomly select a test example (#11) and print features so we can compare to the nearest neightbours:
							<div id="testimonials"><blockquote>
								sample = X_test[10]<br>
								print('Test Sample:')<br>
								print_features(sample, y_test[10])
							</blockquote></div>

							Next, we calculate the euclidean distance to this test example:
							<div id="testimonials"><blockquote>
								distances = []<br>
								for i, row in enumerate(X_train):
								<div class="tab">
								    distance = euclidean_distance(sample, row)<br>
								#     print(f"{i}: {distance} from x1: {sample} and x2: {row}") # Checking calculations above for correctedness<br>
								    distances.append((i, distance))
								</div>
								distances.sort(key=lambda tup: tup[1]) # sorting distances
							</blockquote></div>

							Finally, we print the closest 2 neighbours with their features for comparison:
							<div id="testimonials"><blockquote>
								k = 2 # Number of nearest neighbors<br>
								print('\nTop %d Nearest Neighbors:' % k)<br>
								for nn in range(k):
								<div class="tab">
								    print_features(X_train[distances[nn][0]], y_train[distances[nn][0]])
								</div>
							</blockquote></div><br>

							<u>Using sklearn k-NN</u><br>
							Import function from library:
							<div id="testimonials"><blockquote>
								from sklearn.neighbors import KNeighborsClassifier
							</blockquote></div>
							Just like before, we select test example #11 and print features so we can compare to the nearest neightbours. Then we use the sklearn function <i>KNeighborsClassifier()</i> to specify looking at the top 5 neighbours:
							<div id="testimonials"><blockquote>
								k=5<br>
								neigh = KNeighborsClassifier()<br>
								neigh.fit(X_train, y_train)<br>
								dists, neighbor_ids = neigh.kneighbors(X=[sample], n_neighbors = 5)
							</blockquote></div>
							Finally, we print the closest 5 neighbours with their features for comparison:

							<div id="testimonials"><blockquote>
								print('\nClosest 5 neighbors to this test sample:')<br>
								for knn in range(5):
								<div class="tab">
								    print('\nNeighbor %d ===> distance:%f'%(knn, dists[0][knn]))<br>
								    print_features(X_train[neighbor_ids[0][knn]], y_train[neighbor_ids[0][knn]])
								</div>
							</blockquote></div><br>

							From the results, both produced the same top 2 nearest neighbours. This makes sense because in reading the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" class="saymore">KNeighborsClassifier()</a> the metric is technically Minkowski, but when p=2 (power parameter) it is equivalent to Euclidean metric. Since the default is p=2, we know that the calculations should be the same.
						</li><br>

						<li>
							<b>Create heatmap to analyze the training set distance from the test set distance:</b><br>
							Creating list to store distances between the test set and the training set
							<div id="testimonials"><blockquote>
								distances = []<br>
								for x_test in X_test:
								<div class="tab">
								    distance = np.sum((x_test[np.newaxis, ...] - X_train) ** 2, axis=1)<br>
								    distances.append(distance)
								</div>
							</blockquote></div>

							Creating a "colorbar" graph to display distance from each training example to each test example
							<div id="testimonials"><blockquote>
								distances = np.array(distances)<br>
								plt.figure(figsize=(50, 20))<br>
								plt.imshow(distances)<br>
								plt.colorbar()<br>
								plt.xlabel('Training examples id', fontsize=40)<br>
								plt.ylabel('Test examples id', fontsize=40)<br>
								plt.xticks(np.arange(0, 120, 2), fontsize=20)<br>
								plt.yticks(np.arange(0, 30, 1), fontsize=20)
								plt.show
							</blockquote></div>
							<img src="images/pages/ml/a1-heatmap.png" width = "600" height = "auto" alt=""><br><br>

							<b><i>Note: a good reason to check this is because we want the distances between the sets to be small. This is when k-Nearest Neighbours works best. If the heatmap shows a lot more further distances, this means the sample was not done well.</i></b>
						</li><br>

						<li>
							<b>Creating a line graph to show test accuracy based on k:</b><br>
							Creating list of all test accuracies based on changes in k
							<div id="testimonials"><blockquote>
								test_accs = []<br>
								for k in range(1, X_train.shape[0]):
								<div class="tab">
								  # Create K nearest neighbors classifier<br>
								  neigh = KNeighborsClassifier(n_neighbors=k)<br>
								  neigh.fit(X_train, y_train)<br><br>
								  
								  # Prediction<br>
								  y_pred = neigh.predict(X_test)<br><br>
								  
								  # Calculate accuracy<br>
								  acc = (y_pred == y_test).mean()<br>
								  test_accs.append(acc)
								</div>
							</blockquote></div>
							Creating a line graph to depict accuracy to be able to find the best hyperparameter (in this case, k value) visually:
							<div id="testimonials"><blockquote>
								plt.figure(figsize=(30, 10))<br>
								plt.plot(list(range(1, X_train.shape[0])), test_accs)<br>
								plt.axhline(y=0.93, color='r', linestyle='-')<br>
								plt.axvline(x=29, color='b', linestyle='--')<br>
								plt.xlabel('Number of nearest neighbors (k)')<br>
								plt.ylabel('Test set accuracy')<br>
								plt.xticks(np.arange(0, 120, 2), fontsize=18)<br>
								plt.yticks(np.arange(0, 1.0, 0.05), fontsize=18)<br>
								plt.grid()<br>
								plt.show
							</blockquote></div>
							<u>For 2 features:</u><br>
							<img src="images/pages/ml/a1-2feature-accuracy.png" width = "600" height = "auto" alt=""><br><br>
							<b><i>We can see the accuracy of the test set decreasing the larger k-values we have due to the fact that we are underfitting the model on the training set leading to a decrease in finding the underlying relationship. At a low level of k, we can see that the accuracy performs more poorly due to overfitting on the training model and capturing more noise than the underlying relationship.</i></b><br><br>
							<u>For 4 features:</u><br>
							<img src="images/pages/ml/a1-4feature-accuracy.png" width = "600" height = "auto" alt=""><br><br>
							<b><i>As can be seen the more features that are added the more accurate the predictions. </i></b><br>
							
						</li><br>
					</ol>
					<b>*Note*</b>
					<p>
						After this assignment we discussed the fact that the data should be divided into three sections: training set, validation set, and test set. This way the test set is only used once to check accuracy and the validation set is used to determine the best hyperparameters. 
					</p>
				</div>
			</li>

		<!-- Accordion: Assignment 2-->
			<li class="">
				<div class="title">
					<b>Decision Trees Assignment </b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H2" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
							<li>"AnjaWu_H2_DecisionTrees.ipynb": has a decision tree that was created from Kaggle real and fake news headlines.</li>
						</ul>
					</p>
					<!-- <h6>Highlights of Learning:</h6> -->
					<p>
						
					</p>
				</div>
			</li>
			
		<!-- Accordion: Assignment 3-->
			<li class="">
				<div class="title">
					<b>Regression and Classification</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H3" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
              
							<li>"AnjaWu_HW3_code-Regression&Classification": has the creation of linear and logistic regression analysis on health data analyzed to predict hospital charges. Most code was done by a TA in the course, however, I chose to comment the code to further my understanding on how to build regression models using Scikit Learn.</li>
							<li>"AnjaWu_EntropyCalculations": has code that I made to calculate entropy from the assignment, rather than doing them all by hand.</li>
						</ul>
					</p>
					<!-- <h6>Highlights of Learning:</h6> -->
					<p>
						
					</p>
				</div>
			</li>
		
		<!-- Accordion: Assignment 4-->
			<li class="">
				<div class="title">
					<b>Feed Forward Neural Network</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H4" class="saymore">Here</a> is the link to the GitHub code part of the assignment.
				 
						 <ul class="disc">
							<li>"AnjaWu_HW4_FeedForwardNN.ipynb": has a complete neural network, in which optimal hyperparameters are analyzed and selected.</li>
						</ul>

					</p>
					<!-- <h6>Highlights of Learning:</h6> -->
					<p>
						
					</p>
				</div>
			</li>


		<!-- Accordion: Assignment 5-->
			<li class="">
				<div class="title">
					<b>HR Attrition Analysis (Logistic Regression and Decision Tree)</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/HR-attrition-new" class="saymore">Here</a> is the link to the GitHub code part of the assignment.
						<!-- <ul class="disc">
							<li>"AnjaWu_HW4_FeedForwardNN.ipynb": has a complete neural network, in which optimal hyperparameters are analyzed and selected.</li>
						</ul> -->
					</p>
					<p>
						<a href="images/HR_Attrition_Analysis.pdf" class="saymore">Here</a> is the final written report!
					</p>
					<h6>Summary of Assignment:</h6>
					<p>
						<b>My insights:</b> <br>
						From the logistic model, the random oversampling method performed the best in terms of predictions (accuracy range of 0.59-0.65). When looking at the features, there were certain ones that cannot be controlled by the company such as: age, years at company, number of companies worked, and marital status. However, there were features that could provide insights in which the company could act on to help keep more employees. 
						<ul class="disc">
							<li>In terms of performance and salary: people with a higher performance are more likely to leave but people with a higher percent salary hike are less likely to leave. Meaning the company should reward those high impact individuals who they would like to keep. </li>
							<li>Along this thread, it was found that the longer it has been since a person has been promoted, the more likely they are to leave. Thus it would be important to dig in individual cases whether the company is hiring externally when they should have promoted someone from within. </li>
							<li>The more training an individual had the more likely it was that they would stay. This means the company should encourage professional development.</li>
							<li>Sales executives and research directors tend to be more likely to leave, so it would be worthwhile to flag it to their leadership and look into the why for these groups. </li>
							<li>Individuals who travel more, tend to leave more. Given the lack of information on the company, it would be hard to come up with a reasonable action for the company to take in this regard. </li>
							<li>The last main actionable insight was: the worse an individual's work-life balance was, the more likely they were to leave. The company should ensure they are encouraging days off when needed, in order to retain their employees.</li>
						</ul>
					</p>
					<p>
						The decision tree performed really well at predicting (accuracy range of 0.89-0.95) regardless of the imbalanced sampling method chosen. This can be used to predict which employees might leave within the next year and try to keep them on.
					</p>
					<p>
						<b>My process:</b><br>
						Through the data processing stage there were several things done to ensure data was ready for machine learning algorithms. Initially, I had to encode the categorical features, both ordinal and nominal features. For the ordinal features, I chose a simple encoding of 0 to n-1 (for n features). For the nominal categories, I chose one hot encoding and dropped a specific column as the reference to prevent collinearity. After the feature encoding, I looked at the collinearity of the data and found a potential for multi-collinearity between some features. For this I ran the VIF (variance inflation factor) method and found that there were no values high enough to consider the variables collinear. After moving forward with the data, I noticed an imbalance in the attrition outcome. So with this I tried several methods to deal with it: random oversampling, random undersampling, SMOTE, and Tomek links for the two algorithms: logistic regression and decision tree. Logistic regression was used to see which feature(s) had the biggest effects on attrition and the decision tree was used to best predict attrition (due to it performing the best on imbalanced data). For the logistic regression, I used statsmodels in order to calculate the p-value for the coefficients to determine which were considered the most statistically significant effect on attrition. At the end of the models, I wrote a report comparing the methods and algorithms used plus the insights gained.
					</p>
					<p>
						I only had about a week (part-time) to work on this and naturally that would mean there are improvements to be done. For instance, I would interpret the logistic regression coefficients in a more meaningful way by transforming the coefficients (e^coefficient) to determine how much more likely a person is to leave for each specific feature.
					</p>
					<!-- <h6>Highlights of Learning:</h6> -->
					<p>
						
					</p>
				</div>
			</li>
		</ul>


		<div class="hr">
		</div>

	</div>
</div>

<div class="row">
	<div class="twelve columns">

		<!-- Lecture Material -->
		<div class="circledate">
			<div class="textcircle">
				<br>Class
			</div>
		</div>
		<h4>Lecture Material</h4>
		<div class="dots blogdots">
		</div>
		<p>
			In the tabs below, you will find a summary of the biggest take aways from the lectures. There was a lot of good material covered in the short time Professor Alvarez had but I just wanted to document the summary of the content.
		</p>
		<div class="hr">
		</div>

		<!-- Accordion: 2 KNN -->
		<ul class="accordion">
			<li class="">
				<div class="title">
					<b>k-Nearest Neighbour</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li>

		<!-- Accordion: 3 Decision Trees -->
			<li class="">
				<div class="title">
					<b>Decision Trees</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li>

		<!-- Accordion: 4 Bias-Variance Decomposition -->
			<li class="">
				<div class="title">
					<b>Bias-Variance Decomposition</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 5 Ensemble Methods & Random Forest   -->
			<li class="">
				<div class="title">
					<b>Ensemble Methods & Random Forest</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 6 Linear Regression-->
			<li class="">
				<div class="title">
					<b>Linear and Logistic Regression</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

			<!-- Accordion: 7 Linear Classification (binary and Multi-class)-->
			<li class="">
				<div class="title">
					<b>Linear Classification</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 8 Introduction to Neural Networks -->
			<li class="">
				<div class="title">
					<b>Introduction to Neural Networks</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 9 Optimization Methods-->
			<li class="">
				<div class="title">
					<b>Optimization Methods</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 10 Overfitting and Regularization -->
			<li class="">
				<div class="title">
					<b>Overfitting and Regularization</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 11 Unsupervised Learning (PCA, k-Means) -->
			<li class="">
				<div class="title">
					<b>Unsupervised Learning (PCA)</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
				</div>
			</li> 

		<!-- Accordion: 12 Fairness in ML -->
<!-- 			<li class="">
				<div class="title">
					<b>Fairness in ML</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						This is a big topic that needs to be regarded whenever you use machine learning. It is important to consider 
					</p>
				</div>
			</li>  -->

	<div class="hr">
	</div>
	</div>
</div>

<div class="row">
	<div class="twelve columns">
		
		<!-- Researched Extras -->
		<div class="circledate">
			<div class="textcircle">
				<br>Extras
			</div>
		</div>
		<h4>Researched Extras</h4>
		<div class="dots blogdots">
		</div>
		<p>
			Along the way I spent time researching different aspects of the course that I <strike>wanted</strike> needed to know more about
		</p>

		<!-- Accordion: Statistical Model -->
			<ul class="accordion">
			<li class="">
				<div class="title">
					<b>General ML Model Concepts</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
					
					<!-- Tabs-->
					<dl class="tabs">
						<dd class="active"><a href="#overunderfit">Overfit vs Underfit</a></dd>
						<dd><a href="#hyperparameters">Hyperparameters</a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="overunderfitTab">
							<a href="https://www.edureka.co/community/15471/what-are-the-differences-between-overfitting-underfitting" class="saymore">From this article</a> I got a good understanding of overfitting and underfitting, along with the notes taken in class.<br>
							<u>Overfitting:</u><br>
							<p>
								When overfitting occurs it means the model is picking up on noise instead of the relationship. This occurs if your model is too complex. For instances, small k or large depth for a decision tree.<br>
								When looking at variance (the amount of variability in the data) we are talking about overfitting (for squared error).
							</p>

							<u>Underfitting:</u><br>
							<p>
								When underfitting occurs the model has failed to have enough insights to get the relationship of the data. This occurs if a model is too simple or uses the wrong type of model. For instance, large k or small depth for decision tree or fitting a linear model on non-linear data.<br>
								When talking about bias (how wrong the prediction is) we are talking about undercutting (for squared error).
							</p>

						</li>

						<li id="hyperparametersTab">
							https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac# 
						</li>

					</ul>	
				</div>
			</li>



		<!-- Accordion: Decision Trees -->
			<li class="">
				<div class="title">
					<b>Decision Trees</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
					
					<!-- Tabs-->
					<dl class="tabs">
						<dd class="active"><a href="#decisionalgorithm">Decision Tree Algorithm Types</a></dd>
						<dd><a href="#pruning">Pruning a Tree</a></dd>
						<dd><a href="#dtrules">Extracting Decision Tree Rules</a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="decisionalgorithmTab">
							https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html  
						</li>

						<li id="pruningTab">
							<a href="https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09#" class="saymore">This article</a> had a lot of good ways to avoid overfitting. In it "pruning a tree" was discussed.<br>

							We use "pruning a tree" to avoid overfitting. The method works by setting limits on specific hyperparameters. These hyperparameters for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="saymore">DecisionTreeClassifier()</a> include max_depth, min_samples_leaf, and min_samples_split and can early stop the growth of the tree:
							<ul class="disc">
								<li>
									Max_depth: if you decrease the max_depth allowed, you will decrease the overfitting.
								</li>
								<li>
									Min_samples_leaf: "The minimum number of samples required to be at a leaf node" (default 1). The larger this number the more overfitting is decreased.
								</li>
								<li>
									Min_samples_split: "The minimum number of samples required to split an internal node" (default 2). The larger this number the more overfitting is decreased.
								</li>
							</ul>

						</li>

						<li id="dtrulesTab">
							https://mljar.com/blog/extract-rules-decision-tree/ <br>
							https://mljar.com/blog/visualize-decision-tree/ <- great for visualization
						</li>

					</ul>	

				</div>
			</li>

			<!-- Accordion: k-Means -->
			<li class="">
				<div class="title">
					<b>k-Means</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
					
					<!-- Tabs-->
					<!-- <dl class="tabs">
						<dd class="active"><a href="#decisionalgorithm">Decision Tree Algorithm Types</a></dd>
						<dd><a href="#pruning">Pruning a Tree</a></dd>
						
					</dl>

					<ul class="tabs-content">
						<li class="active" id="decisionalgorithmTab">

						</li>

						<li id="hyperparametersTab">
							
						</li>

						<li id="dtrulesTab">
							
						</li>

					</ul>	
 -->
				</div>
			</li>





		</ul>

	</div>
</div>








					<!-- Tabs-->
					<!-- <dl class="tabs">
						<dd class="active"><a href="#datatype"></a></dd>
						<dd><a href="#datacleaning"></a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="datatypeTab">
						</li>

						<li id="datacleaningTab">
							
						</li>
					</ul>	 -->


<div class="hr">
</div>

<!-- FOOOTER 
================================================== -->
<div id="footer">
	<footer class="row">
	<p class="back-top floatright">
		<a href="#top"><span></span></a>
	</p>
	<center>
		<div class="twelve columns">
			<h1>Connect with Me</h1>
			<a class="social github" href="https://github.com/anjawu"></a>
			<a class="social linkedin" href="https://www.linkedin.com/in/anja-wu/"></a>
		</div>
	</center>
	</footer>
</div>
<div class="copyright">
	<div class="row">
		<div class="six columns">
			 &copy;<span class="small"> Copyright 2021 Anja Wu</span>
		</div>
		<div class="six columns">
			<span class="small floatright"> Template by <a href="www.wowthemes.net">WowThemes.net</a></span>
		</div>
	</div>
</div>
<!-- JAVASCRIPTS 
================================================== -->
<!-- Javascript files placed here for faster loading -->
<script src="javascripts/foundation.min.js"></script>
<script src="javascripts/jquery.easing.1.3.js"></script>
<script src="javascripts/elasticslideshow.js"></script>
<script src="javascripts/jquery.carouFredSel-6.0.5-packed.js"></script>
<script src="javascripts/jquery.cycle.js"></script>
<script src="javascripts/app.js"></script>
<script src="javascripts/modernizr.foundation.js"></script>
<script src="javascripts/slidepanel.js"></script>
<script src="javascripts/scrolltotop.js"></script>
<script src="javascripts/hoverIntent.js"></script>
<script src="javascripts/superfish.js"></script>
<script src="javascripts/responsivemenu.js"></script>
</body>
</html>