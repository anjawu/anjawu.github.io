<!DOCTYPE html>
<!-- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->
<head>
<meta charset="utf-8"/>
<!-- Set the viewport width to device width for mobile -->
<meta name="viewport" content="width=device-width"/>
<title>Anja's Machine Learning</title>
<!-- CSS Files-->
<link rel="stylesheet" href="stylesheets/style.css">

<link rel="stylesheet" href="stylesheets/skins/blue.css">
<!-- skin color -->
<link rel="stylesheet" href="stylesheets/responsive.css">
<!-- IE Fix for HTML5 Tags -->
<!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-216360630-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-216360630-1');
</script>

<!-- creating buckets for time spent on page -->
<script type="text/javascript">
function timer11(){gtag('event','1', {'event_category':'ml_TimeOnPage','event_label':'11-30 seconds','non_interaction':true});}
function timer31(){gtag('event','2', {'event_category':'ml_TimeOnPage','event_label':'31-60 seconds','non_interaction':true});}
function timer61(){gtag('event','3', {'event_category':'ml_TimeOnPage','event_label':'61-180 seconds','non_interaction':true});}
function timer181(){gtag('event','4', {'event_category':'ml_TimeOnPage','event_label':'181-360 seconds','non_interaction':true});}
function timer361(){gtag('event','5', {'event_category':'ml_TimeOnPage','event_label':'361-600 seconds','non_interaction':true});}
function timer601(){gtag('event','6', {'event_category':'ml_TimeOnPage','event_label':'601-1800 seconds','non_interaction':true});}
function timer1801(){gtag('event','7', {'event_category':'ml_TimeOnPage','event_label':'1801+ seconds','non_interaction':true});}
gtag('event','0', {'event_category':'ml_TimeOnPage','event_label':'0-10 seconds','non_interaction':true});
setTimeout(timer11,11000);
setTimeout(timer31,31000);
setTimeout(timer61,61000);
setTimeout(timer181,181000);
setTimeout(timer361,361000);
setTimeout(timer601,601000);
setTimeout(timer1801,1801000);
</script>

</head>
<body>
<!-- HIDDEN PANEL 
================================================== -->
<div id="panel">
	<div class="row">
		<div class="twelve columns">
			<img src="http://www.wowthemes.net/demo/studiofrancesca/images/info.png" class="pics" alt="info">
			<div class="infotext">
				 Thank you for visiting my website!
			</div>
		</div>
	</div>
</div>
<p class="slide">
	<a href="#" class="btn-slide"></a>
</p>
<!-- HEADER
================================================== -->
<div class="row">
	<div class="headerlogo four columns">
		<div class="logo">
			<a href="index.html">
			<h4>Anja Wu</h4>
			</a>
		</div>
	</div>
	<div class="headermenu eight columns noleftmarg">
		<nav id="nav-wrap">
		<ul id="main-menu" class="nav-bar sf-menu">
			<li class="current">
			<a href="index.html">Home</a>
			</li>
			<li>
			Coding Info
			<ul>
				<li><a href="sql.html">SQL</a></li>
				<li><a href="ganalytics.html">Google Data</a></li>
				<li><a href="libraries.html">Python Libraries</a></li>
				<!-- <li><a href="stats.html">Statistics</a></li> -->
				<li><a href="ml.html">Machine Learning</a></li>
				<li><a href="github.html">GitHub</a></li>
				<li><a href="funlearn.html">Fun Things Learnt</a></li>
				<li><a href="versus.html">Versus</a></li>
				<li><a href="environmentsetup.html">Environment Setup</a></li>
			</ul>
			</li>
			<li>
			<a href="projects.html">Projects</a>
			<ul>
				<li><a href="redlight.html">Redlight</a></li>
				<li><a href="costco.html">Costco</a></li>
				<li><a href="schoolboard.html">School Boards</a></li>				
				<li><a href="website.html">Website</a></li>
				<li><a href="indeed.html">Indeed</a></li>
				<li><a href="imdb.html">IMDb</a></li>
				<li><a href="hangman.html">Hangman</a></li>
			</ul>
			</li>
<!-- 			<li>
			<a href="contact.html">Contact</a>
			</li> -->
		</ul>
		</nav>
	</div>
</div>
<div class="clear">
</div>
<!-- SUBHEADER
================================================== -->
<div id="subheader">
	<div class="row">
		<div class="twelve columns">
			<p class="left">
				Machine Learning
			</p>
		</div>
	</div>
</div>
<div class="hr">
</div>
<!-- CONTENT 
================================================== -->
<div class="row">
	<p>
		I recently completed the <a href ="https://vectorinstitute.ai/mothers-and-machine-learning/" class = saymore>Introduction to Machine Learning</a> run by the Vector Institute with Juan Felipe Carrasquilla √Ålvarez as the professor. I really enjoyed the course and learnt so much!
	</p>
	<p>
		All the assignments have been explained and a link to the GitHub section provided. I also summarized my main lecture take aways!
	</p>
</div>


<div class="row">
	<div class="twelve columns">

		<!-- Assignments -->
		<div class="circledate">
			<div class="textcircle">
				<br>Projects
			</div>
		</div>
		<h4>Assignments</h4>
		<div class="dots blogdots">
		</div>
		<p>
			This section has the best take aways from each assignment as opposed to just posting the whole assignment. I link to the GitHub repo that has all of the code posted.
		</p>

		<!-- Accordion: Assignment 1-->
			<ul class="accordion">
			<li class="">
				<div class="title">
					<b>k-Nearest Neighbours Assignment</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H1" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
							<li>"Anja_H1_Question_2_KNN_2Features.ipynb": has the analysis and selection of the "k" hyperparameter in the Iris database from sklearn focusing on just two features (the sepal ones) to predict classification of three species.  </li>
							<li>"Anja_H1_Question_2_KNN_4Features.ipynb": has the above k-Nearest Neighbours analysis but done using all four features for the prediction of the species classification.</li>
						</ul>
					</p>
					 
					<h6>Highlights of Learning</h6> 
					Steps
					<ol>
						<li><a href="#a1s1" class="saymore">Defining a data set from the Iris dataset</a></li>
						<li><a href="#a1s2" class="saymore">Splitting training and test set</a></li>
						<li><a href="#a1s3" class="saymore">Creating scatter plot to visualize</a></li>
						<li><a href="#a1s4" class="saymore">Creating function that will print out the features from the integers in the dataset</a></li>
						<li><a href="#a1s5" class="saymore">Creating Euclidean distance function</a></li>
						<li><a href="#a1s6" class="saymore">Printing out nearest neighbours</a></li>
						<li><a href="#a1s7" class="saymore">Create heatmap to analyze the training set distance from the test set distance</a></li>
						<li><a href="#a1s8" class="saymore">Creating a line graph to show test accuracy based on k</a></li>
					</ol>
					<br><br>
					<ol>
						<li id="a1s1">
							<b>Defining a data set from the Iris dataset (did 2 then 4 features):</b><br>
							Taking the first 2 features from the data matrix:
							<div id="testimonials"><blockquote>
								X = iris.data[:, :2]<br>
								y = iris.target # The class labels
							</blockquote></div>
							Extending the matrix to all four features:
							<div id="testimonials"><blockquote>								
								X = iris.data[:, :4]<br>
								y = iris.target
							</blockquote></div>
						</li><br>

						<li id="a1s2">
							<b>Splitting training and test set:</b><br>
							<div id="testimonials"><blockquote>								
								from sklearn.model_selection import train_test_split
							</blockquote></div>
							We set 20% of the dataset as the test set, and 80% as the training set
							<div id="testimonials"><blockquote>								
								X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
							</blockquote></div>
						</li><br>

						<li id="a1s3">
							<b>Creating scatter plot to visualize (two different sets):</b><br>
							<u>For the first two features</u><br>
							Making a scatterplot for two features (sepal) and three species:
							<div id="testimonials"><blockquote>								
								f, axs = plt.subplots(figsize=(8,6))<br>
								the_scatter = axs.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')
							</blockquote></div>

							Change the legend name of the class to be Iris species like Setosa, Versicolour, and Virginica:
							<div id="testimonials"><blockquote>								
								lines, legend_names = the_scatter.legend_elements()<br>
								legend1 = axs.legend(lines, ['Setosa', 'Versicolour', 'Virginica'], title="Classes")<br>
								axs.add_artist(legend1)
							</blockquote></div>
							<img src="images/pages/ml/a1-g1-sepal.png" width = "385" height = "auto" alt=""><br>

							<u>For the last two features</u><br>
							Making a scatterplot for two features (petal) and three species:
							<div id="testimonials"><blockquote>								
								f, axs = plt.subplots(figsize=(8,6))<br>
								the_scatter = axs.scatter(X[:, 2], X[:, 3], c=y, cmap=plt.cm.Set1, edgecolor='k')
							</blockquote></div>
							<img src="images/pages/ml/a1-g2-petal.png" width = "385" height = "auto" alt=""><br><br>

							As can be seen two of the features (petal) give a lot more information in the boundary decisions. We would expect when we include them for our predictions to be done well.
						</li><br>

						<li id="a1s4"> 
							<b>Creating function that will print out the features from the integers in the dataset:</b><br>
							Due to the fact that the categorical variables needed to be encoded, we must pull out the type in order for the values to be interpretable in our data. So we define a function <i>print_features(x,y)</i> that will print out the Iris type and sepal width/length, in a clean format.
							<div id="testimonials"><blockquote>								
							def print_features(x,y):
							<div class="tab">
								print('Iris type:', ['Setosa', 'Versicolour', 'Virginica'][y-1])<br>
								print('Sepal Length: %.1f \t Sepal Width: %.1f'%(x[0], x[1]))  
							</div>
						</blockquote></div>
						</li><br>

						<li id="a1s5">
							<b>Creating Euclidean distance function:</b><br>
							This was done for fun to check if the same neighbours would be returned as using the k-nearest neighbour function from sklearn (to further understand what was occuring "under the hood" for the algorithm.
							<div id="testimonials"><blockquote>								
								def euclidean_distance(x1, x2):
								<div class="tab">
									distance = sqrt(np.sum((x2-x1)**2))<br>
									return distance
								</div>
							</blockquote></div>
						</li><br>

						<li id="a1s6">
							<b>Printing out nearest neighbours:</b><br>
							<u>Using my Euclidean distance formula</u><br>
							First, we randomly select a test example (#11) and print features so we can compare to the nearest neightbours:
							<div id="testimonials"><blockquote>
								sample = X_test[10]<br>
								print('Test Sample:')<br>
								print_features(sample, y_test[10])
							</blockquote></div>

							Next, we calculate the euclidean distance to this test example:
							<div id="testimonials"><blockquote>
								distances = []<br>
								for i, row in enumerate(X_train):
								<div class="tab">
								    distance = euclidean_distance(sample, row)<br>
								#     print(f"{i}: {distance} from x1: {sample} and x2: {row}") # Checking calculations above for correctedness<br>
								    distances.append((i, distance))
								</div>
								distances.sort(key=lambda tup: tup[1]) # sorting distances
							</blockquote></div>

							Finally, we print the closest 2 neighbours with their features for comparison:
							<div id="testimonials"><blockquote>
								k = 2 # Number of nearest neighbors<br>
								print('\nTop %d Nearest Neighbors:' % k)<br>
								for nn in range(k):
								<div class="tab">
								    print_features(X_train[distances[nn][0]], y_train[distances[nn][0]])
								</div>
							</blockquote></div><br>

							<u>Using sklearn k-NN</u><br>
							Import function from library:
							<div id="testimonials"><blockquote>
								from sklearn.neighbors import KNeighborsClassifier
							</blockquote></div>
							Just like before, we select test example #11 and print features so we can compare to the nearest neightbours. Then we use the sklearn function <i>KNeighborsClassifier()</i> to specify looking at the top 5 neighbours:
							<div id="testimonials"><blockquote>
								k=5<br>
								neigh = KNeighborsClassifier()<br>
								neigh.fit(X_train, y_train)<br>
								dists, neighbor_ids = neigh.kneighbors(X=[sample], n_neighbors = 5)
							</blockquote></div>
							Finally, we print the closest 5 neighbours with their features for comparison:

							<div id="testimonials"><blockquote>
								print('\nClosest 5 neighbors to this test sample:')<br>
								for knn in range(5):
								<div class="tab">
								    print('\nNeighbor %d ===> distance:%f'%(knn, dists[0][knn]))<br>
								    print_features(X_train[neighbor_ids[0][knn]], y_train[neighbor_ids[0][knn]])
								</div>
							</blockquote></div><br>

							From the results, both produced the same top 2 nearest neighbours. This makes sense because in reading the documentation for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" class="saymore">KNeighborsClassifier()</a> the metric is technically Minkowski, but when p=2 (power parameter) it is equivalent to Euclidean metric. Since the default is p=2, we know that the calculations should be the same.
						</li><br>

						<li id="a1s7">
							<b>Create heatmap to analyze the training set distance from the test set distance:</b><br>
							Creating list to store distances between the test set and the training set
							<div id="testimonials"><blockquote>
								distances = []<br>
								for x_test in X_test:
								<div class="tab">
								    distance = np.sum((x_test[np.newaxis, ...] - X_train) ** 2, axis=1)<br>
								    distances.append(distance)
								</div>
							</blockquote></div>

							Creating a "colorbar" graph to display distance from each training example to each test example
							<div id="testimonials"><blockquote>
								distances = np.array(distances)<br>
								plt.figure(figsize=(50, 20))<br>
								plt.imshow(distances)<br>
								plt.colorbar()<br>
								plt.xlabel('Training examples id', fontsize=40)<br>
								plt.ylabel('Test examples id', fontsize=40)<br>
								plt.xticks(np.arange(0, 120, 2), fontsize=20)<br>
								plt.yticks(np.arange(0, 30, 1), fontsize=20)
								plt.show
							</blockquote></div>
							<img src="images/pages/ml/a1-heatmap.png" width = "600" height = "auto" alt=""><br><br>

							<b><i>Note: a good reason to check this is because we want the distances between the sets to be small. This is when k-Nearest Neighbours works best. If the heatmap shows a lot more further distances, this means the sample was not done well.</i></b>
						</li><br>

						<li id="a1s8">
							<b>Creating a line graph to show test accuracy based on k:</b><br>
							Creating list of all test accuracies based on changes in k
							<div id="testimonials"><blockquote>
								test_accs = []<br>
								for k in range(1, X_train.shape[0]):
								<div class="tab">
								  # Create K nearest neighbors classifier<br>
								  neigh = KNeighborsClassifier(n_neighbors=k)<br>
								  neigh.fit(X_train, y_train)<br><br>
								  
								  # Prediction<br>
								  y_pred = neigh.predict(X_test)<br><br>
								  
								  # Calculate accuracy<br>
								  acc = (y_pred == y_test).mean()<br>
								  test_accs.append(acc)
								</div>
							</blockquote></div>
							Creating a line graph to depict accuracy to be able to find the best hyperparameter (in this case, k value) visually:
							<div id="testimonials"><blockquote>
								plt.figure(figsize=(30, 10))<br>
								plt.plot(list(range(1, X_train.shape[0])), test_accs)<br>
								plt.axhline(y=0.93, color='r', linestyle='-')<br>
								plt.axvline(x=29, color='b', linestyle='--')<br>
								plt.xlabel('Number of nearest neighbors (k)')<br>
								plt.ylabel('Test set accuracy')<br>
								plt.xticks(np.arange(0, 120, 2), fontsize=18)<br>
								plt.yticks(np.arange(0, 1.0, 0.05), fontsize=18)<br>
								plt.grid()<br>
								plt.show
							</blockquote></div>
							<u>For 2 features:</u><br>
							<img src="images/pages/ml/a1-2feature-accuracy.png" width = "600" height = "auto" alt=""><br><br>
							<b><i>We can see the accuracy of the test set decreasing the larger k-values we have due to the fact that we are underfitting the model on the training set leading to a decrease in finding the underlying relationship. At a low level of k, we can see that the accuracy performs more poorly due to overfitting on the training model and capturing more noise than the underlying relationship.</i></b><br><br>
							<u>For 4 features:</u><br>
							<img src="images/pages/ml/a1-4feature-accuracy.png" width = "600" height = "auto" alt=""><br><br>
							<b><i>As can be seen the more features that are added the more accurate the predictions. </i></b><br>
							
						</li><br>
					</ol>
					<b>*Note*</b>
					<p>
						After this assignment we discussed the fact that the data should be divided into three sections: training set, validation set, and test set. This way the test set is only used once to check accuracy and the validation set is used to determine the best hyperparameters. 
					</p>
				</div>
			</li>

		<!-- Accordion: Assignment 2-->
			<li class="">
				<div class="title">
					<b>Decision Trees Assignment </b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H2" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
							<li>"AnjaWu_H2_DecisionTrees.ipynb": has a decision tree that was created from Kaggle real and fake news headlines.</li>
						</ul>
					</p>
					<h6>Highlights of Learning:</h6>
					<p>
						Steps
					<ol>
						<li><a href="#a2s1" class="saymore">Create data splitting function</a></li>
						<li><a href="#a2s2" class="saymore">Vectorizing titles from news headlines</a></li>
						<li><a href="#a2s3" class="saymore">Creating accuracy calculation function</a></li>
						<li><a href="#a2s4" class="saymore">Checking max depth of trees based on criterion</a></li>
						<li><a href="#a2s5" class="saymore">Creating loop to check optimal depth and criterion</a></li>
						<li><a href="#a2s6" class="saymore">Plotting depth and criterion to visually be able to select best based on accuracy of val and train set</a></li>
						<li><a href="#a2s7" class="saymore">Testing test accuracy for selected model</a></li>
						<li><a href="#a2s8" class="saymore">Visualizing decision tree</a></li>
					</ol>
					<br>
					</p>
					<ol>
						<li id="a2s1">
							<b>Create data splitting function (into three sections: training, validation, and test set):</b><br>
							We will define a function to split the data for us
							<div id="testimonials"><blockquote>
								def split_data(X, y, train_size=0.7, val_size=0.15):
							</blockquote></div>
							Then we have to figure out the shape of our data to be able to split the data properly.
							<div id="testimonials"><blockquote class="tab"> 
								total_data = X.shape[0] 
							</blockquote></div>
							After we have the shape, we can specify the integers required for the size of each set based on the inputted data and the <i>train_size</i> and <i>val_size</i> specified.
							<div id="testimonials"><blockquote class="tab"> 
								train_size = int(train_size * total_data) <br>
					   		val_size = int(val_size * total_data) <br>
					    	test_size = total_data - train_size - val_size 
							</blockquote></div>
							Now we want to make sure that the data is randomized instead of being split sequentially:
							<div id="testimonials"><blockquote class="tab"> 
								all_indices = np.random.permutation(np.arange(total_data)) 
							</blockquote></div>
							Then we will split all the indices based on the size specified (.70 for training, .15 for val and .15 for test):
							<div id="testimonials"><blockquote class="tab"> 
								train_indices = all_indices[:train_size]<br>
						    val_indices = all_indices[train_size:train_size + val_size]<br>
						    test_indices = all_indices[train_size+val_size:]
							</blockquote></div>
							Now we have to split our data into features and outcome. In this case, X being the words vectorized, y being the real or fake category.
							<div id="testimonials"><blockquote class="tab"> 
								train_X, train_y = X[train_indices], y[train_indices]<br>
						    val_X, val_y = X[val_indices], y[val_indices]<br>
						    test_X, test_y = X[test_indices], y[test_indices]
							</blockquote></div>
							Finally, we need to have our function return a dictionary so that it outputs our split datasets for all 3 along with the X and y.
							<div id="testimonials"><blockquote class="tab"> 
								return {
									<div class="tab">
						        'train': (train_X, train_y),<br>
						        'val':  (val_X, val_y),<br>
						        'test': (test_X, test_y)
						      </div>
						    }
							</blockquote></div>
						</li>

						<li id="a2s2">
							<b>Vectorizing titles from news headlines:</b>
							<p>
								Since the inputted data is words from headlines that then need to be classified by real or fake news, we need to vectorize the input in order to be able to breakdown the title into words.
							</p>
							<p>
								So we create a function to read the data and vectorize it, <i>load_data</i>.
							</p>
							<div id="testimonials"><blockquote>
								def load_data(paths):
							</blockquote></div>
							After we have loaded the data, we need to convert the text to a matrix of token counts using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" class="saymore">CountVectorizer()</a>. We also want to create a list for each title (to store words) and then a list to count how many words lines we have.
							<div id="testimonials"><blockquote class="tab">
								vec = CountVectorizer(input='content')<br>
						    lines = []<br>
						    counts = []
							</blockquote></div>
							Then we want to go through each line in the path, count how many there are and add each line to a list. <a href="https://www.w3schools.com/python/ref_file_readlines.asp" class="saymore">.readlines()</a> returns a list that has each line from the file. <a href="https://www.w3schools.com/python/ref_list_extend.asp" class="saymore">.extend()</a> allows the appending of more than one element to a list. 
							<div id="testimonials"><blockquote class="tab">
								for p in paths: 
								<div class="tab">
					        with open(p) as f:
					        <div class="tab">
					          file_lines = f.readlines()
					         </div>
					        counts.append(len(file_lines))<br>
					        lines.extend([l.strip() for l in file_lines])
					      </div>
							</blockquote></div>
							<p>
								<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit" class="saymore">.fit()</a> returns a vector corresponding to each vocabulary dictionary from all headlines. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform" class="saymore">.transform()</a> takes every line from the files makes them into a matrix. So we can combine those two using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#:~:text=Fitted%20vectorizer.-,fit_transform(raw_documents%2C%20y%3DNone),Document%2Dterm%20matrix.,-get_feature_names()" class="saymore"> .fit_transform()</a>. Then we make it into an array for ease of use.
							</p>
							<div id="testimonials"><blockquote class="tab">
								data_matrix = vec.fit_transform(lines).toarray()
							</blockquote></div>
							<p>
								Because the inputted data was in the form of real news titles first and then fake news titles next, we can do the following. An array of 0s is made for labels for real data and and array of 1s for labels of fake data. Then <a href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html" class="saymore">np.concatenate</a> combines the labels of real and fake news into one array.
							</p>
							<div id="testimonials"><blockquote class="tab">
								y = np.concatenate((np.zeros(counts[0]), np.ones(counts[1]))) 
							</blockquote></div>
							<p>
								Then we us our split_data() function to take the data_matrix we made above and puts it as X and the real/fake label as y and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#:~:text=get_feature_names_out(%5Binput_features%5D),names%20for%20transformation." class="saymore">.get_feature_names_out()</a> maps every single word that appears in all the headers and creates a vector.
							</p>
							<div id="testimonials"><blockquote class="tab">
								return split_data(data_matrix, y), vec.get_feature_names_out()
							</blockquote></div>
							<p>
								Using the above function, we pull out the data and features from the two text files: real news titles and fake news titles.
							</p>
							<div id="testimonials"><blockquote>
								data, feature_names = load_data(['/content/clean_real.txt', '/content/clean_fake.txt'])
							</blockquote></div>
						</li>

						<li id="a2s3">
							<b>Creating accuracy calculation function</b><br>
							<p>
								We create a function to compute the accuracy of a given model on input data X and label t.
							</p>
							<div id="testimonials"><blockquote>
								def get_acc(model, X, t):
								<div class="tab">
							    y_pred = model.predict(X)<br>
							    y_test = t<br>
							    acc = (y_pred == y_test).mean()<br>
							    return acc
							  </div>
							</blockquote></div>
						</li>

						<li id="a2s4">
							<b>Checking max depth of trees based on criterion</b><br>
							<p>
								We define a decision tree with the criterion of choice (need to test for each criterion you want as you can get different depths) and print out the <i>max_depth</i> so we have some idea of the depth we should be testing for the best depth hyperparameter. Remember that the tree must be for onto our training dataset. Since the fit takes X and y, we can write <i>*</i> to "unpack" the <i>data['train']</i> dictionary to get both the X and y lists.
							</p>
							<div id="testimonials"><blockquote>
								clf = tree.DecisionTreeClassifier(criterion = 'entropy')<br>
								clf = clf.fit(*data['train'])<br>
								print(clf.tree_.max_depth)
							</blockquote></div>
						</li>

						<li id="a2s5">
							<b>Creating loop to check optimal depth and criterion</b><br>
							<p>
								We create a list of depths we want to explore. These are selected with reasonable spacing, not exceeding the <i>max_depth</i> by much. 
							</p>
							<div id="testimonials"><blockquote>
								depths = [1, 5, 8, 10, 15, 30, 35, 45, 50, 55, 60, 65, 70, 75, 80, 90, 100, 105, 140] 
							</blockquote></div>
							<p>
								Since we are not sure whether we want to use entropy or Gini as the criterion, we must test both. For Entropy: train the models with the entropy criterion then we list depths and data as variables so when we loop through we can test different depths and use not our training and validation datasets. 
							</p>
							<div id="testimonials"><blockquote>
								res_entropy = select_model(depths,data, "entropy")
							</blockquote></div>
							<p>
								Since we want to track the best depth and best accuracy, we create variables and set them to nothing.
							</p>
							<div id="testimonials"><blockquote>
								best_d_entropy = None<br>
								best_acc_entropy = 0
							</blockquote></div>
							<p>
								Then we loop over the various depths, while training the model on the training set and checking the accuracy on the validation set. different models and accuracies to find the optimal model according to its validation accuracy
								<div id="testimonials"><blockquote>
									for d in res_entropy:
									<div class="tab">
								    val_acc = res_entropy[d]['val']<br>
								    print("Depth: {}   Train: {}    Val: {}".format(d, res_entropy[d]['train'], val_acc))<br>
								    if val_acc  > best_acc_entropy:
								    <div class="tab">
							        best_d_entropy = d<br>
							        best_acc_entropy = val_acc
								     </div>
								  </div>
								</blockquote></div>
							</p>
							<p>
								Now we can run the exact same process but this time using our select_model() function we can specify "gini":
								<div id="testimonials"><blockquote>
									res_gini = select_model(depths,data,"gini")
								</blockquote></div>
							</p>
							<p>
								We would end up with an output like this
							</p>
							<center><img src="images/pages/ml/a2-acc-readout.png" width="300" height=‚Äùauto‚Äù alt=""></center>
						</li>

						<li id="a2s6">
							<b>Plotting depth and criterion to visually be able to select best based on accuracy of val and train set</b><br>
							<p>
								First, we have to store all the accuracy values for both the training and validation sets.
							</p>
							<div id="testimonials"><blockquote>
								entropy_val = []
								entropy_train = []<br>
								depth_val = []<br>
							</blockquote></div>
							<p>
								Then we create a loop to store every accuracy value for every depth value in the list specified above.
							</p>
							<div id="testimonials"><blockquote>
								for d in res_entropy:
								<div class="tab">
							    entropy_val.append(res_entropy[d]['val'])<br>
							    entropy_train.append(res_entropy[d]['train'])<br>
							    depth_val.append(d)
							  </div>
							</blockquote></div>
							<p>
								We can do the exact same thing for the gini criterion.
							</p>
							<p>
								We want to calculate the absolute minimum and maximum of the accuracies to be able to set the Y-axis for the graph.
							</p>
							<div id="testimonials"><blockquote>
								minimum_acc = min(min(gini_train), min(gini_val), min(entropy_train), min(entropy_val))<br>
								maximum_acc = max(max(gini_train), max(gini_val), max(entropy_train), max(entropy_val))
							</blockquote></div>
							<p>
								Next, we can use <a href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html" class="saymore">np.argmax()</a> to get the corresponding depth (x-valur) for the maximum validation set accuracy. This is used to make a vertical line to show us where the maximum accuracies occur.
							</p>
							<div id="testimonials"><blockquote>
								entropy_max_x = depth_val[np.argmax(entropy_val)]
								gini_max_x = depth_val[np.argmax(gini_val)]
							</blockquote></div>
							<p>
								Next, we can create the actual line graph comparing Entropy and Gini criterion, with varying depths.
							</p>
							<div id="testimonials"><blockquote>
								plt.figure(figsize=(30, 10))<br>
								plt.plot(depth_val, entropy_train, label = "IG Train Dataset", color="rebeccapurple", linewidth=4)<br>
								plt.plot(depth_val, entropy_val, label = "IG Val Dataset", color="mediumorchid", linewidth=4)<br>
								plt.plot(depth_val, gini_train, label = "Gini Train Dataset", color="seagreen", linewidth=4)<br>
								plt.plot(depth_val, gini_val, label = "Gini Val Dataset", color="lightgreen", linewidth=4)<br><br>

								plt.title('Figure 1: Accuracy Comparison ', fontsize=25)<br>
								plt.xlabel('Depth Value', fontsize=20)<br>
								plt.ylabel('Accuracy', fontsize=20)<br>
								plt.legend(bbox_to_anchor=(0.8, -0.1), ncol=4, fontsize=20)<br><br>

								# use <a href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html" class="saymore">np.arange()</a> to space out the axes evenly.<br>
								plt.xticks(np.arange(0, max(depth_val), 5), fontsize=18)<br>
								plt.yticks(np.arange(minimum_acc, maximum_acc, 0.02), fontsize=18)<br><br>

								# Looking for x and y coordinates for the maximum accuracy for both models<br>
								plt.axhline(y = max(entropy_val), color='r', linestyle='dashed')<br>
								plt.axvline(x=entropy_max_x, color='r', linestyle='dashed')<br>
								plt.axhline(y = max(gini_val), color='g', linestyle='dotted')<br>
								plt.axvline(x=gini_max_x, color='g', linestyle='dotted')<br><br>

								plt.grid()<br>
								plt.show
							</blockquote></div>
							<p>
								We end up with
							</p>
							<center><img src="images/pages/ml/a2-acc-linegraph.png" width="600" height=‚Äùauto‚Äù alt=""></center>
						</li>

						<li id="a2s7">
							<b>Testing test accuracy for selected model</b><br>
							<p>
								Once we have run the above several times we can find the best hyperparameters to test the model against the test set.
							</p>
							<div id="testimonials"><blockquote>
								test_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 80)<br>
								test_model = test_model.fit(data['train'][0], data['train'][1])<br>
								get_acc(test_model, *data['test'])
							</blockquote></div>
						</li>

						<li id="a2s8">
							<b>Visualizing decision tree</b><br>
							<p>
								I did this with two different ways: using tree.plot_tree and the other editing the .dot file that was produced from tree_viz. I found a <a href="https://mljar.com/blog/visualize-decision-tree/" class="saymore">great resource</a> that goes through several ways to visualize trees. 
							</p>
							<ol>
								<li>
									Using plot_tree function from sklearn tree package
								</li>
								<p>
									First, specify figure size and then the tree you want to draw (same as when it was fitted before)
								</p>
								<div id="testimonials"><blockquote>
									fig = plt.figure(figsize=(15,10))<br>
									clf = tree.DecisionTreeClassifier(max_depth=80, criterion = 'entropy')<br>
									clf = clf.fit(*data['train'])
								</blockquote></div>
								<p>
									After that we can display the top two layers. We use "<i>_</i>" to allow us to skip showing the annotations that are returned using <i>plot_tree</i>.
								</p>
								<div id="testimonials"><blockquote>
									_ = tree.plot_tree(clf, feature_names = vec.get_feature_names_out(), filled=True, max_depth=1)
								</blockquote></div>
								<center><img src="images/pages/ml/a2-dt.png" width="300" height=‚Äùauto‚Äù alt=""></center>
								<li>
									Using .dot file edits
								</li>
								<p>
									First, we must fit the tree like we did before (saved as tree_viz). Then we must export it to dot format using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html" class="saymore">.export_graphviz</a> from sklearn.
								</p>
								<div id="testimonials"><blockquote>
									dot_data_tree = tree.export_graphviz(tree_viz, out_file=None, max_depth=2, feature_names = vec.get_feature_names_out(), class_names=['Real news','Fake news'], filled=True)
								</blockquote></div>
								<p>
									If we put <i>out_file=None</i>, then once we compile the following code, it will render a decision tree similar to our previous version.  We must import graphviz and use <i>.Source()</i> to create the PNG format for the rendering of the .dot file.
								</p>
								<div id="testimonials"><blockquote>
									import graphviz<br>
									graph = graphviz.Source(dot_data_tree, format="png") <br>
									graph
								</blockquote></div>
								<p>
									Produces this
								</p>
								<center><img src="images/pages/ml/a2-dt-dot.png" width="400" height=‚Äùauto‚Äù alt=""></center>
								<p>
									Now, if we wanted to use the .dot file to make more detailed changes, in the original line exporting the tree the <i>out_file</i> must be changed to contain the name of choice for tree ending with ".dot".
								</p>
								<div id="testimonials"><blockquote>
									dot_data_tree = tree.export_graphviz(tree_viz, out_file='name.dot', max_depth=2, feature_names = vec.get_feature_names_out(), class_names=['Real news','Fake news'], filled=True)
								</blockquote></div>
								<p>
									Once you compile the previous line, you will now have a .dot file you can open in your text editor. I will bold the parts I added/changed from the original .dot file.
								</p>
								<div id="testimonials"><blockquote>
									digraph Tree {
										node [shape=<b>Mdiamond</b>, fontname="helvetica",color=black,fontcolor=black];<br>
										// edge [fontname="helvetica", arrowhead = vee, <b>color = coral3</b>] ;<br>
										0 [label="the <= 0.5\nentropy = 0.974\nsamples = 2286\nvalue = [1360, 926]"] ;<br><br>

										node [<b>shape=tab</b>, fontname="helvetica",color=black,fontcolor=<b>blueviolet</b>] ;<br>
										edge [fontname="helvetica", arrowhead = <b>vee</b>, <b>color = blueviolet</b>] ; <br>
										1 [label="donald <= 0.5\nentropy = 0.932\nsamples = 1931\nvalue = [1259, 672]"] ;<br>
										0 -> 1 [labeldistance=2.5, labelangle=45];#, headlabel="True", <b>labelfontcolor=blueviolet</b>] ;<br>
										2 [label="hillary <= 0.5\nentropy = 0.985\nsamples = 1278\nvalue = [730, 548]"] ;<br>
										1 -> 2 ;<br>
										3 [label="...", shape=none] ;<br>
										2 -> 3 ;<br>
										// 348 [label="(...)"] ;<br>
										// 2 -> 348 ;<br>
										355 [label="hillary <= 0.5\nentropy = 0.701\nsamples = 653\nvalue = [529, 124]"] ;<br>
										1 -> 355 ;<br>
										356 [label="...", shape=none];<br>
										355 -> 356 ;<br>
										// 521 [label="(...)"] ;<br>
										// 355 -> 521 ;<br><br>

										node [<b>shape=tab</b>, fontname="helvetica",color=black,fontcolor=<b>coral3</b>] ;<br>
										edge [fontname="helvetica", <b>arrowhead = vee</b>, color = <b>coral3</b>] <br>
										544 [label="trumps <= 0.5\nentropy = 0.862\nsamples = 355\nvalue = [101, 254]"] ;<br>
										0 -> 544 [labeldistance=2.5, labelangle=-45];#, headlabel="False", labelfontcolor=<b>coral3</b>] ;<br>
										545 [label="it <= 0.5\nentropy = 0.83\nsamples = 343\nvalue = [90, 253]"] ;<br>
										544 -> 545 ;<br>
										// 546 [label="(...)"] ;<br>
										// 545 -> 546 ;<br>
										699 [label="...", shape=none];<br>
										545 -> 699 ;<br>
										700 [label="market <= 0.5\nentropy = 0.414\nsamples = 12\nvalue = [11, 1]"] ;<br>
										544 -> 700 ;<br>
										// 701 [label="(...)"] ;<br>
										// 700 -> 701 ;<br>
										702 [label="...", shape=none] ;<br>
										700 -> 702 ;<br>
										}
								</blockquote></div>
								<center><img src="images/pages/ml/a2-dt-dot2.png" width="400" height=‚Äùauto‚Äù alt=""></center>
							</ol>
						</li>
					</ol>
				</div>
			</li>
			
		<!-- Accordion: Assignment 3-->
			<li class="">
				<div class="title">
					<b>Regression and Classification</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H3" class="saymore">Here</a> is the link to the GitHub code part of the assignment:
						<ul class="disc">
              
							<li>"AnjaWu_HW3_code-Regression&Classification": has the creation of linear and logistic regression analysis on health data analyzed to predict hospital charges. Most code was done by a TA in the course, however, I chose to comment the code to further my understanding on how to build regression models using Scikit Learn.</li>
							<li>"AnjaWu_EntropyCalculations": has code that I made to calculate entropy from the assignment, rather than doing them all by hand.</li>
						</ul>
					</p>
					<h6>Highlights of Learning:</h6>
					<p>
						Steps
					<ol>
						<li><a href="#a3s1" class="saymore">Check data structure</a></li>
						<li><a href="#a3s2" class="saymore">Check data distribution for outcome variable</a></li>
						<li><a href="#a3s3" class="saymore">Check collinearity</a></li>
						<li><a href="#a3s4" class="saymore">Plot the variables to analyze distribution</a></li>
						<li><a href="#a3s5" class="saymore">Changing BMI value to ordinal categorical labels</a></li>
						<li><a href="#a3s6" class="saymore">Plot the independent variables vs charges to see if there are any insights to be gained</a></li>
						<li><a href="#a3s7" class="saymore">Encoding values</a></li>
						<li><a href="#a3s8" class="saymore">Splitting data</a></li>
						<li><a href="#a3s9" class="saymore">Instantiate and fit the linear model on our training set to predict medical charges</a></li>
						<li><a href="#a3s10" class="saymore">Prediction y_test</a></li>
						<li><a href="#a3s11" class="saymore">Calculate MSE and MAE</a></li>
						<li><a href="#a3s12" class="saymore">Logistic model to predict smoker vs non-smoker</a></li>
					</ol>
					<br>
					</p>
					<ol>
						<li id="a3s1">
							<b>Check data structure:</b><br>
							<p>
								First, print out the number of rows and columns in the dataset
							</p>
							<div id="testimonials"><blockquote>
								print("Dimensionality of the DataFrame:", df.shape)
							</blockquote></div>
							<p>
								Then, we can print out summary statistics for each variable in the dataset, just to have a broad overview of the data.
							</p>
							<div id="testimonials"><blockquote>
								df.describe()
							</blockquote></div>
							<p>
								Print out the data type of each feature in the dataset.
							</p>
							<div id="testimonials"><blockquote>
								print("Data type of each feature:")<br>
								df.dtypes
							</blockquote></div>
							<p>
								Determine if there are any missing datapoints or duplicate rows in the dataset.
							</p>
							<div id="testimonials"><blockquote>
								print("\nAre there any missing datapoints in the dataset?", df.isnull().values.any())<br>
								print("Number of duplicated rows:", df.duplicated().sum())
							</blockquote></div>
							<p>
								Remove the duplicate row from the dataset (keeping the first iteration).
							</p>
							<div id="testimonials"><blockquote>
								df.drop_duplicates(keep='first', inplace=True)
							</blockquote></div>
							<p>
								Confirm the final number of rows and columns in the dataset after data cleaning.
							</p>
							<div id="testimonials"><blockquote>
								print("df.shape =", df.shape)<br>
								print("Number of rows =", df.shape[0])<br>
								print("Number of columns =", df.shape[1])
							</blockquote></div>
						</li>

						<li id="a3s2">
							<b>Check data distribution for outcome variable:</b><br>
							<p>
								Plot a histogram to show the distribution of our outcome variable: the "charges" variable. <a href="https://seaborn.pydata.org/generated/seaborn.histplot.html" class="saymore"><i>kde</a> = True</i> draws a line estimate of distribution using the kernel density estimate.
							</p>
							<div id="testimonials"><blockquote>
								import seaborn as sns<br><br>

								sns.histplot(df['charges'], kde=True)
							</blockquote></div>
							<p>
								Then we can use The Shapiro-Wilk test to test the null hypothesis that the data was drawn from a normal distribution. The distribution must be normal prior to doing linear regression. 
							</p>
							<div id="testimonials"><blockquote>
								from scipy import stats<br><br>

								print(stats.shapiro(df['charges']));
							</blockquote></div>
							<center><img src="images/pages/ml/a3-shapiro.png" width="400" height=‚Äùauto‚Äù alt=""></center>
							<p>
								The Shapiro-Wilk result is approximately 0.814 with extremely small p-value, therefore, the data was most likely NOT drawn from a normal distribution (enough evidence to reject null).
							</p>
							<p>
								<b>**We need to have the distribution be normal in order to do a linear regression, so we must transform the data.**</b>
							</p>
							<p>
								In order to transform the target variable to a normal distribution we can use the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html" class="saymore">.boxcox()</a>. The function transforms the data using Box-Cox power. We must specify the first index [0] because <i>stats.boxcox</i> returns a tuple (Box-Cox power transformed array & lambda max log-likelihood) and we only want the first index.
							</p>
							<div id="testimonials"><blockquote>
								charges_transformed = stats.boxcox(df['charges'])[0]
							</blockquote></div>
							<p>
								Then plot it to check.
							</p>
							<div id="testimonials"><blockquote>
								sns.histplot(charges_transformed, kde=True);
								plt.xlabel('charges (transformed)')
							</blockquote></div>
							<center><img src="images/pages/ml/a3-boxcox.png" width="400" height=‚Äùauto‚Äù alt=""></center>
						</li>

						<li id="a3s3">
							<b>Check collinearity:</b><br>
							<p>
								Low collinearity implies independence of variables. We need independent variables in order to have the most accurate regression model. We can use a heatmap to check for collinearity between variables. The darker the value the higher the correlation.
							</p>
							<div id="testimonials"><blockquote>
								sns.heatmap(df.corr(), cmap='Blues', annot=True);
							</blockquote></div>
							<center><img src="images/pages/ml/a3-collinearity.png" width="300" height=‚Äùauto‚Äù alt=""></center>
							<p>
								Correlation coefficient values below 0.3 are considered to be weak; 0.3-0.7 are moderate; >0.7 are strong. In this case, all correlations are 0.3 or below, so we can conclude that the variables are independent from one another.<br>
								<b>**Independence of variables is a condition for regression.**</b>
							</p>
						</li>

						<li id="a3s4">
							<b>Plot the variables to analyze distribution:</b><br>
							<p>
								Plot the distribution of age, BMI, and number of children to see what kind of data we are working with.
							</p>
							<div id="testimonials"><blockquote>
								fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(20,5))<br>
								sns.histplot(x=df['age'], kde=True, ax=ax0);<br>
								sns.histplot(x=df['bmi'], kde=True, ax=ax1);<br>
								sns.countplot(x=df['children'], ax=ax2);
							</blockquote></div>
							<center><img src="images/pages/ml/a3-3-indep.png" width="600" height=‚Äùauto‚Äù alt=""></center>
						</li>

						<li id="a3s5">
							<b>Changing BMI value to ordinal categorical labels:</b><br>
							<p>
								In order to get better understanding on the effect of BMI on charges, we can change the BMI value to the ordinal categorical labels normally used:  (1) underweight; (2) normal weight; (3) overweight; and (4) obese. We want to use <a href="https://numpy.org/doc/stable/reference/generated/numpy.select.html" class="saymore">np.select()</a> to returns array from input (condition) and output (labels) to add as a column in the original dataframe.
							</p>
							<div id="testimonials"><blockquote>
								conditions = [(df['bmi'] < 18.5),
								<div class="tab"> 
		              (df['bmi'] >= 18.5) & (df['bmi'] < 25),<br>
		              (df['bmi'] >= 25) & (df['bmi'] < 30),<br>
		              (df['bmi'] >= 30)]
		            </div>
		            labels = ['underweight', 'normal weight', 'overweight', 'obese']<br>
		            df['bmi_categories'] = np.select(conditions, labels)<br>
							</blockquote></div>
						</li>

						<li id="a3s6">
							<b>Plot the independent variables vs charges to see if there are any insights to be gained:</b><br>
							<div id="testimonials"><blockquote>
								fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(20,5))<br>
								sns.lineplot(x='age', y='charges', data=df, ax=ax0);<br>
								sns.barplot(x='bmi_categories', y='charges', data=df,<br>
								order=['underweight', 'normal weight', 'overweight', 'obese'], ax=ax1);<br>
								sns.barplot(x='children', y='charges', data=df, ax=ax2);<br>
							</blockquote></div>
							<center><img src="images/pages/ml/a3-1st-3-indep.png" width="600" height=‚Äùauto‚Äù alt=""></center>
							<p>
								We can see that for BMI there is a statistically significant difference for an increase in charges for someone who is obese. Also there seems to be a statistical significance for a decrease in charges for people with 5 children. Otherwise, just by the graphs we cannot see statistically significant differences.
							</p>
							<p>
								Same thing down for the other 3 independent variables. Keep in mind that .pointplot() returns point estimates and confidence intervals.
							</p>
							<div id="testimonials"><blockquote>
								fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(20,5))<br>
								sns.pointplot(x='sex', y='charges',data=df, ax=ax0);<br>
								sns.pointplot(x='smoker',y='charges', data=df, ax=ax1);<br>
								sns.pointplot(x='region',y='charges', data=df, ax=ax2);
							</blockquote></div>
							<center><img src="images/pages/ml/a3-2nd-3-indep.png" width="600" height=‚Äùauto‚Äù alt=""></center>
							<p>
								Neither sex nor region seems to have a statistically significant difference for charges but smoker vs non-smokers does have a statistically significant difference to the charges incurred.
							</p>
						</li>
						
						<li id="a3s7">
							<b>Encoding values</b><br>
							<p>
								Now that we have analyzed all the graphs we want to do a linear regression to predict the medical charges that a person will most likely incur based on the features given. In order to do that we have to transform our categorical features into numerical values. In our case, we will be using a simple label encoder that turns the target labels to values between 0 and n_classes-1 (used for ordinal categorical features).
							</p>
							<div id="testimonials"><blockquote>
								from sklearn.preprocessing import LabelEncoder<br><br>

								encoder = LabelEncoder()<br>
								df['sex_encoded'] = encoder.fit_transform(df['sex'])<br>
								df['smoker_encoded'] = encoder.fit_transform(df['smoker'])<br>
								df['region_encoded'] = encoder.fit_transform(df['region'])<br>
								df['charges_transformed'] = stats.boxcox(df['charges'])[0]<br>
							</blockquote></div>
						</li>
						
						<li id="a3s8">
							<b>Splitting data</b><br>
							<p>
								We define X (features) and y (target) and remove duplicate features that will not be used in the model. We will first split the dataset into different datasets: (1) training set; and (2) test set. We retain 10% of the data for testing, and use a random state value of "0".
							</p>
							<div id="testimonials"><blockquote>
								from sklearn.model_selection import train_test_split<br><br>

								X = df.drop(['sex', 'smoker', 'region', 'charges', 'bmi_categories', 'charges_transformed'], axis=1)<br>
								y = df['charges_transformed']<br>
								X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
							</blockquote></div>
						</li>
						
						<li id="a3s9">
							<b>Instantiate and fit the linear model on our training set to predict medical charges</b><br>
							<p>
								First, we  instantiate a linear regression model
							</p>
							<div id="testimonials"><blockquote>
								from sklearn.linear_model import LinearRegression<br><br>

								linear_model = LinearRegression() 
							</blockquote></div>
							<p>
								Then we fit the model using the training data and we can print out the intercept and coefficients for the linear regression model.
							</p>
							<div id="testimonials"><blockquote>
								linear_model.fit(X_train, y_train) <br><br>

								print(linear_model.intercept_)<br>
								print(linear_model.coef_)
							</blockquote></div>
						</li>
						
						<li id="a3s10">
							<b>Prediction y_test</b><br>
							<p>
								For each record in the test set, make a prediction for the y value (transformed value of charges). The predicted values are stored in the y_pred array.
							</p>
							<div id="testimonials"><blockquote>
								y_pred = linear_model.predict(X_test)
							</blockquote></div>
						</li>
						
						<li id="a3s11">
							<b>Calculate MSE and MAE</b><br>
							<p>
								The metrics package in Python can derive the model evaluation metrics for mean squared error and mean absolute error along with the R^2 value.
							</p>
							<div id="testimonials"><blockquote>
								from sklearn import metrics<br><br>

								print("Mean squared error (MSE) =", metrics.mean_squared_error(y_test, y_pred))<br>
								print("Mean absolute error (MAE) =", metrics.mean_absolute_error(y_test, y_pred))<br>
								print("R^2 =", metrics.r2_score(y_test, y_pred))
							</blockquote></div>
							<p>
								The results we get are
							</p>
							<div id="testimonials"><blockquote>
								Mean squared error (MSE) = 0.3544682461419291<br>
								Mean absolute error (MAE) = 0.39858557168153946<br>
								R^2 = 0.8284407853175861
							</blockquote></div>
							<p>
								A R^2 value of approximately 83% indicates that our model can explain 83% of the variance in the data, so it performs quite well! This means we can use our features to predict the approximate amount of medical costs (based on insurance claims) the individual is likely to incur.
							</p>
						</li>
						
						<li id="a3s12">
							<b>Logistic model to predict smoker vs non-smoker</b><br>
							<p>
								Now we want to use a logistic model to predict smoker vs non-smokers (because it is a binary outcome). First we need to define the new df and split data - just like before.
							</p>
							<div id="testimonials"><blockquote>
								X = df[['age', 'bmi', 'children', 'sex_encoded', 'region_encoded', 'charges_transformed']]<br>
								y = df['smoker_encoded']<br>
								X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
							</blockquote></div>

							<p>
								Next, we instantiate a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" class="saymore">Logistic Regression</a> model and name it "logit_model".
							</p>
							<div id="testimonials"><blockquote>
								from sklearn.linear_model import LogisticRegression<br><br>

								logit_model = LogisticRegression()<br>
							</blockquote></div>

							<p>
								Fit the logistic regression model with the training data
							</p>
							<div id="testimonials"><blockquote>
								logit_model.fit(X_train, y_train) 
							</blockquote></div>

							<p>
								Next, we predict the label of smoker versus not for the test set.
							</p>
							<div id="testimonials"><blockquote>
								y_pred = logit_model.predict(X_test)
							</blockquote></div>

							<p>
								Now, we generate the Confusion Matrix for this logistic regression model to check how well the predictions performed. The confusion matrix is created this time using the sklearn metrics package.
							</p>
							<div id="testimonials"><blockquote>
								from sklearn import metrics<br><br>

								cm = metrics.confusion_matrix(y_test, y_pred, labels=logit_model.classes_)<br>
								disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logit_model.classes_)<br>
								disp.plot(cmap='Blues')<br>
								plt.show()
							</blockquote></div>
							<center><img src="images/pages/ml/a3-log-reg-acc-matrix.png" width="300" height=‚Äùauto‚Äù alt=""></center>

							<p>
								The confusion matrix does a good job of visually displaying how well the prediction did but we can see numerically by using the <i>metrics.classification_report()</i>.
							</p>
							<div id="testimonials"><blockquote>
								print(metrics.classification_report(y_test, y_pred, target_names=['non-smoker', 'smoker']));
							</blockquote></div>
							<center><img src="images/pages/ml/a3-log-reg-acc.png" width="300" height=‚Äùauto‚Äù alt=""></center>
						</li>

					</ol>
				</div>
			</li>
		
		<!-- Accordion: Assignment 4-->
			<li class="">
				<div class="title">
					<b>Feed Forward Neural Network</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/H4" class="saymore">Here</a> is the link to the GitHub code part of the assignment.
				 
						 <ul class="disc">
							<li>"AnjaWu_HW4_FeedForwardNN.ipynb": has a complete neural network (created by a TA and completed by myself), in which optimal hyperparameters are analyzed and selected.</li>
						</ul>

					</p>
					<h6>Highlights of Learning:</h6>
					<p>
						Steps
						<ol>
							<li><a href="#a4s1" class="saymore">Creating data set</a></li>
							<li><a href="#a4s2" class="saymore">Define the network architecture</a></li>
							<li><a href="#a4s3" class="saymore">Training the model</a></li>
							<li><a href="#a4s4" class="saymore">Changing hyperparameters: activation functions</a></li>
							<li><a href="#a4s5" class="saymore">Changing hyperparameters: learning rate</a></li>
							<li><a href="#a4s6" class="saymore">Changing hyperparameters: number of neurons in hidden layer</a></li>
							<li><a href="#a4s7" class="saymore">Changing dataset details: magnitude of noise</a></li>
							<li><a href="#a4s8" class="saymore">Changing dataset details: number of labels</a></li>
						</ol>
						<br>
					</p>
					<ol>
						<li id="a4s1">
							<b>Creating data set</b><br>
							<p>
								Specify the number of points and the number of branches (different colours of swirls).
							</p>
							<div id="testimonials"><blockquote>
								N = 50 # number of points per branch<br>
								K = 3  # number of branches
							</blockquote></div>
							
							<p>
								In order to count the number of points that will be contained in the training set we must multiply the number of points per branch with the number of branches.
							</p>
							<div id="testimonials"><blockquote>
								N_train = N*K 
							</blockquote></div>
							
							<p>
								Then we need to create the matrix for the data points and their labels.
							</p>
							<div id="testimonials"><blockquote>
								x_train = np.zeros((N_train,2)) # matrix containing the 2-dimensional datapoints<br>
								t_train = np.zeros(N_train, dtype='uint8') # labels (not in one-hot representation)
							</blockquote></div>
							
							<p>
								The <i>mag_noise</i> determines how close together and interwoven the various spiral colours are. The difference in theta will space apart the degrees between each spiral.
							</p>
							<div id="testimonials"><blockquote>
								mag_noise = 0.3  # controls how much noise gets added to the data<br>
								dTheta    = 4    # difference in theta in each branch
							</blockquote></div>
							
							<p>
								Now that we have set up the variables, we can do the data generation. First, for all the branches we will specify the indices to be used in the dataset by multiplying the number of points by the number of branches. Then we will make evenly spaced points using <a href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html" class="saymore">.linspace()</a> in a radius with numbers from 0.01 to 1. The theta determines the width (in terms of degree in radians) of the spiral colours. Then we concat (using <a href="https://numpy.org/doc/stable/reference/generated/numpy.c_.html" class="saymore">np.c_</a>) the cosine and sine of theta, which results as x,y coordinates giving us a special.The label of the spiral is given the number corresponding to the branch number.
							</p>
							<div id="testimonials"><blockquote>
								for j in range(K):
								<div class="tab">
								  ix = range(N*j,N*(j+1))<br>
								  r  = np.linspace(0.01,1,N) # radius<br>
								  th = np.linspace(j*(2*np.pi)/K,j*(2*np.pi)/K + dTheta,N) + np.random.randn(N)*mag_noise # theta<br>
								  x_train[ix] = np.c_[r*np.cos(th), r*np.sin(th)]<br>
								  t_train[ix] = j
								</div>
							</blockquote></div>

							<p>
								Now, we plot the spirals.
							</p>
							<div id="testimonials"><blockquote>
								fig = plt.figure(1, figsize=(5,5))<br>
								plt.scatter(x_train[:, 0], x_train[:, 1], c=t_train, s=40)<br>
								plt.xlim([-1,1])<br>
								plt.ylim([-1,1])<br>
								plt.xlabel(r'$x_1$')<br>
								plt.ylabel(r'$x_2$')<br>
								plt.show()
							</blockquote></div>
						</li>

						<li id="a4s2">
							<b>Define the network architecture</b><br>
							<p>
								We have to instantiate a Feed Forward Neural Network class. We define the initialization of the class by using <i>super()</i>. <i>super()</i> is calling to inherit a lot of functionality from a feed-forward NN that is pre-defined in torch.
							</p>
							<div id="testimonials"><blockquote>
								import torch<br><br>

								class FeedforwardNN(torch.nn.Module):
								<div class="tab">
								    def __init__(self, input_size, hidden_size, output_size):
								    <div class="tab">
			              	super(FeedforwardNN, self).__init__()
			             </div>
			           </div>
							</blockquote></div>

							<p>
								Then we have to specify the sizes which come from the input of the initialization.
							</p>
							<div id="testimonials"><blockquote>
								self.input_size = input_size<br>
				        self.hidden_size = hidden_size<br>
				        self.output_size = output_size
							</blockquote></div>
							
							<p>
								Next, we have to specify the flow of the layers and the type of transformation for the neural network.
							</p>
							<div id="testimonials"><blockquote>
								self.linear1 = torch.nn.Linear(self.input_size, self.hidden_size)<br>
        				self.linear2 = torch.nn.Linear(self.hidden_size, self.output_size)
							</blockquote></div>
							
							<p>
								Then we have to define the function by calling the activation function from the torch library.
							</p>
							<div id="testimonials"><blockquote>
								self.relu    = torch.nn.ReLU()<br>
				        self.sigmoid = torch.nn.Sigmoid()<br>
				        self.softmax = torch.nn.Softmax()
							</blockquote></div>
							
							<p>
								After that, we need to define how our neural network propagates. Since we want a feed-forward network, we make a definition of a forward() function. In this function we define which activation function should be used in the hidden and output layers.
							</p>
							<div id="testimonials"><blockquote>
								def forward(self, x):
								<div class="tab">
					        #Layer 1:<br>
					        linear1_out = self.linear1(x)<br>
					        h1 = self.softmax(linear1_out)<br><br>

					        #Layer 2:<br>
					        linear2_out = self.linear2(h1)<br>
					        h2 = self.softmax(linear2_out)<br><br>

					        #Network output:<br>
					        y = h2
					      </div>
							</blockquote></div>
							
							<p>
								Then to finish the propagation function we must return the final value after all the transformations.
							</p>
							<div id="testimonials"><blockquote class="tab">
								return y
							</blockquote></div>
							
							<p>
								Here we specify the size that we want for our model and create the model.
							</p>
							<div id="testimonials"><blockquote>
								input_size = 2 #2D data<br>
								hidden_size = 4<br>
								output_size = K <br>
								model = FeedforwardNN(input_size, hidden_size, output_size)
							</blockquote></div>
							
							<p>
								Next, we store the input data as a PyTorch tensor.
							</p>
							<div id="testimonials"><blockquote>
								x_train = torch.tensor(x_train, dtype = torch.float)
							</blockquote></div>
							
							<p>
								For best practice, it is a good idea to change the target values using one hot encoding.
							</p>
							<div id="testimonials"><blockquote>
								t_onehot = np.zeros((t_train.size, K))<br>
								t_onehot[np.arange(t_train.size),t_train] = 1<br>
								t_onehot = torch.tensor(t_onehot, dtype = torch.float)
							</blockquote></div>
							
							<p>
								We must use backpropagation to minimize the cost function - in this case we use a learning rate of 1 and <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" class="saymore">the stocastic gradient descent algorithm</a>.
							</p>
							<div id="testimonials"><blockquote>
								learning_rate = 1
								optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
							</blockquote></div>
							
							<p>
								We define the cost function - in this case <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" class="saymore">the cross entropy loss</a>.
							</p>
							<div id="testimonials"><blockquote>
								cost_func = torch.nn.CrossEntropyLoss()
							</blockquote></div>
							
							<p>
								Finally, we define the number of times to run gradient descent (epochs).
							</p>
							<div id="testimonials"><blockquote>
								N_epochs = 55000 
							</blockquote></div>
						</li>

						<li id="a4s3">
							<b>Training the model</b><br>
							<p>
								Create lists to store the epoch number, the cost, and the accuracy of every run.
							</p>
							<div id="testimonials"><blockquote>
								epoch_list    = []<br>
								cost_training = []<br>
								acc_training  = []
							</blockquote></div>
							
							<p>
								We then create a plot function that will update with each training run.
							</p>
							<div id="testimonials"><blockquote>
								def updatePlot():
							</blockquote></div>
							
							<p>
								First, we want to design the graph. We want to have the x-value minimum and maximum defined based on each set, with padding. Then we specify the grid spacing.
							</p>
							<div id="testimonials"><blockquote class="tab"> 
								padding = 0.1<br>
						    spacing = 0.02<br>
						    x1_min, x1_max = x_train[:, 0].min() - padding, x_train[:, 0].max() + padding<br>
						    x2_min, x2_max = x_train[:, 1].min() - padding, x_train[:, 1].max() + padding<br>
						    x1_grid, x2_grid = np.meshgrid(np.arange(x1_min, x1_max, spacing), np.arange(x2_min, x2_max, spacing))
							</blockquote></div>
							
							<p>
								We want to generate coordinates covering the whole plane. We do this by using <a href="https://numpy.org/doc/stable/reference/generated/numpy.ravel.html" class="saymore">.ravel()</a> that takes the x value grid and turns it into a 1D array., which is then stored as an x, y coordinate in a tensor format. Then this is inputted into our feed-forward nn model. From there, we take the the output given from the model and detach it from the tensor format to be used as an array and get the final index (using <a href="https://numpy.org/doc/stable/reference/generated/numpy.argmax.html" class="saymore">np.argmax()</a>). This let's us get the prediction of the last prediction in the array.
							</p>
							<div id="testimonials"><blockquote class="tab">
								torch_input = torch.tensor(np.c_[x1_grid.ravel(), x2_grid.ravel()], dtype = torch.float)<br>
						    NN_output = model(torch_input)<br>
						    predicted_class = np.argmax(NN_output.detach().numpy(), axis=1)
							</blockquote></div>
							
							<p>
								Now we can get to the plotting. We want to plot 3 different things:the classifier (showing the boundaries based on the predictions), the cost function with each run, and the training accuracy for each run.
							</p>
							<p>
								First, for the classifier: we must create a <a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.subplot.html" class="saymore"> subplot</a> (number of rows, number of columns, position). Then we use <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html" class="saymore">.contoured()</a> to draw the boundaries, using the grid and the predicted values). We specify a blend (transparency) level for the colours (<i>alpha =.8</i>). 
							</p>
							<div id="testimonials"><blockquote class="tab">
						    plt.subplot(121)<br>
						    plt.contourf(x1_grid, x2_grid, predicted_class.reshape(x1_grid.shape), K, alpha=0.8)<br>
						    plt.scatter(x_train[:, 0], x_train[:, 1], c=t_train, s=40)<br>
						    plt.xlim(x1_grid.min(), x1_grid.max())<br>
						    plt.ylim(x2_grid.min(), x2_grid.max())<br>
						    plt.xlabel(r'$x_1$')<br>
						    plt.ylabel(r'$x_2$')
							</blockquote></div>
							
							<p>
								Next, we plot the cost function during training. This is done by calling upon the <i>epoch_list</i> we created along with the <i>cost_training</i> list.
							</p>
							<div id="testimonials"><blockquote class="tab">
								plt.subplot(222)<br>
						    plt.plot(epoch_list,cost_training,'o-')<br>
						    plt.xlabel('Epoch')<br>
						    plt.ylabel('Training cost')
							</blockquote></div>
							
							<p>
								Finally, we plot the accuracy function during training. This is done by calling upon the <i>epoch_list</i> we created along with the <i>acc_training</i> list.
							</p>
							<div id="testimonials"><blockquote class="tab">
								plt.subplot(224)<br>
						    plt.plot(epoch_list,acc_training,'o-')<br>
						    plt.xlabel('Epoch')<br>
						    plt.ylabel('Training accuracy')
							</blockquote></div>
							
							<p>
								Now to get the value from our three lists above (epoch_list, cost_training and acc_training), we create a for loop. We will loop through every epoch from 0 to our pre-specified max epochs.
							</p>
							<div id="testimonials"><blockquote>
								for epoch in range(N_epochs):
							</blockquote></div>
							
							<p>
								First, we need to set the gradients to zero because PyTorch accumulates the gradients, we do this by using <a href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html" class="saymore">optimizer.zero_grad()</a>. We then input our model and the outcome for our cost_func and then using <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html" class="saymore">.backward()</a> we compute the gradient. Then after this has been completed we must update the parameters, using <a href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html" class="saymore">optimizer.step()</a>.
							</p>
							<div id="testimonials"><blockquote class="tab">
								optimizer.zero_grad() <br>
						    NN_output = model(x_train) <br>
						    cost = cost_func(NN_output, t_onehot)<br>
						    cost.backward() <br>
						    optimizer.step()
							</blockquote></div>
							
							<p>
								Since it would overload the computer we will set the plot to update and print results every 500 epochs. Just like before we define the predicted_class by using the max of the model output array. The accuracy we calculate by taking the mean of the predictions. Finally we update our 3 lists to contain the values for every 500 epochs.
							</p>
							<div id="testimonials"><blockquote>
								if epoch % 500 == 0:
								<div class="tab">
					        predicted_class = np.argmax(NN_output.detach().numpy(), axis=1)<br>
					        accuracy = np.mean(predicted_class == t_train)<br><br>
					        
					        epoch_list.append(epoch)<br>
					        cost_training.append(cost.detach().numpy())<br>
					        acc_training.append(accuracy)
					      </div>
							</blockquote></div>
							
							<p>
								Then we update the plot of the resulting classifier.
							</p>
							<div id="testimonials"><blockquote class="tab">
								fig = plt.figure(2,figsize=(10,5))<br>
				        fig.subplots_adjust(hspace=.3,wspace=.3)<br>
				        plt.clf() # <a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.clf.html" class="saymore">clears the figure</a>  <br>
				        updatePlot()<br>
				        display.display(plt.gcf()) # <a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.gcf.html" class="saymore">displays the current figure</a> <br>
				        print("Iteration %d:\n  Training cost %f\n  Training accuracy %f\n" % (epoch, cost, accuracy) )<br>
				        display.clear_output(wait=True)
							</blockquote></div>
							
							<p>
								I created lists to record: max accuracy, max accuracy epoch value, max accuracy cost value. These lists were written here to collect data. This was used to figure out the best parameters.
							</p>
							<div id="testimonials"><blockquote>
								accuracy_max = max(acc_training)<br>
								max_index = acc_training.index(max(acc_training))<br>
								epoch_max = epoch_list[max_index]<br>
								cost_max = cost_training[max_index]
							</blockquote></div>

							<p>
								The same was done for the purpose of getting more information for each parameter changed in my part of the assignment. Here is a sample:
							</p>
							<div id="testimonials"><blockquote>
								alpha6_final_accr.append(accuracy)<br>
								alpha6_max_accr.append(accuracy_max)<br>
								alpha6_max_accr_epoch.append(epoch_max)<br>
								alpha6_final_cost.append(cost)<br>
								alpha6_max_accr_cost.append(cost_max)
							</blockquote></div>

							<p>
								Here is what was displayed:
							</p>
							<center><img src="images/pages/ml/a4-plot.png" width="400" height=‚Äùauto‚Äù alt=""></center>
						</li>

						<li id="a4s4">
							<b>Changing hyperparameters: activation functions</b><br>
							<p>
								I did every permutation for all three activation functions, Softmax, Sigmoid, ReLU, for both layers. However, that was done more for curiosity's sake. The softmax is used in the final output layer when there is a multi-class problem, such as this one. Sigmoid is used best when looking at probability and ReLU should only be used in the output layer if there are two outcomes.
							</p>
							<p>
								I found some interesting results:
							</p>
							<center><img src="images/pages/ml/a4-activn-fn.png" width="800" height=‚Äùauto‚Äù alt=""></center>
							<p>
								Analyzing data for max of softmax, sigmoid, relu (second function being softmax):<br>
								The maximum occurs at: sigmoid_max_accr <br>
								with a maximum accuracy of: 0.8593333333333334
							</p>
							<p>
								The same was done with the second function being sigmoid<br>
								The maximum occurs at: sigmoid_max_accr <br>
								with a maximum accuracy of: 0.716
							</p>
							<p>
								The same was done with the second function being relu<br>
								The maximum occurs at: relu_max_accr <br>
								with a maximum accuracy of: 0.6839999999999999
							</p>
							<p>
								In conclusion, on average it can clearly be seen that the activation combination that works best is:
								<ul class="disc">
									<li>
										First layer activation function: sigmoid
									</li>
									<li>
										Second layer activation function: softmax
									</li>
								</ul>
								with accuracy of 0.859 on average.
							</p>
							<p>
								As can be seen above, it also has the least cost for the maximum accuracy (average of 0.724206). However, it also did have the most amount of "optimization runs" needed (epoch = 8000).
							</p>
						</li>

						<li id="a4s5">
							<b>Changing hyperparameters: learning rate</b><br>
							<p>
								In changing the learning rate, there were several values tried: 0.1,0.5, 1, 2, 3, 4. As a summary, we see:
							</p>
							<center><img src="images/pages/ml/a4-learningrate.png" width="400" height=‚Äùauto‚Äù alt=""></center>
							<p>
								The maximum occurs at: 4_max_accr with a maximum accuracy of: 0.828
							</p>
							<center><img src="images/pages/ml/a4-lr4.png" width="400" height=‚Äùauto‚Äù alt=""></center>
							<p>
								In conclusion, as can be seen, the learning rate of 4 (0.828000) had the best accuracy, followed by 1. 4 also had the lowest cost (0.735654) and was in the bottom 3 for epoch runs (23050).
							</p>

						</li>

						<li id="a4s6">
							<b>Changing hyperparameters: number of neurons in hidden layer </b><br>
							<p>
								Number of neurons in the hidden layers was changed to include:  2, 4, 6, 10, 14, 15, 16, 17, 20. Here are the results for the accuracy based on the number of neurons in the hidden layer.
							</p>
							<center><img src="images/pages/ml/a4-neurons.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>
							<p>
								A scatterplot of number of neurons vs accuracy was created for all 10 trials to analyze the variation and accuracy to help determine the best.
							</p>
							<center><img src="images/pages/ml/a4-neuron-acc-10.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>
							<p>
								This graph was then condensed to contain the mean for all 10 trials along with error bars. In order to make CI indicators, to show how variant the avgs are from the other points, <a href="https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.errorbar.html" class="saymore">plt.errorbar()</a> was used with the <i>yerr</i> being the 'ci' that I calculated per neuron:

							</p>
							<div id="testimonials"><blockquote>
								plt.errorbar(x_nodes_avgs, avg_df.loc['mean'], yerr=avg_df.loc['ci'], color='red', linestyle='')<br><br>

								plt.show
							</blockquote></div>
							<center><img src="images/pages/ml/a4-neuron-acc-10-avg.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>
							<p>
								As can be seen, 16 neurons in the hidden layer did the best, on average. Close behind was 17 and then 10. 16 also had less variation than the other Q values. It is statistically better (less noise) than the surrounding number of neurons.
							</p>
						</li>

						<li id="a4s7">
							<b>Changing dataset details: magnitude of noise</b><br>
							<p>
								The magnitude of noise in the data that was analyzed was: 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0. it is expected the more noise you have the harder classification will be. But I was curious as to what extent the learning accuracy decreased. For 10 trials run for each magnitude here is the accuracy:
							</p>
							<center><img src="images/pages/ml/a4-mag.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>

							<p>
								Like before, a scatterplot of number of neurons vs accuracy was created for all 10 trials to analyze the variation and accuracy to help determine the best.
							</p>
							<center><img src="images/pages/ml/a4-mag-acc-10.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>


							<p>
								Then same as before, the above graph was condensed to include error bars to show variance.
							</p>
							<center><img src="images/pages/ml/a4-mag-acc-10-avg.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>

							<p>
								As stated before, obviously the algorithms did better with less noise in the data. There was a large dip after 0.6 magnitude and it is interesting to note that with 0.9 magnitude the algorithms did better than surrounding (and it also had the least variability in the data). I'm not quite sure why that is, but it was an interesting fact from the data collected.
							</p>
						</li>

						<li id="a4s8">
							<b>Changing dataset details: number of labels</b><br>
							<p>
								The number of labels in the dataset that was chosen to analyze was 2, 3, 4, 5, 6, 7. Like before, the more labels you have the harder it will be for the model to predict accurately, so I wanted to explore how the accuracy changed between the number of labels.
							</p>
							<center><img src="images/pages/ml/a4-labels.png" width="400" height=‚Äùauto‚Äù alt=""></center><br>

							<p>
								Plotting the scatterplot of number of labels vs accuracy for 10 trails per level:
							</p>
							<center><img src="images/pages/ml/a4-labels-acc-10.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>

							<p>
								Then same as before, the above graph was condensed to include error bars to show variance.
							</p>
							<center><img src="images/pages/ml/a4-labels-acc-10-avg.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>

							<p>
								As expected, the training accuracy severely decreases as the number of labels increases, but the pattern was linear. The accuracy could probably be improved by increasing the number of hidden layer neurons, and increasing epochs even more. I wanted to keep the majority of variables the same throughout my trials in order to be able to compare the change in labels (having to adjust epochs and learning rate to get optimal results for the rest of the variables being held the same).
							</p>
						</li>


					</ol>
				</div>
			</li>


		<!-- Accordion: Assignment 5-->
			<li class="">
				<div class="title">
					<b>HR Attrition Analysis (Logistic Regression and Decision Tree)</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://github.com/anjawu/machine-learning/tree/main/HR-attrition-new" class="saymore">Here</a> is the link to the GitHub code part of the assignment.
					</p>
					<p>
						<a href="images/HR_Attrition_Analysis.pdf" class="saymore">Here</a> is the final written report!
					</p>
					<h6>Summary of Assignment:</h6>
					<p>
						<b>My insights:</b> <br>
						From the logistic model, the random oversampling method performed the best in terms of predictions (accuracy range of 0.59-0.65). When looking at the features, there were certain ones that cannot be controlled by the company such as: age, years at company, number of companies worked, and marital status. However, there were features that could provide insights in which the company could act on to help keep more employees. 
						<ul class="disc">
							<li>In terms of performance and salary: people with a higher performance are more likely to leave but people with a higher percent salary hike are less likely to leave. Meaning the company should reward those high impact individuals who they would like to keep. </li>
							<li>Along this thread, it was found that the longer it has been since a person has been promoted, the more likely they are to leave. Thus it would be important to dig in individual cases whether the company is hiring externally when they should have promoted someone from within. </li>
							<li>The more training an individual had the more likely it was that they would stay. This means the company should encourage professional development.</li>
							<li>Sales executives and research directors tend to be more likely to leave, so it would be worthwhile to flag it to their leadership and look into the why for these groups. </li>
							<li>Individuals who travel more, tend to leave more. Given the lack of information on the company, it would be hard to come up with a reasonable action for the company to take in this regard. </li>
							<li>The last main actionable insight was: the worse an individual's work-life balance was, the more likely they were to leave. The company should ensure they are encouraging days off when needed, in order to retain their employees.</li>
						</ul>
					</p>
					<p>
						The decision tree performed really well at predicting (accuracy range of 0.89-0.95) regardless of the imbalanced sampling method chosen. This can be used to predict which employees might leave within the next year and try to keep them on.
					</p>
					<p>
						<b>My process:</b><br>
						Through the data processing stage there were several things done to ensure data was ready for machine learning algorithms. Initially, I had to encode the categorical features, both ordinal and nominal features. For the ordinal features, I chose a simple encoding of 0 to n-1 (for n features). For the nominal categories, I chose one hot encoding and dropped a specific column as the reference to prevent collinearity. After the feature encoding, I looked at the collinearity of the data and found a potential for multi-collinearity between some features. For this I ran the VIF (variance inflation factor) method and found that there were no values high enough to consider the variables collinear. After moving forward with the data, I noticed an imbalance in the attrition outcome. So with this I tried several methods to deal with it: random oversampling, random undersampling, SMOTE, and Tomek links for the two algorithms: logistic regression and decision tree. Logistic regression was used to see which feature(s) had the biggest effects on attrition and the decision tree was used to best predict attrition (due to it performing the best on imbalanced data). For the logistic regression, I used statsmodels in order to calculate the p-value for the coefficients to determine which were considered the most statistically significant effect on attrition. At the end of the models, I wrote a report comparing the methods and algorithms used plus the insights gained.
					</p>
					<p>
						I only had about a week (part-time) to work on this and naturally that would mean there are improvements to be done. For instance, I would interpret the logistic regression coefficients in a more meaningful way by transforming the coefficients (e^coefficient) to determine how much more likely a person is to leave for each specific feature.
					</p>
				</div>
			</li>
		</ul>


		<div class="hr">
		</div>

	</div>
</div>

<div class="row">
	<div class="twelve columns">

		<!-- Lecture Material -->
		<div class="circledate">
			<div class="textcircle">
				<br>Class
			</div>
		</div>
		<h4>Lecture Material</h4>
		<div class="dots blogdots">
		</div>
		<p>
			In the tabs below you will find a summary of the biggest take aways from each of the lectures. There was a lot of good material covered in the short time Professor Alvarez had but I just wanted to document the summary of the content.
		</p>
		<div class="hr">
		</div>

		<!-- Accordion: 1 Intro-->
		<ul class="accordion">
			<li class="">
				<div class="title">
					<b>Intro to ML</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						In machine learning there are different ways for a machine to learn:
						<ul class="disc">
							<li>
								<b>Supervised learning:</b> you know correct input and output and you want program to learn mapping x -> y
							</li>
							<li>
								<b>Unsupervised learning:</b> given data find patterns/structure in data
							</li>
							<li>
								<b>Online learning:</b> supervised learning but when data is given in sequence
							</li>
							<li>
								<b>Reinforcement learning:</b> learning by trial and error in a way but with negative and positive reinforcements (e.g. in a game like tetris: you get scored based on your moves as a computer - the higher score, the bigger score you get - program the computer to understand the reinforcement)
							</li>
							<li>
								<b>Active learning:</b> supervised learning without labels, e.g. when you give an image and ask the oracle to label the picture
							</li>
						</ul>
					</p>
					Steps:
					<ol>
						
						<li>
							ML workflow:
							<div class="tab">
							Should I use ML on this problem?<br>
							Is there a pattern to detect<br>
							Can I solve it analytically?<br>
							Do I have data?
							</div>
						</li>
						<li>
							Gather and organize data
						</li>
						<li>
							Preprocessing, cleaning, visualizing
						</li>
						<li>
							Establishing a baseline
						</li>
						<li>
							Choosing a model, loss, regularization
						</li>
						<li>
							Optimization
						</li>
						<li>
							Hyperparameter search
						</li>
						<li>
							Analyze performance and mistakes and iterate back to step 5 (or 3)
						</li>
					</ol><br>
					<p>
						Important fact is to try simple models before moving to complex. <i>Example: try logistic regression before building a deep neural net because it is cheaper computationally and easier to interpret.</i>
					</p>
				</div>
			</li>

		<!-- Accordion: 2 KNN -->
			<li class="">
				<div class="title">
					<b>k-NN</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>k-Nearest Neighbours</h6>
					<p>
						Nearest neighbours is a machine learning method in which a label is given to an unknown based on the feature mappings to the physically closest features of known labels. The mathematical algorithm looks like:
					</p>
					<center><img src="images/pages/ml/l2-nn-formula.png" width="400" height=‚Äùauto‚Äù alt=""></center><br>
					<p>
						There are <a href="https://www.analyticsvidhya.com/blog/2021/08/how-knn-uses-distance-measures/" class="saymore">several different distance formulas</a> that can be used. In the course, we discussed Euclidean. Euclidean is the same as Minkowski distance for the case where p=2. The Minkowski algorithm formula is:
					</p>
					<center><img src="images/pages/ml/l2-minkowski.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
					<p>
						In the case of p=2, we get the Euclidean distance:
					</p>
					<center><img src="images/pages/ml/l2-euclidean.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					<p>
						Decision boundaries are used to classify points based on where they are located (according to the features). We can visualize (if in 2D) the decision boundaries using a Voronoi diagram. These decision boundaries are defined by the nearest neighbours algorithms. The k-nearest neighbours algorithm looks at k-amount of neighbours around a specific point and classifies that point based on the majority label of the k-neighbours:
					</p>
					<center><img src="images/pages/ml/l2-knn.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					<p>
						The value of k is determined by the user. This is a hyperparameter which will affect the decision boundaries of the classification. It is important to find the ideal k value as:
					</p>
					<ul class="disc">
						<li>
							Too small of a k: 
							<ul class="disc">
								<li>
									will give you a good reading of the fine-grained patterns
								</li>
								<li>
									BUT can overfit the data: meaning it becomes too sensitive to fine-grained patterns that are only specific to the training model. This will lead to bad validation/test set accuracy.
								</li>
							</ul>
						</li>
						<li>
							Too big of a k:
							<ul class="disc">
								<li>
									Will be less likely to be impacted by singularities 
								</li>
								<li>
									BUT can underfit the data: meaning it doesn‚Äôt pick up on smaller regularities. This will lead to not only bad validation/test set accuracy but also training set accuracy.
								</li>
							</ul>
						</li>
					</ul>

					<h6>Curse of Dimensionality</h6>
					<p>
						This occurs with all high dimension data. In particular, for kNN most points are considered far apart and approximately the same distance. This can be seen in a proof where the mean of the distances is the dimension, meaning in high dimension the points are further apart and the standard deviation (sqrt(d)) becomes small relative to mean (d), meaning that the distances do not have much variation - implying approximately the same distance.
					</p>
					<p>
						We can use dimension reduction techniques (more on this later) to reduce dimensions and allow our model to perform better. 
					</p>

					<h6>Normalization</h6>
					<p>
						We have two types of regularization: L1 and L2. We will discuss this more in detail later. In a nutshell, for kNN they can be sensitive to large ranges of feature values. In order to combat this you can normalize each dimension to be zero.
					</p>
					<h6>Why kNN?</h6>
					<ul class="disc">
						<li>
							Fewer hyperparameters
						</li>
						<li>
							Able to handle attributes that interact in complex ways
						</li>
					</ul>
				</div>
			</li>

		<!-- Accordion: 3 Decision Trees -->
			<li class="">
				<div class="title">
					<b>Decision Trees</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6>Decision Tree</h6>
					<p>
						A decision tree makes predictions by splitting on features according to an algorithm to get the best split. 
					</p>
					<p>
						The structure of a decision tree:
						<center><img src="images/pages/ml/l3-dt-structure.png" width="500" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						There are two types of decision trees aka CART (Classification and Regression Tree):
						<ol>
							<li>
								Classification Tree
								<div class="tab">
									<ul class="disc">
										<li>
											Output is discrete
										</li>
										<li>
											Leaf value is the most common value within the leaf node set
										</li>
									</ul>
								</div>
							</li>
							<li>
								Regression Tree
								<div class="tab">
									<ul class="disc">
										<li>
											Output is continuous
										</li>
										<li>
											Leaf value is the most mean value within the leaf node set
										</li>
									</ul>
								</div>
							</li>
						</ol>
					</p>
					<h6>Learning</h6>
					<p>						
						The decision tree splits according to a greedy heuristic. This means that at each split the algorithm splits according to the optimal choice for that specific node. The optimal choice is determined by the greatest reduction for the loss function. This is done recursively for each node until the leaf node is reached.
					</p>
					<p>
						What algorithm do we use for our loss function?<br>
						Information Gain or Gini Impurity
					</p>
					<h6>Entropy/Information Gain</h6>
					<p>
						Entropy of discrete random variables is a number that quantifies the uncertainty of the outcomes. This means that a high entropy will not be able to be predicted well because the split of the data is closer to even (would look more like a uniform distribution). Whereas, a low entropy will give us more of a defined split in which we would have a greater chance of predicting where our data falls.
					</p>
					<p>
						Entropy is calculated as:
						<center><img src="images/pages/ml/l3-entropy-calc.png" width="400" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Entropy of joint distributions:
						<center><img src="images/pages/ml/l3-joint-entropy.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Specific conditional entropy:
						<center><img src="images/pages/ml/l3-specific-conditional-entropy.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Conditional entropy:
						<center><img src="images/pages/ml/l3-conditional-entropy.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Information Gain is what determines the split. The higher the information gain the better. Information gain is (1 - entropy) or for conditional cases:
						<center><img src="images/pages/ml/l3-IG-conditional.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Information gain <a href="https://analyticsindiamag.com/understanding-the-maths-behind-the-gini-impurity-method-for-decision-tree-split/" class="saymore">favours smaller partitions (distributions) with a variety of diverse values.</a>
					</p>
					<h6>Gini Impurity</h6>
					<p>
						<a href="https://medium.com/poli-data/deep-dive-into-the-basics-of-gini-impurity-in-decision-trees-with-math-intuition-46c721d4aaec" class="saymore">Gini impurity</a> calculates probability of diversity of the data. Gini index of 0 implies that the split is pure, this means that all the data falls in one division.
					</p>
					<p>
						Mathematically calculated:
						<center><img src="images/pages/ml/l3-gini-impurity.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Gini Impurity <a href="https://analyticsindiamag.com/understanding-the-maths-behind-the-gini-impurity-method-for-decision-tree-split/" class="saymore">favours bigger partitions (distributions).</a>
					</p>
					<h6><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="saymore">Hyperparameters</a></h6>
					<p>
						There are many hyperparameters that can be changed to improve accuracy and to prevent overfitting and underfitting. Here are some:
						<ul class="disc">
							<li>
								Max depth of tree: the lower this number, the more you risk underfitting the model.
							</li>
							<li>
								Minimum number of samples required to split an internal node: the lower this number, the more you risk overfitting to the training.
							</li>
							<li>
								Minimum number of samples required to be at a leaf node: the lower this number, the more you risk overfitting to the training.
							</li>
							<li>
								Purity at nodes: you can specify the lowest level of impurity that you want to split by. The lower the impurity level, the more you risk overfitting.
							</li>
						</ul>
					</p>
					<h6>Why use decision trees?</h6>
					<ul class="disc">
						<li>
							More interpretable 
						</li>
						<li>
							Works well with lots of attributes (but only a few important ones)
						</li>
						<li>
							Works well with imbalanced data
						</li>
						<li>
							Fairly fast 
						</li>
					</ul>
				</div>
			</li>

		<!-- Accordion: 4 Bias-Variance Decomposition -->
			<li class="">
				<div class="title">
					<b>Bias-Variance Decomposition</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						We can quantify the effects of underfitting and overfitting models in terms of bias/variance decomposition. We have the formula for calculating expected loss (using squared error), given by:
						<center><img src="images/pages/ml/l4-expected-loss.png" width="600" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						As can be seen:
						<ul class="disc">
							<li>Bias: how wrong the expected prediction is (underfitting)</li>
							<li>Variance: the amount of variability in the predictions (overfitting)</li>
							<li>Bayes error: the inherent unpredictability of the targets</li>
						</ul>
						<i>Note: Even though this analysis only applies to squared error, we often loosely use ‚Äúbias‚Äù and ‚Äúvariance‚Äù as synonyms for ‚Äúunderfitting‚Äù and ‚Äúoverfitting‚Äù.</i>
					</p>
					<p>
						Low Bayes error makes predicting easier. Using a Venn diagram as an example, if the Bayes error is low the circles have little to no intersection:
						<center><img src="images/pages/ml/l4-low-bayes-err.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						If Bayes error is high, it is a lot harder to predict:
						<center><img src="images/pages/ml/l4-high-bayes-err.png" width="150" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Really awesome visual for understanding effects of bias and variance:
						<center><img src="images/pages/ml/l4-bias-variance-accuracy.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						Bayes optimal is defined as the best any learning algorithm can do. In order to calculate Bayes optimal, you need to have access to the true population distribution. This very rarely occurs in real life problems.
					</p>
				</div>
			</li> 

		<!-- Accordion: 5 Ensembling Learning  -->
			<li class="">
				<div class="title">
					<b>Ensembling Learning</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						<a href="https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/" class="saymore">Ensemble learning</a> is a general meta approach to machine learning that seeks better predictive performance by combining the predictions from multiple models.
					</p>
					<p>
						The three main classes of ensemble learning methods are bagging, stacking, and boosting.
						<ul class="disc">
							<li>
								<i>Bagging (aka Bootstrap aggregating):</i> is an ensemble learning method that seeks a diverse group of ensemble members by varying the training data. The final model is made by combining ensemble members using simple statistics, such as voting or averaging. It involves fitting many decision trees on different samples of the same dataset and averaging the predictions. An example would be Random Forest (large number of individual decision trees split randomly that operate as an ensemble)!
							</li> 
							<li>
								<i>Stacking (Stacked Generalization):</i> is an ensemble method that involves fitting many different models types on the same data and using another model to learn how to best combine the predictions.
							</li>
							<li>
								<i>Boosting:</i> is an ensemble method that seeks to change the training data to focus attention on examples that previous fit models on the training dataset have gotten wrong. It involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions.
							</li>
						</ul>
					</p>
					<h6>Bagging</h6>
					<p>
						We cannot change Bayes error, as we have no control over it. The expected value of the average prediction is the same expected value of the individual, so the bias does not change either. We can see mathematically that bagging reduces the variance (overfitting of a model) when we average over independent samples:
						<center><img src="images/pages/ml/l5-variance.png" width="400" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						The variance is 1/m smaller as we have taken the average. There is a proof that I did as an exercise that showed that bagging also reduces the variable even when the samples are not independent. It just does not reduce it as greatly as when they are independent.
					</p>
				</div>
			</li> 

		<!-- Accordion: 6 Regression Models-->
			<li class="">
				<div class="title">
					<b>Regression Models</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Ideally, we want to have the following flow for our algorithms:
						<ul class="disc">
							<li>Choose a model</li>
							<li>Define a loss function, this is used to quantify how bad the fit to the data is.</li>
							<li>Pick a regularizer, <a href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning" class="saymore"> this minimizes the adjusted loss function and prevents overfitting or underfitting.</a></li>
							<li>After those, you can use an optimization algorithm, if need be. An optimization algorithm is a procedure that finds the parameters that result in an optimum.</li>
						</ul>
						We want to vectorize our data because it will increase the speed of computation due to linear algebra calculations on matrices.
					</p>
					<h6>Loss functions</h6>
					<p>
						There are <a href="https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e" class="saymore"> different loss functions depending on the data you are working with. The ones that we looked at are:
						<ul class="disc">
							<li>
								Squared error loss function
								<ul class="disc">
									<li>
										<a href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-error" class="saymore"> MSE (mean squared error)</a>: use when doing a regression, that is normally distributed, and where you want to penalize bigger mistakes more than smaller ones. 
									</li>
									<li>
										MAE (mean absolute error)
									</li>
								</ul>
							</li>
							<li>
								Cost function: loss function averaged over all training examples. For a linear squared error loss function we would get:
								<center><img src="images/pages/ml/l6-cost-fn.png" width="250" height=‚Äùauto‚Äù alt=""></center><br>
							</li>
							<li>
								Cross-entropy loss: this is a loss function used for logistic regression. It heavily penalizes the wrong classification. The equation and graph look like:
							</li>
						</ul>
					</p>
					<h6>Optimization Algorithm</h6>
					<p>
						There are also different optimization algorithms that can be used:
						<ul class="disc">
							<li>
								Direct calculus solution: this does not occur often, and even when it does it can be so computationally expensive that one would opt for the iterative solution.
							</li>
							<li>
								Gradient descent (iterative solution) is this the main optimization algorithm:
								<ul>
									<li>
										The main concept of gradient descent is taking partial derivatives of the cost function and moving towards the optimum value (derivative of 0). This is done in an iterative fashion moving in the direction of steepest descent.
									</li>
									<li>
										We can adjust a learning rate to determine the size of the ‚Äústeps‚Äù we take for each iteration of the gradient descent algorithm.
									</li>
									<li>
										There will be more on this when we go over Neural Networks
									</li>
								</ul>
							</li>
						</ul>
					</p>
					<h6>Regression</h6>
					<p>
						Regression is very interpretable. In order to accurately interpret the regression you must look at both the p-values of all the coefficients and the coefficients themselves. P-values are often between 0.05 and 0.01. Each coefficient is given a p-value, which can tell you if your result is statistically significant (in this case meaning that the feature DOES have a statistically significant impact on your outcome). The sign of the coefficient tells you whether the relationship between the feature and the outcome are positive (increasing) or negative (decreasing). The value of the coefficient tells you the magnitude of the effect of the feature on the outcome.
					</p>
					<p>
						It is important that you understand that setting up your model and picking a p-value is very important. <a href="https://info.umkc.edu/drbanderson/p-hacking-and-the-problem-of-multiple-comparisons/" class="saymore">Here is an article</a> that speaks well to three big errors: multiple comparisons, p-hacking, and hypothesizing after the results are known (HARKing). 
					</p>
					<h6>Linear Regression</h6>
					<p>
						For a linear regression, a linear functions created from the features is used to make a prediction, y, of our target value, t, using weights, w, and bias, b:
					</p>
					<center><img src="images/pages/ml/l6-lin-reg.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
					<p>
						This can be done with D-features. The larger D is, the more complex the model will be - so dimension reduction techniques should be used if possible. One possible way of doing linear regression on a relationship that is not linear is by feature mapping. 
					</p>
					<p>
						Polynomial feature mapping occurs when the data is fit with a degree-M poly nominal function:
						<center><img src="images/pages/ml/l6-poly-eqn.png" width="300" height=‚Äùauto‚Äù alt=""></center><br>
						which will then give us the feature mapping of:
						<center><img src="images/pages/ml/l6-poly-feature-map.png" width="200" height=‚Äùauto‚Äù alt=""></center><br>
						which is linear wrt each weight.
					</p>
					<p>
						It is important to keep in mind that too small of an M will result in underfitting but too large of an M will result in overfitting.
					</p>
					<h6>Logistic Regression</h6>
					<p>
						Logistic regression is used when we have a binary classification problem. In this case, we do not have values outside of [0,1]. This is why the logistic function is a kind of sigmoid:
						<center><img src="images/pages/ml/l6-log-fn.png" width="400" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						The coefficients of a logistic regression are highly interpretable because you can transform the values to be the percent increase/decrease towards your outcome. This is done by putting the coefficient as an exponent of e. Here are two articles that discuss how to interpret the coefficient: <a href="https://towardsdatascience.com/a-simple-interpretation-of-logistic-regression-coefficients-e3a40a62e8cf" class="saymore">1</a> and <a href="https://quantifyinghealth.com/interpret-logistic-regression-coefficients/" class="saymore">2</a>.
					</p>
					<p>
						<a href="https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148" class="saymore">More details on logistic regression.</a>
					</p>
				</div>
			</li> 

		<!-- Accordion: 7/8 Introduction to Neural Networks -->
			<li class="">
				<div class="title">
					<b>Introduction to Neural Networks</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						A neural network is a simplistic version modelled after the human brain. The format is:
						<ul class="disc">
							<li>Layers (input, hidden, and output)</li>
							<li>Nodes (or neurons)</li>
							<li>
								Connections between neurons (how connections are made depend on the type of neural network - e.g. in a feed-forward NN each neuron from one layer is connected to the neurons directly in the layer proceeding it)
								<p>
									Each connection is modelled by:
									<center><img src="images/pages/ml/l7-connection-formula.png" width="150" height=‚Äùauto‚Äù alt=""></center><br>
									Where the activation function can change from layer to layer (common ones being: ReLU, softmax, hard-threshold, logistic, sigmoid and many more)
								</p>
								<p>
									Activation functions are currently being researched to find the best ones to increase predictions. ReLU has been a top performing one as it is non-linear "enough", but there are issues that arise (such as dying ReLU problem where a neuron is unable to learn). So there have been modifications made and new versions of ReLU exist. 
								</p>
							</li>
					</p>
					<p>
						A multi-layer perceptron is a multilayer network that has fully connect layers (meaning all input nodes are connected to output nodes for each layer).
					</p>
					<p>
						Example of a feed-forward NN:
						<center><img src="images/pages/ml/l7-nn-diagram.png" width="550" height=‚Äùauto‚Äù alt=""></center><br>
					</p>
					<p>
						<a href="https://www.youtube.com/watch?v=qO_NLVjD6zE" class="saymore">Here</a> is a video that discusses some issues with neural networks and how to deal with it (vanishing and exploding gradient).
					</p>
					<h6>Optimization</h6>
					<p>
						You can optimize by brute force or by using variations of gradient descent. Optimization occurs when parameters are updated to give the greatest learning or lowest error rate.
					</p>
					<p>
						In order to train the neural network, you must go through runs (epochs) of the neural network where each iteration the weights (and bias) are updated by learning - backpropagation (e.g. using gradient descent). Backpropagation is done automatically using libraries such as PyTorch or TensorFlow.
					</p>
					<h6>Gradient Descent</h6>
					<p>
						Gradient descent is the method of taking the partial derivative of the loss function wrt the weight and taking the average to move towards the direction of steepest descent to achieve a global extrema.
					</p>
					<p>
						Stochastic gradient descent is when you randomly uniformly sample the training set and do batch gradient update on each set. We are able to do this because when randomly sampled we get an unbiased estimate of the batched gradient. Also on average the SGD will move downwards, it will just tend to follow a more noisy trend compared to batch gradient descent.
					</p>
					<p>
						We can adjust the learning rate, which dictates how large of a step we take for each learning iteration (epoch).
					</p>
					<p>
						Here are two videos that describe gradient descent well: <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w" class="saymore">3Blue1Brown</a> and <a href="https://www.youtube.com/watch?v=qg4PchTECck" class="saymore">VisuallyExplained</a>.
					</p>

					<h6>Convolutional Networks</h6>
					<p>
						These are usually used when working with images. They are used because their convolution layers have filters which can break down an image to its parts (such as edges, lines, or more complex shapes/objects). Each layer can have one or more filters that allow certain features to be pulled out of the image. In a CNN, we normally have detection layers (convolution layers) and pooling layers.
					</p>
					<p>
						<a href="https://www.youtube.com/watch?v=YRhxdVk_sIs&t=5s" class="saymore">Great video</a> explaining visually convolution networks. To summarize the video: the process of a CNN layer is a filter (matrix) is convolved (scanned) over the image (can be varying sizes) and a dot product is done to get new values to be stored in a new image matrix which has pulled out features you are looking for with the filter.
					</p>

					<h6>Pooling</h6>
					<p>
						Pooling allows us to reduce the size of the representation and build in invariance to small transformations. How it works is the user defines a sized matrix (nxn) and the stride (how many pixels the filter will move across the filter, i.e. m). There are different functions that can be used in order to pool the values, most often it is taking the max. The nxn filter is placed on our original image and the ‚Äúmax‚Äù (or other value) is taken of the matrix and stored in a new (smaller) image. Then we slide over by our stride and continue the process. This is done to reduce the parameters in the network and it helps reduce overfitting. 
					</p>
					<p>
						<a href="https://www.youtube.com/watch?v=ZjM_XQa5s6s" class="saymore">Great video</a> explaining visually max pooling.
					</p>

					<h6>Resources</h6>
					<p>
						I would highly recommend these two series to watch to fully understand neural networks:
						<ul class="disc">
							<li><a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" class="saymore">3Blue1Brown</li>
							<li><a href="https://www.youtube.com/watch?v=gZmobeGL0Yg&list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU&index=1" class="saymore">Deeplizard</li>
							<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html" class="saymore"> Michael Nielsen's deep learning book</a></li>
					</p>
				</div>
			</li> 

		<!-- Accordion: 10 Generalization -->
			<li class="">
				<div class="title">
					<b>Generalization (Overfitting and Regularization)</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						There are several things that can be done to train a model to generalize well (reduce <a href="https://www.youtube.com/watch?v=DEMmkFC6IGM" class="saymore">overfitting</a>):
					</p>
					<h6><a href="https://www.youtube.com/watch?v=rfM4DaLTkMs&list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU&index=17" class="saymore">Data augmentation</a></h6>
							<ul class="square">
								<li>
									If you need more data you can transform the data you have to create more training examples that are slightly different.
								</li>
							</ul>

							<h6>Reduce number of parameters or model complexity:</h6>
							 <ul class="square">
							 	<li>Underfitting can have too few parameters</li>
							 	<li>Overfitting can have too many parameters</li>
							 	<li>
							 		For a NN, you can:
							 		<ul >
							 			<li>Reduce number of layers</li>
							 			<li>Reduce number of parameters per layer</li>
							 			<li>Add linear bottleneck: layer which restricts the number of connections able to be passed through</li>
							 		</ul>
							 	</li>
							 </ul>


							<h6>Weight decay / L2 regularization</h6>
							<ul class="square">
								<li>
									Regularizer is a function that quantifies how much we prefer one hypothesis over another in our model. We attach a function to the cost function to penalize larger weights.
								</li>
								<li>
									We want smaller weights so that the model is not swayed by small discrepancies.
								</li>
								<li>
									<a href="https://www.youtube.com/watch?v=iuJgyiS7BKM&list=PLZbbT5o_s2xq7LwI2y8_QtvuXZedL6tQU&index=35" class="saymore">Here</a> is an amazing video that explains the L2 regularization well.
								</li> 
								<li>
									We can choose different regularizer penalties. In the case of L2 penalty we use the squared norms (can be Euclidean distance) of the weight matrices. This will penalize weights that are large to a greater degree than smaller weights.
								</li>
								<li>
									There are many kinds of regularizers to choose from (e.g. sum of absolute values, LASSO regression, etc.)
								</li>
							</ul>

							<h6>Early stopping</h6>
							<ul class="square">
								<li>
									This will monitor performance on the validation set and stop the training when the validation error starts increasing instead of decreasing.
								</li>
								<li>
									There can be an issue that arises because not every epoch will create a decrease in validation error, so it is part of the model to determine when the validation error has actually leveled off.
								</li>
								<li>
									Early stopping works because weights start off small and progressively get bigger, so stopping early in the epoch count will allow the weights to stay smaller.
								</li>
							</ul>

							<h6>Ensembles</h6>
							<ul class="square">
								<li>Averaging over trials using various methods will make your generalization better off.</li>
							</ul>

							<h6>Stochastic regularization (e.g. dropout)</h6>
							<ul class="square">
								<li>
									With dropout, the network is run many times and for each trial various nodes are dropped from the network. The predictions are then averaged over the trials.
								</li>
								<li>
									Other stochastic regularizers have been proposed: DropConnect (dropping connections instead of nodes), batch normalization, etc.
								</li>
							</ul>

				</div>
			</li> 

		<!-- Accordion: 11 Unsupervised Learning (PCA) -->
			<li class="">
				<div class="title">
					<b>Unsupervised Learning (PCA)</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<p>
						Unsupervised learning is when there are no labels to the data and we must find patterns within the data. Principal component analysis is a type of unsupervised machine learning algorithm in which linear dimensionality reduction occurs.
					</p>
					<p>
						Principal component analysis is a type of unsupervised machine learning algorithm in which linear dimensionality reduction occurs.
					</p>
					<p>
						The data is mapped to a lower dimensional space by projections. We want to pick the subspace that maximizes variance in order to preserve the most data (and minimize error). Next, we want to ensure that we are picking an orthogonal basis. After those two requirements are met, we project each vector individually and sum together their projections. As a result of the process we end up with a decorrelation of features.
					</p>
					<p>
						Representative learning is learning where a mapping is created that is easier to visualize or manipulate.
					</p>
				</div>
			</li> 

		<!-- Accordion: 12 Fairness in ML -->
<!-- 			<li class="">
				<div class="title">
					<b>Fairness in ML</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						This is a big topic that needs to be regarded whenever you use machine learning. It is important to consider 
					</p>
				</div>
			</li>  -->

	<div class="hr">
	</div>
	</div>
</div>





<div class="row">
	<div class="twelve columns">
		
		<!-- Researched Extras -->
		<div class="circledate">
			<div class="textcircle">
				<br>Extras
			</div>
		</div>
		<h4>Researched Extras</h4>
		<div class="dots blogdots">
		</div>
		<p>
			Along the way I spent time researching different aspects of the course that I <strike>wanted</strike> needed to know more about
		</p>

		<!-- Accordion: Statistical Model -->
			<ul class="accordion">
			<li class="">
				<div class="title">
					<b>General ML Model Concepts</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
					
					<!-- Tabs-->
					<dl class="tabs">
						<dd class="active"><a href="#overunderfit">Overfit vs Underfit</a></dd>
						<dd><a href="#hyperparameters">Hyperparameters</a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="overunderfitTab">
							<a href="https://www.edureka.co/community/15471/what-are-the-differences-between-overfitting-underfitting" class="saymore">From this article</a> I got a good understanding of overfitting and underfitting, along with the notes taken in class.<br>
							<u>Overfitting:</u><br>
							<p>
								When overfitting occurs it means the model is picking up on noise instead of the relationship. This occurs if your model is too complex. For instances, small k or large depth for a decision tree.<br>
								When looking at variance (the amount of variability in the data) we are talking about overfitting (for squared error).
							</p>

							<u>Underfitting:</u><br>
							<p>
								When underfitting occurs the model has failed to have enough insights to get the relationship of the data. This occurs if a model is too simple or uses the wrong type of model. For instance, large k or small depth for decision tree or fitting a linear model on non-linear data.<br>
								When talking about bias (how wrong the prediction is) we are talking about undercutting (for squared error).
							</p>
						</li>

						<li id="hyperparametersTab">
							https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac# 
						</li>

					</ul>	
				</div>
			</li>



		<!-- Accordion: Decision Trees -->
			<li class="">
				<div class="title">
					<b>Decision Trees</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						
					</p>
					
					<!-- Tabs-->
					<dl class="tabs">
						<dd class="active"><a href="#decisionalgorithm">Decision Tree Algorithm Types</a></dd>
						<dd><a href="#pruning">Pruning a Tree</a></dd>
						<dd><a href="#dtrules">Extracting Decision Tree Rules</a></dd>
					</dl>

					<ul class="tabs-content">
						<li class="active" id="decisionalgorithmTab">
							https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html  
						</li>

						<li id="pruningTab">
							<a href="https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09#" class="saymore">This article</a> had a lot of good ways to avoid overfitting. In it "pruning a tree" was discussed.<br>

							We use "pruning a tree" to avoid overfitting. The method works by setting limits on specific hyperparameters. These hyperparameters for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="saymore">DecisionTreeClassifier()</a> include max_depth, min_samples_leaf, and min_samples_split and can early stop the growth of the tree:
							<ul class="disc">
								<li>
									Max_depth: if you decrease the max_depth allowed, you will decrease the overfitting.
								</li>
								<li>
									Min_samples_leaf: "The minimum number of samples required to be at a leaf node" (default 1). The larger this number the more overfitting is decreased.
								</li>
								<li>
									Min_samples_split: "The minimum number of samples required to split an internal node" (default 2). The larger this number the more overfitting is decreased.
								</li>
							</ul>
						</li>

						<li id="dtrulesTab">
							https://mljar.com/blog/extract-rules-decision-tree/ <br>
							https://mljar.com/blog/visualize-decision-tree/ <- great for visualization
						</li>

					</ul>	

				</div>
			</li>

			<!-- Accordion: k-Means -->
			<li class="">
				<div class="title">
					<b>k-Means</b>
				</div>
				<div class="content" style="overflow: hidden; display: none; ">
					<h6></h6>
					<p>
						https://towardsdatascience.com/interpretable-k-means-clusters-feature-importances-7e516eeb8d3c 
					</p>
					
					<!-- Tabs-->
					<!-- <dl class="tabs">
						<dd class="active"><a href="#decisionalgorithm">Decision Tree Algorithm Types</a></dd>
						<dd><a href="#pruning">Pruning a Tree</a></dd>
						
					</dl>

					<ul class="tabs-content">
						<li class="active" id="decisionalgorithmTab">

						</li>

						<li id="hyperparametersTab">
							
						</li>

						<li id="dtrulesTab">
							
						</li>

					</ul>	
 -->
				</div>
			</li>





		</ul>

	</div>
</div>


<div class="hr">
</div>

<!-- FOOOTER 
================================================== -->
<div id="footer">
	<footer class="row">
	<p class="back-top floatright">
		<a href="#top"><span></span></a>
	</p>
	<center>
		<div class="twelve columns">
			<h1>Connect with Me</h1>
			<a class="social github" href="https://github.com/anjawu"></a>
			<a class="social linkedin" href="https://www.linkedin.com/in/anja-wu/"></a>
		</div>
	</center>
	</footer>
</div>
<div class="copyright">
	<div class="row">
		<div class="six columns">
			 &copy;<span class="small"> Copyright 2021 Anja Wu</span>
		</div>
		<div class="six columns">
			<span class="small floatright"> Template by <a href="www.wowthemes.net">WowThemes.net</a></span>
		</div>
	</div>
</div>
<!-- JAVASCRIPTS 
================================================== -->
<!-- Javascript files placed here for faster loading -->
<script src="javascripts/foundation.min.js"></script>
<script src="javascripts/jquery.easing.1.3.js"></script>
<script src="javascripts/elasticslideshow.js"></script>
<script src="javascripts/jquery.carouFredSel-6.0.5-packed.js"></script>
<script src="javascripts/jquery.cycle.js"></script>
<script src="javascripts/app.js"></script>
<script src="javascripts/modernizr.foundation.js"></script>
<script src="javascripts/slidepanel.js"></script>
<script src="javascripts/scrolltotop.js"></script>
<script src="javascripts/hoverIntent.js"></script>
<script src="javascripts/superfish.js"></script>
<script src="javascripts/responsivemenu.js"></script>
</body>
</html>